---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, cache=TRUE, warning = FALSE)
# Load any R packages you need here
library(tidyverse)
library(visage)
```

\clearpage\pagenumbering{arabic}\setcounter{page}{0}

# Background and Motivation


The introduction of the report is organised as follows. Section \ref{se:predictive-modelling} discusses the predictive modelling culture and the use of data plots in predictive model diagnostics. Section \ref{se:vi} outlines the definition and terminology of visual inference. Section \ref{se:prespecification} presents the justification of the validity of visual inference. Section \ref{se:sampling-from-null} describes the sampling techniques in visual inference with an example of the classical normal linear regression model. Section \ref{se:lineup} introduces the lineup protocol and the estimation of $p$-value and power of a visual test. And lastly, Section \ref{se:limitation} points out the limitation of lineup protocol, which is also the motivation of this research, and the need of building an automatic visual inference system.


<!-- ## Motivation -->

<!-- Have you doubted what you found from a data plot was really there? Or have you had reservations about visual discoveries because they might be illusions? Data plots are prevalent in every field of nature and social science as it is one of the most recognized data visualization techniques. Without data plots, trends and patterns embed in the data would not be easy to discover. However, data plots are often over- or under-interpreted by human readers which leads to unsecured findings. This is unsatisfactory as confirmatory research is the foundation of science. -->

<!-- Visual inference, an inferential framework developed by @buja_statistical_2009, provides an option to test the visual findings, which helps solidify our confidence in using data plots. But one of the drawbacks of this framework is it requires human evaluation of data plots. Such requirement is unattainable when the number of data plots need for testing is large. Even if it can be fulfilled, the cost of labour would be exceedingly high. -->

<!-- ## Research Problem -->

<!-- The main objective of this research is to build an automatic visual inference system that can evaluate data plots. We aim to apply this automatic system on predictive model diagnostics because one of the scenarios best fitted for visual inference is where there does not exist a conventional test, and flexible predictive models which adopt machine learning algorithms do not generally have a conventional hypothesis testing procedure.  -->


## Predictive Modelling and Visual Diagnostics {#se:predictive-modelling}

@donoho_50_2017 in its summary of data science stated that the concept of the predictive modelling culture could be traced back to an article written by Breiman (2001). In contrast to the generative modelling culture, which aims to develop stochastic models to make inferences about the data generating process, predictive modelling emphasizes the ability of the model to make accurate predictions. 

Predictive models are primarily evaluated by predictive accuracy with the use of validation and test data, but in predictive model diagnostics, especially model testing and tuning, data plots play an irreplaceable role. In these diagnostics, though numeric summaries are mostly available and some are even endorsed by finite or asymptotic properties, graphical representation of data is still preferred, or at least needed by researchers, due to its intuitiveness and the possibility for unexpected discoveries which may be abstract and unquantifiable. 

However, unlike confirmatory data analysis built upon rigorous statistical procedures, e.g., hypothesis testing, visual diagnostics relies on graphical perception - human’s ability to interpret and decode the information embedded in the graph [@cleveland_graphical_1984], which is to some extent subjective. Further, visual discovery suffers from its unsecured and unconfirmed nature where the degree of the presence of the visual features typically can not be measured quantitatively and objectively, which may lead to over or under-interpretations of the data. One such example is finding an over-interpretation of the separation between gene groups in a two-dimensional projection from a linear discriminant analysis when in fact there are no differences in the expression levels between the gene groups and separation is not an uncommon occurence [@roy_chowdhury_using_2015].

## Visual Inference {#se:vi}

Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualizatino" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots [@gelman_exploratory_2004]. Later, in the Bayesian context, data plots was systematically considered as model diagnostics by taking advantage of the data simulated from the assumed statistical models [@gelman_bayesian_2003; @gelman_exploratory_2004]. 

It was surprising that the essential components of visual inference had actually been established in @buja_inference_1999, but it was not until 10 years later that @buja_statistical_2009 formalized it as an inferential framework to extend confirmatory statistics to visual discoveries. This framework redefines the test statistics, tests, null distribution, significance levels and $p$-value for visual discovery modelled on the confirmatory statistical testing. Figure \@ref(fig:parallelism) outlines the parallelism between conventional tests and visual discovery. 

![(ref:parallelism) \label{fig:parallelism}](figures/rsta2009012001.jpg){width=450 height=341}

(ref:parallelism) Parallelism between multiple quantitative testing and visual discovery [@buja_statistical_2009]. Visible features in a plot are viewed as a collection of test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})~(i \in I)$, and any visual discoveries that are inconsistent with the null hypothesis are treated as evidence against the null. For model diagnostics, the null hypothesis would be the assumed model, and visual discoveries could be any visual features in favour of any alternatives.

In visual inference, a visual discovery is defined as a rejection of a null hypothesis, and the same null hypothesis can be rejected by many different visual discoveries [@buja_statistical_2009]. For model diagnostics, the null hypothesis would be the assumed model, while the visual discoveries would be any findings that are inconsistent with the null hypothesis. The same assumed model, such as classical linear regression model, can be rejected by many reasons with residual plot, including nonlinearity and heteroskedasticity as shown in Figure \@ref(fig:residual-plot-cubic-heter). 

```{r residual-plot-cubic-heter, fig.cap='Residuals vs. fitted values plot for a classical linear regression model. The residuals are produced by fitting a two-predictor multiple linear regression model with data generated from a cubic linear model. From the residual plot, "butterfly shape" can be observed which generally would be interpretd as evidence of heteroskedasticity. Further, from the outline of the shape, nonlinear patterns exist. Both visual discoveries are evidence against the null hypothesis, though heteroskedasticity actually does not exist in the data generating process.'}
library(tidyverse)

set.seed(10086)
cm <- visage::cubic_model(a = 0, b = 100, c = 1)

cm$gen(1000, fit_model = TRUE) %>%
cm$plot() +
  theme_light()
```


## Pre-specification of Visual Discoverable Features {#se:prespecification}

As discussed in @buja_statistical_2009, in the practice of model diagnostics, the range of possible visual discoveries is not pre-specified. In other words, people do not explicitly specify which visual feature(s) they are looking for before the reading the diagnostic plot. In contrary, conventional hypothesis testing always requires the pre-specification of the parameter space $\Theta$ of the parameter of interest $\theta \in \Theta$ to form a valid inferential procedure. To address this issue, a collection of test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})~(i \in I)$ is defined, where $\boldsymbol{\mathrm{y}}$ is the data and $I$ is a set of all possible visual features. @buja_statistical_2009 described each of the test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})$ as a measurement of the degree of presence of a visual feature. Alternatively, @majumder_validation_2013 avoids the use of visual features and defined the visual statistics $T(.)$ as a mapping from a dataset to a data plot. Both definitions of visual test statistics are valid, but in the rest of the report the first definition will be used as it covers some details needed by the following discussion.

The size of the collection $T^{(i)}(\boldsymbol{\mathrm{y}})~(i \in I)$ depends on the size of the set $I$. Thus, if one can define $I$ comprehensively, i.e, pre-specify all the visual discoverable features, the validity issue will be solved. Unfortunately, to our knowledge, there is no such a way to list all visual features. In linear regression diagnostics, possible visual features of a residual plot may be outliers, shapes and clusters. But this is an incomplete list which does not enumerate all the visual features.

Similarly, @wilkinson_graph-theoretic_2005 proposed the work called graph theoretic scagnostics, which adopted the idea of "scagnostics" - scatter plot diagnostics from (can't find the 1984 citation). It includes 9 computable scagnostics measures defined on planar proximity graphs: "Outlying", "Convex", "Skinny", "Stringy", "Straight", "Monotonic", "Skewed", "Clumpy" and "Striated" which attempts to describe outliers, shape, density, trend and coherence of the data. This approach is inspiring but it still does not give the complete list of visual discoverable features. In fact, it is possible that such a list will never be complete as suggested in @buja_statistical_2009.

Thinking out of the box, @buja_statistical_2009 argued that there is actually no need for pre-specification of visual discoverable features. In model diagnostics, when the null hypothesis is rejected, the reasons for rejecting the hypothesis will also be known. This is because observers can not only point out the fact that visual discoveries have been found, but also describe the particular visual features they observed. Those features will correspond to the subset of the collection of visual test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})~(i \in I)$ which resulted in rejection. This argument helps justifies the validity of visual inference.

## Sampling from the Null Distribution {#se:sampling-from-null}

In visual inference, the null distribution of plots refers to the infinite collection of plots of null datasets sampled from $H_0$. It is defined as the analogue of the null distribution of test statistics in conventional test [@buja_statistical_2009]. In practice, a finite number of plots of null datasets could be generated, called null plots. 

In the context of model diagnostics, sampling data from $H_0$ is equivalent to sampling data from the assumed model. As @buja_statistical_2009 suggested, $H_0$ is usually composited by a collection of distributions controlled by nuisance parameters. Since statistical models can have various forms, there is no general solution to this problem, but it sometimes can be reduced to so called "reference distribution" by applying one of the three methods: (i) sampling from a conditional distribution given a minimimal sufficient statistic under $H_0$, (ii) parametric bootstrap sampling with nuisance parameters estimated under $H_0$, and (iii) Bayesian posterior predictive sampling.

The conditional distribution given a minimal sufficient statistic is the best justified reference distribution among the three [@buja_statistical_2009]. Suppose there exists a minimal sufficient statistic $\boldsymbol{S}(\boldsymbol{y})$ under the null hypothesis, any null datasets $\boldsymbol{y^{*}}$ should fulfil the condition $\boldsymbol{S}(\boldsymbol{y}) = \boldsymbol{s}$. Using the classical linear regression model as example, the minimal sufficient statistic is $\boldsymbol{S}(\boldsymbol{y}) = (\hat{\boldsymbol{\beta}}, \boldsymbol{e}'\boldsymbol{e})$, where $\hat{\boldsymbol{\beta}}$ are the coefficient estimators and $\boldsymbol{e}'\boldsymbol{e}$ is the residual sum of square. Alternatively, the minimal sufficient statistic can be constructed as $\boldsymbol{S}(\boldsymbol{y}) = (\hat{\boldsymbol{y}}, ||\boldsymbol{e}||)$, where $\hat{\boldsymbol{y}}$ are the fitted values and $||\boldsymbol{e}||$ is the length of residuals, which is more intuitive as suggested by @buja_statistical_2009. Since the fitted values are held fixed, the variation can only occur in the residual space. And because the length of residual is also held fixed, residuals obtained from a null dataset has to be a random rotation of $\boldsymbol{e}$ in the residual space. With this property, null residuals can be simulated by regressing $N$ i.i.d standard normal random draws on the regressors, then rescaling it by the ratio of residual sum of square in two regressions. 


## Lineup Protocol {#se:lineup}

With the validity of visual inference being justified and the simulation of null plots being provided, another aspect of hypothesis testing that needs to be addressed is the control of false positive rate or Type I error. Any visual statistic $T^{(i)}(\boldsymbol{\mathrm{y}})$ needs to pair with a critical value $c^{(i)}$ to form a hypothesis test. When a visual feature $i$ is discovered by the observer from a plot, the corresponding visual statistic $T^{(i)}(\boldsymbol{\mathrm{y}})$ may not be known as there is no general agreement on the measurement of the degree of presence of a visual feature. It is only the event that $T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}$ is confirmed. Similarly, if any visual discovery is found by the observer, we say, there exists $i \in I:~T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}$ [@buja_statistical_2009].

Using the above definition, the family-wise Type I error can be controlled if one can provide the collection of critical values $c^{(i)}~(i \in I)$ such that $P(\mathrm{there~exists~} i \in I: T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}|\boldsymbol{\mathrm{y}}) \leq \alpha$, where $\alpha$ is the significance level. However, since the quantity of $T^{(i)}(\boldsymbol{\mathrm{y}})$ may not be known, such collection of critical values can not be provided.

@buja_statistical_2009 proposed the lineup protocol as a visual test to calibrate the Type I error issue without the specification of $c^{(i)}~(i \in I)$. It is inspired by the "police lineup" or "identity parade" which is the act of asking the eyewitness to identify criminal suspect from a group of irrelevant people. The protocol consists of $m$ randomly placed plots, where one plot is the actual data plot, and the remaining $m - 1$ plots have the identical graphical production as the data plot except the data has been replaced with data consistent with the null hypothesis. Then, an observer who have not seen the actual data plot will be asked to point out the most different plot from the lineup.

Under the null hypothesis, it is expected that the actual data plot would have no distinguishable difference with the null plots, and the probability of the observer correctly picks the actual data plot is $1/m$. If we reject the null hypothesis as the observer correctly picks the actual data plot, then the Type I error of this test is $1/m$.

This provides us with an mechanism to control the Type I error, because $m$ - the number of plots in a lineup can be chosen. A larger value of $m$ will result in a smaller Type I error, but the limit to the value of $m$ depends on the number of plots a human is willing to view [@buja_statistical_2009]. Typically, $m$ will be set to $20$ which is equivalent to set $\alpha = 0.05$, a general choice of significance level for conventional testing among statisticians.

Further, if we involve $K$ independent observers in a visual test, and let $X$ be a random variable denoting the number of observers correctly picking the actual data plot. Then, under the null hypothesis $X \sim \mathrm{Binom}_{K,1/m}$, and therefore, the $p$-value of a lineup of size $m$ evaluated by $K$ observer is given as
\begin{equation} \label{eq:pvaluesingle}
P(X \geq x) = \sum_{i=x}^{K}{{K}\choose{i}}\left(\frac{1}{m}\right)^i\left(\frac{m-1}{m}\right)^{k-i},
\end{equation}

where $x$ is the realization of number of observers correctly picking the actual data plot [@majumder_validation_2013].

The multiple individuals approach avoids the limit of $m$, while provides visual tests with $p$-value much smaller than $0.05$. In fact, the lower bound of $p$-value decreases exponentially as $K$ increases. With just $4$ individuals and $20$ data plots in a lineup, the $p$-value could be as small as $0.0001$. Additionally, by involving multiple observers, variation of individual ability to read plots can be addressed to some degree as different opinions about visual discoveries can be collected.  

Compared to the conventional test, whose power only depends on the parameter of interest $\theta$, several studies [see @hofmann_graphical_2012; @majumder_validation_2013; @majumder_human_2014; @roy_chowdhury_using_2015; @loy_variations_2016] have shown the power of the visual test is subject-specific. Thus, to be able to account for individual's ability, an individual is required to evaluate multiple lineups [@majumder_validation_2013].  

Suppose individuals have the same ability and a lineup has been evaluated by multiple individuals, under the alternative hypothesis, the estimated power for a lineup can be expressed as $\hat{p} = x/K$, the estimated probability of identifying the actual data plot from the lineup. If the individual skill needs to be taken into account, and $L$ lineups have been evaluated by $K$ individuals, @majumder_validation_2013 suggests that mixed effects logistic regression model can be fit as:

$$g(p_{li}) = W_{li}\delta + Z_{li}\tau_{li},$$
where $g(.)$ is the logit link function $g(p) = log(p)  - log(1-p)$; $0 \leq p \leq 1$. $W_{li}$, $1 \leq i \leq K$, $1 \leq l \leq L$, is the covariate matrix including lineup-specific elements and demographic information of individuals, and $\delta$ is a vector of parameters. $Z$ is the random effects matrix, and $\tau$ is a vector of variables follow $N(\boldsymbol{0},\sigma_{\tau}\boldsymbol{I}_{KL\times KL})$. 

Then, the estimated power for lineup $l$ and individual $i$ can be calculated as $\hat{p}_{li} = g^{-1}(W_{li}\hat{\delta} + Z_{li}\hat{\tau}_{li})$ [@majumder_validation_2013].

## Limitations of Lineup Protocol and Automatic Visual Inference {#se:limitation}

Although lineup protocol has already been integrated into data analysis of various topics, such as diagnostics of hierarchical linear models [@loy_diagnostic_2013], geographical Research [@widen_graphical_2016] and forensic examinations [@krishnan_hierarchical_2021], the involvement with human judgements limits its popularity. Similar to handicraft in pre-industrial society, lineup protocol conducted by humans is infeasible on a large scale, high in labour cost and time consuming. Moreover, it is strongly unfriendly to vision-impaired people.

The "steam engine" of visual inference needs to be developed for relieving people's workload by automating repeating tasks and providing standard result in a control environment. Large-scale evaluation of visual tests is not possible without the use of technology and machines. 

Modern computer vision model could be a promising solution to this problem. As a subfield of AI, computer vision with the modern deep learning architectures solved numerous critical problems in automation. Inspired by the vision processing in living organisms, the convolutional neural network was introduced by @fukushima_neocognitron_1982. Soon, this architecture was applied to hand-written number recognition trained with back-propagation by @lecun_backpropagation_1989. This was one of the earliest attempts which human successfully extract information from digital images via self-learning algorithms. Modern computer vision model is typically built on the deep neural network with convolutional layers [@fukushima_neocognitron_1982]. Convolutional layers take advantage of the hierarchical pattern in data and provide regularized versions of fully-connected layers. It downscales and transforms the image by summarising information in a small space. Numerous studies have shown that it can be used to effectively tackle vision tasks, such as image recognition [@rawat_deep_2017]. With the development of graphics processing units and the spread of high-performance personal computers, researches in computer vision become a new hype in the 21st century. Achievements such as computer-aided diagnosis [@lee_image_2015], pedestrian detection [@brunetti_computer_2018] and facial recognition [@emami_facial_2012] had a significant impact on our daily life.

Using computer vision models to read data plots is not a general choice. some fields have adopted this idea by applying computer vision models to read recurrence plots for time series regression [@ojeda_multivariate_2020], time series classification [@chu_automatic_2019; @hailesilassie_financial_2019; @hatami_classification_2018; @zhang_encoding_2020], anomaly detection [@chen_convolutional_2020] and pairwise causality analysis [@singh_deep_2017]. However, evaluating lineups with computer vision model is a new field of study.

# Research Problem

<!-- 1. Provide visual diagnostics for projection pursuit optimisation to examine whether the optimisers are able to find the optima, compare optimisation paths, inspect optimisers’ coverage of the space, and detect anomalies in the algorithms. -->

<!-- 2. Extend and enhance the visual diagnostic tools developed in the first project to stochastic optimal control problems, which is a multistage optimisation problem with more than two exogenous variables. -->

<!-- 3. Improve the interpretability and explainability of stochastic optimal control algo- rithms by identifying the most important drivers in the optimal decisions suggested by the optimisers and studying how the decision changes across time. -->


The main objective of this research is to build an automatic visual inference system for the purpose of conducting large-scale visual test. The research will focus on three projects as follows:


1. Propose a prototype of automatic visual inference system for evaluating lineups of residual plots of the classical normal linear regression model and study factors that affect the performance of the system with comparison to human subjects.

2. Extend and enhance the automatic visual inference system proposed in the first project to general lineup protocol problems by adopting image similarity assessed by computer vision models.

3. Validate and apply the automatic visual inference system to predictive model diagnostics on empirical data.


# Overview of the Thesis

The first project proposes a prototype of automatic visual inference system for evaluating lineups of residual plots, with a focus on the classical normal linear regression model given it is one of the simplest predictive models. The automatic system is identical to the lineup protocol except evaluators are replaced by computer vision models. The computer vision models are trained by using data simulated from linear models with violations of different classical assumptions, such as linearity and homoscedasticity. Data of human performance on evaluating residual plots generated under the same simulation setting is collected by conducting online human subject experiments, such that the comparison between the power of human subject-assessed visual tests and the system-assessed visual tests can be made. Finally, factors that affect the performance of the automatic system are studied for improving the architecture and design of the computer vision models.

The second project aims to extend the proposed automatic system to lineup protocol with no assumptions on the type of the data plot and the data generating process. It only requires the protocol to asks the evaluators to select the most different plot(s) from the lineup. The automatic system will be built upon image similarity produced by computer vision models. 

The third project aims to apply the automatic system to predictive model diagnostics on empirical data. Potential predictive models considered by this research are linear model, generalized linear models and non-parametric regression. Multiple human subject experiments will need to be conducted to validate the effectiveness of the automatic system.  

# Automatic Visual Statistical Inference for linear regression diagnostics

This project focuses on building a prototype of automatic visual statistical inference system for evaluating residual plots of classical normal linear regression model. To set up a comparison between the computer vision models and humans, human subject experiments were conducted to understand the ability of human reading residual plots.

Because the first project is still ongoing, we consider the result we have is not enough to write as a paper at the moment. Hence, the material provided in this section is two main parts of the draft paper that will contain enough details for understanding the project. 
  
## Human Subject Exeriments

To collect data of human performance on reading residual plot of linear regression model with nonlinearity and heteroskedasticity defects, two human experiments were conducted. Participants of both experiments were recruited using an online platform called Prolific [@prolific_prolific_2022]. 

Prolific provides an international participant pool with the option to apply flexible pre-screening filters. In this study, we recruited $62$ participants who was fluent in English with at least $10$ previous submissions and $98$% approval rate in other Prolific studies for quality control. Further, balance sample across gender was imposed to prevent gender bias. 

In Prolific, researchers can either approve or reject submissions based on the quality of the responses. If a submission is approved by the researcher, the participant will be paid a certain amount of money per hour of time spent on the experiment. To assess the quality of the responses, two attention checks were given to each participant during the experiment, where at least one of them was required to pass for the approval of submission. 

Throughout the experiments, participants were requested to complete a short survey and evaluate 20 lineups on a website in an hour. Each lineup consists of one actual residual plot and 19 null residual plots produced by plotting null residuals simulated from the residual rotation distributions. Among the 20 lineups, there were two extremely easy lineups used as attention checks which everyone should get correct. The short survey was intended to collect information about participant that might affect their ability in reading data plot including age, highest level of education, preferred pronoun and previous experience in similar study. For the evaluation of the lineup, participant first needed to select one or multiple most different data plot(s) from the lineup by clicking the corresponding image. Then, the primary reason for choosing the plots needed to be provided by picking one of the given options - "outlier(s)", "cluster(s)", "shape" and "other". Explanations about these options were provided in the training page and the mouseover text. Table \@ref(tab:lineup-reason) gives the detailed explanations about these reasons. If the option "other" was selected, text input for the specified reason would be collected. Lastly, the degree of difference between their chosen data plot(s) and other data plots needed to be selected among five levels - "not at all", "slightly", "moderately", "very" and "extremely". Notably, if participants could not tell the difference between the data plots, there was an option to skip the evaluation of the lineup. However, to prevent participants abusing this option, warnings were given at the beginning of the study that skipping too many lineups might lead to rejection of submission since it demonstrated clear low-effort throughout the experiment.


```{r}
data.frame(c("outlier(s)", "cluster(s)", "shape", "other"), 
           c("In a data plot, an outlier is a point that differs significantly from other points.", 
             "In a data plot, a cluster is a group of points positioned closely together. \nAnd usually, there will be gaps between different clusters.",
             "It could be any common shapes we would see in real life, like triangle shape, U-shape, butterfly shape, etc.",
             "Patricipants needs to give their reasons in the text input.")) %>%
knitr::kable(col.names = c("Reasons", "Explanations"), 
             booktabs = TRUE, 
             caption  = 'Explanations about reasons for choosing data plots from a lineup', 
             label = "lineup-reason") %>%
  kableExtra::kable_styling(latex_options = c("scale_down"))
```

The study website was powered by Flask [@grinberg_flask_2018], a web framework written in Python 3 [@van_rossum_python_2009], hosted on PythonAnywhere [@pythonanywhere_pythonanywhere_2022], a web hosting service provider. Due to the limit of the storage on PythonAnywhere, lineup images needed for the website were hosted separately on Github Pages [@gtihub_github_2022-1], a static site hosting service provided by Github [@gtihub_github_2022], stored in private Github repositories. The uniform resource locator (URL) to lineup images were set to be unique random strings such that images can not be accessed by general approaches without knowing the correct URL. This avoided participants seeing the lineup beforehand. The front-end of the website was built using a JavaScript [@flanagan_javascript_2006] library jsPsych [@de_leeuw_jspsych_2015] which is specialized in creating online behavioral experiments. One of the reasons we chose to use this library was it had a modularized but highly customizable template which could record participant's response time automatically. This is essential for us to confirm the quality of the data by checking exceptionally fast or slow responses. Figure \@ref(fig:diagram-experiment) summarizes the online experiment setup.

![Diagram of online experiment setup. The server-side of the study webiste used Flask as backend hosted on PythonAnywhere. And the client-side used jsPsych to run experiment.  \label{fig:diagram-experiment}](figures/experiment_tech.pdf){width=450 height=450}

The first two pages of the website are the explanatory statement and the consent form. Participants were asked to read the documents and agree with the terms to advance to the short survey. With the completion of the survey, the next page is the training page, which contained instructions on how to interact with the webpage. Followed by the training page, there are 20 lineups each on a single page. Figure \@ref(fig:website-layout) illustrates the layout of the website. At the end of the experiment, participants would be redirected back to Prolific and waited for researchers' responses.

![Layout of the study website. Participants needed to choose the most different plots on the right and select their reasons and confidence levels on the left. \label{fig:website-layout}](figures/website.png){width=550 height=299}

Next we will discuss the simulation setup for this study. The experiment data was simulated by the use of the programming language `R` [@r_core_team_r_2021]. For the ease of reproducibility, functions to build models, simulate data from models, produce lineup with data, allocate stimulis for subjects and evaluate subject responses from this study are bundled in the package `visage` with a unique object-oriented programming system built upon the environment feature of `R`. In the description of the simulation, corresponding functionalities of the package will be introduced. 

Two models were used in the study, where both were linear models with some degree of violations of classical assumptions. 

### Cubic Model

The first model was a cubic linear model with two regressors, which can be expressed by:
\begin{equation} \label{eq:cubic}
\boldsymbol{Y}= 1 + (2-c)\boldsymbol{X} + c\boldsymbol{Z} + a[(2-c)\boldsymbol{X}]^2+a(c\boldsymbol{Z})^2+b[(2-c)\boldsymbol{X}]^3+b(c\boldsymbol{Z})^3+\boldsymbol{\varepsilon},
\end{equation}
where $c \in (0,2)$, $a \in (-3,3)$, $b \in (-3,3)$, $\boldsymbol{\varepsilon}\overset{iid}{\sim} N(\boldsymbol{0},\sigma^2\boldsymbol{I})$, $\boldsymbol{Y}$, $\boldsymbol{X}$ and $\boldsymbol{Z}$ are $n\times1$ matrices. 

This defines a cubic relationship between $\boldsymbol{Y}$, $\boldsymbol{X}$ and $\boldsymbol{Z}$. Meanwhile, to create nonlinearity defect, the null model followed the assumptions of the classical normal linear regression model (CNLRM), fitted by OLS is:
\begin{equation} \label{eq:cubic_null}
\boldsymbol{Y}=\beta_0+\beta_1\boldsymbol{X}+\beta_2\boldsymbol{Z}+\boldsymbol{u},
\end{equation}
where $\boldsymbol{u} \sim N(0,\sigma^2_u\boldsymbol{I})$.

Clearly, omitted-variable bias will present since the null model leaves out the quadric and cubic terms. 

\begin{lemma}[Distribution of residuals produced by the cubic model] \label{lemma:cubic}
Given the data generating process in Equation (\ref{eq:cubic}), and null model in Equation (\ref{eq:cubic_null}). Let $\boldsymbol{X}_a=[\boldsymbol{1},\boldsymbol{X},\boldsymbol{Z}]$ denotes the set of regressors in matrix form. Then, the residuals obtained from the null model are $$\boldsymbol{e} \sim N(\boldsymbol{R}_a\boldsymbol{X}_b\boldsymbol{\beta}_b, \sigma^2\boldsymbol{R}_a),$$ where $\boldsymbol{R}_a=\boldsymbol{I}-\boldsymbol{X}_a(\boldsymbol{X}_a'\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a'$, $\boldsymbol{X}_b=[\boldsymbol{X}^2,\boldsymbol{Z}^2,\boldsymbol{X}^3,\boldsymbol{Z}^3]$ and $\boldsymbol{\beta}_b=(a(2-c)^2,ac^2,b(2-c)^3,bc^3)'$.
\end{lemma}

\begin{proof}
Using the Frisch–Waugh–Lovell theorem, the residuals obtained by the null model are $$\boldsymbol{e}=\boldsymbol{R}_a\boldsymbol{Y}=\boldsymbol{R}_a(\boldsymbol{X}_a\boldsymbol{\beta}_a+\boldsymbol{X}_b\boldsymbol{\beta}_b+\boldsymbol{\varepsilon}),$$ where $\boldsymbol{R}_a=\boldsymbol{I}-\boldsymbol{X}_a(\boldsymbol{X}_a'\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a'$, $\boldsymbol{\beta}_a=(1,2-c,c)'$, $\boldsymbol{X}_b=[\boldsymbol{X}^2,\boldsymbol{Z}^2,\boldsymbol{X}^3,\boldsymbol{Z}^3]$ and $\boldsymbol{\beta}_b=(a(2-c)^2,ac^2,b(2-c)^3,bc^3)'$. 

Because $\boldsymbol{R}_a\boldsymbol{X}_a=\boldsymbol{0}$, we have $\boldsymbol{e}=\boldsymbol{R}_a(\boldsymbol{X}_b\boldsymbol{\beta}_b+\boldsymbol{\varepsilon}).$ Since $\boldsymbol{\varepsilon} \sim N(0,\sigma^2\boldsymbol{I})$, it follows that $\boldsymbol{e} \sim N(\boldsymbol{R}_a\boldsymbol{X}_b\boldsymbol{\beta}_b, \sigma^2\boldsymbol{R}_a)$. 
\end{proof}

Lemma \@ref(lemma:cubic) shows that the expectation of the residuals is clearly a function of $\boldsymbol{X}$ and $\boldsymbol{Z}$, which indicates the residuals and the fitted values $\hat{Y} = \boldsymbol{X}_a(\boldsymbol{X}_a'\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a'\boldsymbol{Y}$ are associated. Hence, it is expected that visual discoveries can be found in the residuals vs. fitted values plot. Let $X_i$ and $Z_i$, $i = 1,...,n$, be independent random variables follow uniform distribution $U(-1,1)$. Given the expectation of the residuals, we could plot the expected values of residuals against the observed values. Figure \@ref(fig:cubic-shape-1), \@ref(fig:cubic-shape-2) and \@ref(fig:cubic-shape-3) illustrate the shape of residuals and their expected values under different parameter settings. 

From the Figure \@ref(fig:cubic-shape-1), it can be observed that with fixed $\sigma$ and $c$, $a$ and $b$ are controlling the 2D projection of a hypersurface, and seemingly performing some rotations along different axes. Figure \@ref(fig:cubic-shape-2) shows that with fixed $a$, $b$ and $\sigma$, $c$ is controlling the contribution of $\boldsymbol{X}$ and $\boldsymbol{Z}$ to $\boldsymbol{Y}$. As $c$ moves toward $0$ or $2$, one regressor will dominate another, which will mitigate some joint effects and resemble a typical cubic function. In Figure \@ref(fig:cubic-shape-3), $a$, $b$ and $c$ are held fixed, and $\sigma$ is controlling the noises around the expected values. The underlying shape is clearer for smaller $\sigma$.

```{r cubic-shape-1, fig.cap = "A matrix of residuals vs. fitted values plot under different parameter settings of the cubic model. Resdiauls are drawns in black and the expected values are drawn in red. The plots show that if $\\sigma$ and $c$ are held fixed, $a$ and $b$ rotate a two-dimension projection of a hypersurface."}
library(visage)
library(tidyverse)

make_plot <- function(a, b, c, e_sigma) {
  mod <- cubic_model(a, b, c, e_sigma)
  dat <- mod$gen(1000, fit_model = TRUE)
  dat$exp <- mod$E(dat)
  mod$plot(dat, alpha = 0.3, size = 0.5, add_zero_line = FALSE, theme = theme_light(base_size = 5)) +
    geom_point(aes(.fitted, exp, col = "red"), alpha = 0.6, size = 0.1) +
    xlab("fitted values") +
    ylab("residuals") +
    ggtitle(bquote(a:.(a)~','~b:.(b)~','~c:.(c)~','~sigma:.(e_sigma))) +
    theme(legend.position = "none")
}

expand.grid(c=seq(1),
            a=seq(-1,1,1),
            b=seq(-1,1,1),
            e_sigma=c(0.5)) %>%
  arrange(a,b,c,e_sigma) %>%
  rowwise() %>%
  mutate(plots = list(make_plot(a,b,c,e_sigma))) -> result

result$plots %>% patchwork::wrap_plots(ncol=3)
```

```{r cubic-shape-2, fig.cap = "A matrix of residuals vs. fitted values plot under different parameter settings of the cubic model. Resdiauls are drawns in black and the expected values are drawn in red. The plots show that if $a$, $b$ and $\\sigma$ are held fixed, $c$ controls the contribution of $\\boldsymbol{X}$ and $\\boldsymbol{Z}$ to $\\boldsymbol{Y}$.", fig.height=1.9}
expand.grid(c=c(0,0.5,1,1.5,2),
            a=c(-1),
            b=c(1),
            e_sigma=c(0.5)) %>%
  arrange(a,b,c,desc(e_sigma)) %>%
  rowwise() %>%
  mutate(plots = list(make_plot(a,b,c,e_sigma))) -> result

result$plots %>% patchwork::wrap_plots(ncol=5)
```

```{r cubic-shape-3, fig.cap = "A matrix of residuals vs. fitted values plot under different parameter settings of the cubic model. Resdiauls are drawns in black and the expected values are drawn in red. The plots show that if $a$, $b$ and $c$ are held fixed, $\\sigma$ controls the strength of the signal.", fig.height=1.9}
expand.grid(c=c(1),
            a=c(-1),
            b=c(1),
            e_sigma=c(0.25,0.5,1,2,4)) %>%
  arrange(a,b,c,desc(e_sigma)) %>%
  rowwise() %>%
  mutate(plots = list(make_plot(a,b,c,e_sigma))) -> result

result$plots %>% patchwork::wrap_plots(ncol=5)
```

The residuals used in these three figures are simulated from the cubic models built using the `cubic_model()` function from the package `visage`. `cubic_model()` is a cubic model class constructor, which takes arguments `a`, `b`, `c`, `sigma`, `x` and `z`, where the first four are numeric values defined above, and `x` and `z` are random variable instances created by the random variable abstract base class constructor `rand_var()`. 

If we would like $\boldsymbol{X}$ and $\boldsymbol{Z}$ to be random uniform variables ranged from $-1$ to $1$, it can be achieved by using the random uniform variable class constructor `rand_uniform()` inherited from the random variable abstract base class. It only takes two arguments which are the lower bound and the upper bound of the support.

```{r echo = TRUE, cache = FALSE}
library(visage)
mod <- cubic_model(a = -3, b = -3, c = 1, sigma = 0.5, 
                   x = rand_uniform(-1, 1), z = rand_uniform(-1, 1))
```

An instance of cubic model class contains methods of simulating data and making residual plot. Method `mod$gen()` returns a data frame containing realizations of $X$, $Z$, $Y$ and $\varepsilon$ simulated from the model. The number of realizations depends on the integer argument `n`. In addition, if argument `fit_model = TRUE`, a null model will be fitted using the simulated data and residuals and fitted values will be included in the returned data frame.

```{r echo = TRUE, cache = FALSE}
mod$gen(n = 5, fit_model = TRUE)
```

Method `mod$plot()` produce a `ggplot` [@wickham_ggplot2_2011] object. It takes a data frame containing columns `.resid` and `.fitted` as input, along with a character argument `type` indicating the type of the data plot, and other aesthetic arguments such as `size` and `alpha` to control the appearance of the plot. 


```{r test3, echo = TRUE, cache = FALSE}
mod$plot(mod$gen(n = 100, fit_model = TRUE), type = "resid", size = 1)
```

Lineup is a matrix of residual plots which can be produced by using the methods `mod$gen_lineup()` and `mod$plot_lineup()`. Method `mod$gen_lineup` takes the number of realizations `n` and the number of plots in a lineup `k` as inputs. And the method `mod$plot_lineup()` has the same user interface as `mod$plot()`.

```{r echo = TRUE, cache = FALSE}
mod$plot_lineup(mod$gen_lineup(n = 50, k = 20), type = "resid", size = 1)
```


The cubic model class also provides method to compute the expected values of residuals. Method `mod$E()` takes a data frame with columns `x` and `z` as input, and returns a vector of expected values of residuals. 

```{r echo = TRUE, cache = FALSE}
mod$E(mod$gen(n = 5))
```

Since we know that under the null hypothesis, the residual $\boldsymbol{e}\sim N(\boldsymbol{0},\sigma^2\boldsymbol{R}_a)$. Thus, the difference between the expected values $\boldsymbol{R}_a\boldsymbol{X}_b\boldsymbol{\beta}_b$ and $\boldsymbol{0}$ represents the direct impact of the parameters $a$ and $b$ on the residuals. It is expected that the larger the magnitude of the expected value relative to the variance and covariance, the easier the human to spot the patterns in the residual plot.

To obtain a measure of the impact of $a$ and $b$ on the residuals adjusted for variance and covariance, we need to address several properties of the residuals. First, the variance of the residuals $\sigma^2\boldsymbol{R}_a$ is not an identity matrix. This can be fixed by standardizing the residuals by their variance-covariance matrix. Second, the difference between $\boldsymbol{R}_a\boldsymbol{X}_b\boldsymbol{\beta}_b$ and $\boldsymbol{0}$ could be negative, which is not ideal for comparison. Thus, the magnitude needs to be squared. Third, the measure needs to be a scalar. We could apply a weighted average operator $\boldsymbol{W}$ on the transformed expected residuals to obtain a single numeric value. For simplicity, we set $\boldsymbol{W}=n^{-1}\boldsymbol{1}$. Considering the high time complexity of computing the square root of $\boldsymbol{R}_a$, off-diagonal elements of $\boldsymbol{R}_a$ are set to be zeros. This gives the effect size: $$ES=n^{-1}||\sigma^{-1}\boldsymbol{R}_a^{-\frac{1}{2}}\boldsymbol{R}_a\boldsymbol{X}_b\boldsymbol{\beta}_b||^2=n^{-1}\sigma^{-2}||\boldsymbol{R}_a^{\frac{1}{2}}\boldsymbol{X}_b\boldsymbol{\beta}_b||^2\approx n^{-1}\sigma^{-2}||diag(\boldsymbol{R}_a)^{\frac{1}{2}}\boldsymbol{X}_b\boldsymbol{\beta}_b||^2,$$ where $diag(\boldsymbol{R}_a)$ is the diagonal matrix constructed from the diagonal elements of $\boldsymbol{R}_a$.

The interpretation of this effect size is the impact of parameter $a$ and $b$ on the squared deviation of the standardized expected residual per observation. It is not directly related to the shape, or the pattern human observed from the residual plot, but it is a reasonable approximation of the degree of the visual deviation from the null residuals under our cubic model setting. Figure \@ref(fig:effectsize) shows four residual plots with different effect sizes. As effect sizes increases, the strength of the signal become stronger.

```{r effectsize, fig.cap = "Cubic model residual plots under different effect sizes. The larger the effect size, the stronger the signal."}
mod <- cubic_model(-1, -1, 1, 0.5)
dat <- mod$gen(500, fit_model = TRUE)
mod$plot(dat, theme = theme_light(base_size = 5), size = 1) +
  ggtitle(glue::glue("Effect size: {round(mod$effect_size(dat), 2)}")) -> p1

mod <- cubic_model(-2, -1, 1, 0.5)
dat <- mod$gen(500, fit_model = TRUE)
mod$plot(dat, theme = theme_light(base_size = 5), size = 1) +
  ggtitle(glue::glue("Effect size: {round(mod$effect_size(dat), 2)}")) -> p2

mod <- cubic_model(-5, -5, 1, 0.5)
dat <- mod$gen(500, fit_model = TRUE)
mod$plot(dat, theme = theme_light(base_size = 5), size = 1) +
  ggtitle(glue::glue("Effect size: {round(mod$effect_size(dat), 2)}")) -> p3

mod <- cubic_model(-20, -20, 1, 0.5)
dat <- mod$gen(500, fit_model = TRUE)
mod$plot(dat, theme = theme_light(base_size = 5), size = 1) +
  ggtitle(glue::glue("Effect size: {round(mod$effect_size(dat), 2)}")) -> p4

list(p1, p2, p3, p4) %>% patchwork::wrap_plots(ncol=2)
```

Under the cubic model setting, there is an exact conventional test for testing the nonlinearity defect, which is F-test. For F-test, the null hypothesis is $H_0:a=b=0$, and the alternative hypothesis is $H_1:\text{at least one of them} \neq 0$. During the simulation of the lineup data, the F-statistic and the p-value will be recorded for comparison between the power of conventional test and visual test.

### Heteroskedasticity Model

Another model used in the experiments was a heteroskedasticity model with one regressor, which can be expressed by: 
\begin{equation} \label{eq:heter}
Y_i = 1 + X_i + \varepsilon_i, ~i = 1,...,n,
\end{equation}
where $a \in \{-1,0,1\}$, $b\in (0,32)$ and $\varepsilon_i \overset{iid}{\sim} N(0,1+b(2-|a|)(X_i-a)^2)$.

To create heteroskedasticity defect, OLS was used to fit the null model:
\begin{equation} \label{eq:heter_null}
\boldsymbol{Y}=\beta_0+\beta_1\boldsymbol{X}+\boldsymbol{u},
\end{equation}
where $\boldsymbol{u} \sim N(\boldsymbol{0}, \sigma^2_u\boldsymbol{I})$.

In this case, estimators of $\beta_0$ and $\beta_1$ are unbiased, but the error term has non-constant variance.

\begin{lemma}[Distribution of residuals produced by the heteroskedasticity model] \label{lemma:heter}
Given the data generating process in Equation (\ref{eq:heter}) and null model in Equation (\ref{eq:heter_null}). Let $\boldsymbol{X}_a=[\boldsymbol{1},\boldsymbol{X}]$ denotes the set of regressors in matrix form. The residuals obtained from the null model are $$\boldsymbol{e} \sim N(\boldsymbol{0}, \boldsymbol{R}_a\boldsymbol{V}),$$ where $\boldsymbol{R}_a=\boldsymbol{I}-\boldsymbol{X}_a(\boldsymbol{X}_a'\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a'$ and $\boldsymbol{V}$ is a diagonal matrix with $V_{ii}=1+b(2-|a|)(X_i  -  a)^2$, $i = 1,...,n$. 
\end{lemma}

\begin{proof}
Using the Frisch–Waugh–Lovell theorem, the residuals obtained by the null model are $\boldsymbol{e}=\boldsymbol{R}_a\boldsymbol{Y}=\boldsymbol{R}_a(\boldsymbol{X}_a\boldsymbol{\beta}_a+\boldsymbol{\varepsilon}),$ where $\boldsymbol{R}_a=\boldsymbol{I}-\boldsymbol{X}_a(\boldsymbol{X}_a'\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a'$ and $\boldsymbol{\beta}_a=(1,1)'$.

Because $\boldsymbol{R}_a\boldsymbol{X}_a=\boldsymbol{0}$, we have $\boldsymbol{e}=\boldsymbol{R}_a\boldsymbol{\varepsilon}$. Hence, the residuals $\boldsymbol{e}$ follow $N(\boldsymbol{0}, \boldsymbol{R}_a\boldsymbol{V})$, where $\boldsymbol{V}$ is a diagonal matrix with $V_{ii}=1+b(2-|a|)(X_i  -  a)^2$, $i = 1,...,n$. 
\end{proof}

Lemma \@ref(lemma:heter) shows that variance and covariance of the residuals depend on $\boldsymbol{X}$. We could plot the one standard deviation around the residuals to indicate the region about 68% of the residuals should landed on. Figure \@ref(fig:hetershape) illustrates different shapes of residuals undder various values of $a$ and $b$. From the plot, it can be observed that if $a = 0$, the residual plot looks like the shape of butterfly. If $a = \pm 1$, it looks like a triangle, and the sign of $a$ determines the direction of the shape. Parameter $b$ is controlling the strength of the signal. As $b$ increases, the pattern becomes less noisy.

```{r hetershape, fig.height=5.25, fig.cap="A matrix of residuals vs. fitted values plot under different parameter settings of the heteroskedasticity model. Resdiauls are drawns in black and the one standard deviation around zero is drawn in red. The plots show that $a$ controls the shape and direction of the resdiaul plot, while $b$ controls the strength of the signal."}
make_plot2 <- function(a, b) {
  mod <- heter_model(a = a, b = b)
  dat <- mod$gen(1000, fit_model = TRUE)
  dat$one_sigma <- sqrt(1 + (2 - abs(a)) * (dat$x - a)^2 * b)
  mod$plot(dat, alpha = 0.3, size = 0.5, add_zero_line = FALSE, theme = theme_light(base_size = 5)) +
    geom_point(aes(.fitted, one_sigma, col = "red"), alpha = 0.6, size = 0.1) +
    geom_point(aes(.fitted, -one_sigma, col = "red"), alpha = 0.6, size = 0.1) +
    xlab("fitted values") +
    ylab("residuals") +
    ggtitle(bquote(a:.(a)~','~b:.(b))) +
    theme(legend.position = "none")
}


expand.grid(a = c(-1,0,1),
            b = 2^c(0,2,4)) %>%
  arrange(b,a) %>%
  rowwise() %>%
  mutate(plots = list(make_plot2(a = a, b = b))) -> results

results$plots %>% patchwork::wrap_plots(ncol=3)
```

Similar to the cubic model, the heteroskedasticity model could also be built by the heteroskedasticity model class constructor `heter_model()`. This function takes three arguments as inputs, which are `a`, `b` and `x`. `a` and `b` are numeric parameters defined in Equation (\ref{eq:heter}). `x` needs to be a random variable object.

```{r echo = TRUE}
library(visage)
mod <- heter_model(a = 0, b = 16, x = rand_uniform(-1, 1))
```

Since both the cubic model class and the heteroskedasticity class are inherited from the visual inference model class, which has defined methods of simulating data, making residual plot and producing lineup, heteroskedasticity model object can be used in a similar way as cubic model object. The following codes give examples of the use of the object.

```{r echo = TRUE}
mod$gen(5, fit_model = TRUE)
```

```{r echo = TRUE}
mod$plot(mod$gen(100, fit_model = TRUE), size = 1)
```
```{r echo = TRUE}
mod$plot_lineup(mod$gen_lineup(100), size = 1)
```
According to Lemma \ref{lemma:heter}, when $b = 0$, the matrix $\boldsymbol{V}$ collapses to an identity matrix. Assume the shape of butterfly and triangle have identical visual impacts, the only factor affects human to recognize the pattern is the strength of the signal, where parameter $a$ has no role to play. In addition, sample size has a huge impact on the chance of human recognizing the pattern. As shown in Figure \@ref(fig:hetern), as sample size increases, the pattern becomes easier to observe. However, this effect is less noticeable with large sample size as the outline of the shape has been drawn and residuals have less probability to land outside of it. Thus, the effect size of this model can be expressed by $ES = b\sqrt{n}$. The square root operator is used for addressing the large sample issue. Figure \@ref(fig:hetereffectsize) shows the effectiveness of the effect size.


```{r hetern, fig.height=1.9, fig.cap="Residuals generated from the same heteroskedasticity model but with different sample size. As sample size increases, the shape of butterfly becomes more obvious."}
mod$plot(mod$gen(10, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle("n: 10") -> p1
mod$plot(mod$gen(50, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle("n: 50") -> p2
mod$plot(mod$gen(100, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle("n: 100") -> p3
mod$plot(mod$gen(500, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle("n: 500") -> p4

list(p1, p2, p3, p4) %>% patchwork::wrap_plots(ncol=4)
```

```{r hetereffectsize, fig.cap="Residuals of heteroskedasticity model with different effect size. As effect size increases, the shape of butterfly becomes less noisy."}
set.seed(10086)
mod <- heter_model(0, 1)
mod$plot(mod$gen(500, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle(glue::glue("Effect size: {round(mod$effect_size(mod$gen(500)),2)}")) -> p1
mod <- heter_model(0, 3)
mod$plot(mod$gen(500, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle(glue::glue("Effect size: {round(mod$effect_size(mod$gen(500)),2)}")) -> p2
mod <- heter_model(0, 9)
mod$plot(mod$gen(500, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle(glue::glue("Effect size: {round(mod$effect_size(mod$gen(500)),2)}")) -> p3
mod <- heter_model(0, 81)
mod$plot(mod$gen(500, fit_model = TRUE), size = 1, theme = theme_light(base_size = 5)) + ggtitle(glue::glue("Effect size: {round(mod$effect_size(mod$gen(500)),2)}")) -> p4

list(p1, p2, p3, p4) %>% patchwork::wrap_plots(ncol=2)
```

For the heteroskedasticity model, the conventional test we used was Breusch–Pagan test [@breusch_simple_1979], which tested whether the variance of the error terms of the regression is dependent on the regressors with the auxiliary regression equation $$\boldsymbol{e}^2 = \gamma_0 + \gamma_1 \boldsymbol{X} + \gamma_2 \boldsymbol{X}^2 + \boldsymbol{v}.$$ @majumder_validation_2013 suggested that visual test is not expected to perform equally well as conventional test especially when there exists a exact conventional test. However, in contrast to the F-test used in the cubic model, Breusch–Pagan test is an approximate test. Thus, the power of visual test may exceed the power of Breusch–Pagan test. Throughout the study, Breusch–Pagan test statistic and p-value were recorded for comparison.

### Distribution of regressors

The model definitions given in the previous two sections does not include the specification of the regressors. In this sections, distribution of $\boldsymbol{X}$ and $\boldsymbol{Z}$ will be discussed. 

The cubic model involved the use of both $\boldsymbol{X}$ and $\boldsymbol{Z}$. In the simulation, $X_i$, $i=1,...,n$, had equal chance to follow one of the following distributions: $N(0, 0.09)$, $U(-1, 1)$, $Lognormal(0,0.36)/3 - 1$ and $-Lognormal(0,0.36)/3 + 1$. Uniform and normal distribution were symmetric and common. Adjusted lognormal distribution and adjusted negative lognormal distribution provided right-skewed and left-skewed density respectively. These distributions were chosen such that most the realizations will fall between $-1$ and $1$.

The distribution of $Z_i$, $i=1,...,n$, had $50$% chance to be a uniform distribution ranged from $-1$ to $1$, and $50$% chance to be a discrete uniform distribution with $z_n$ outcomes simulated from a uniform distribution ranged from $-1$ to $1$. $z_n$ itself was a discrete uniform distribution with outcomes $\{10,11,12,13,14,15,16,17,18,19,20\}$, which defined the number of possible values $Z_i$ could take. As shown in Figure \@ref(fig:discreteex), this setup would create discreteness in residual plot, which could enrich the pool of visual patterns.

```{r discreteex, fig.cap="Discreteness in residuals created by using a discrete uniform random variable as one of the regressors in a cubic model. For each residual plot of the lineup, there are three clusters because the number of possible values the regressor can take is three."}
set.seed(10093)
mod <- cubic_model(a = 2, b = -2, c = 1, z = rand_uniform_d(-1, 1, k = 3))
mod$plot_lineup(mod$gen_lineup(500), theme = theme_light(), alpha = 0.3)
```

$\boldsymbol{X}$ used in the heteroskedasticity model was a combination of $\boldsymbol{X}$ and $\boldsymbol{Z}$ used in the cubic model. It could be one of the five distributions mentioned above - normal distribution, uniform distribution, adjusted lognormal distribution, adjusted negative lognormal distribution and discrete uniform distribution. 

### Experiment I and II

In this study, multiple selection was allowed for the evaluation of a lineup. Given our desired significance level was $\alpha = 0.05$ and the number of plots in a lineup was $20$, a single visual test procedure needed to involve multiple evaluations. Otherwise, the $p$-value of the test would be greater than $0.05$ in spite of the actual data plot being selected. The number of plots selected by a user depended on the user and the difficulty of the lineup. According to the pilot study evaluated by  $10$ faculty members of Department of Econometrics and Business Analytics, Monash University, the number of selections would be generally smaller than $4$, which suggested a visual test consisted of $5$ evaluations were sufficient to yield $p$-value smaller than $0.05$ with at least three detections. Thus, each lineup was replicated for five times and evaluated by different participants. However, our study website was designed such that any set of lineups consisting of 20 lineups generated beforehand would be shown to only one person only once. And whether the participant would finish the experiment was unpredictable. Therefore, some lineups had insufficient number of evaluations, but some had more evaluations than expected. This slightly affected the estimate of the power of the visual test, which will be discussed in Section \@ref(p-value-and-power-estimation-of-visual-test-allowed-for-multiple-selections).

We called the lineup being detected if the actual data plot was one of the selections of an evaluation of a lineup. The sample size calculation was based on the detection rate of the lineup, which was closely related to the difficulty level of the lineup. With the data collected from the pilot study, two logistic regressions given in Table \@ref(tab:pilotglm) and Figure \@ref(fig:pilotglm) were developed to describe the relationship between the natural logarithm of effect size and the detection rate for the cubic model and the heteroskedasticity model.


```{r }
file_names <- list.files(here::here("data/pilot_study"))

library(tidyverse)
result_high <- NULL
for (file_name in file_names[grep("oct" , file_names)]) {
  dat <- read_csv(here::here("data/pilot_study", file_name))
  if (is.null(result_high)) {
    result_high <- dat
  } else {
    result_high <- bind_rows(result_high, dat)
  }
}

result_high %>%
  mutate(detect = as.numeric(answer)) %>%
  ggplot() +
  ggbeeswarm::geom_quasirandom(aes(log(effect_size),
                                   detect),
                               groupOnX = FALSE) +
  geom_smooth(aes(log(effect_size), detect, group = 1),
              method = "glm", method.args = list(family = binomial)) +
  xlab("Log of effect size") +
  ylab("Detection rate") +
  ggtitle("Cubic Model") -> p1

mod1 <- glm(detect ~ log(effect_size), data = mutate(result_high, detect = as.numeric(answer), log_effect_size = log(effect_size)), family = binomial)

```

```{r}
file_names <- list.files(here::here("data/pilot_study"))

library(tidyverse)
result_heter <- NULL
for (file_name in file_names[grep("nov" , file_names)]) {
  dat <- read_csv(here::here("data/pilot_study", file_name))
  if (is.null(result_heter)) {
    result_heter <- dat
  } else {
    result_heter <- bind_rows(result_heter, dat)
  }
}

result_heter %>%
  mutate(detect = as.numeric(answer)) %>%
  ggplot() +
  ggbeeswarm::geom_quasirandom(aes(log(effect_size),
                                   detect),
                               groupOnX = FALSE) +
  geom_smooth(aes(log(effect_size), detect, group = 1),
              method = "glm", method.args = list(family = binomial)) +
  xlab("Log of effect size") +
  ylab("Detection rate") +
  ggtitle("Heteroskedasticity Model") -> p2
```

```{r results='asis'}
library(stargazer)
mod2 <- glm(detect ~ log(effect_size), data = mutate(result_heter, detect = as.numeric(answer), log_effect_size = log(effect_size)), family = binomial)
stargazer(mod1, mod2,
          header = FALSE,
          label = "tab:pilotglm",
          column.labels = c("Cubic", "Heteroskedasticity"),
          title = "Logistic regressions with detection rate as response variable and natural logarithm of the effect size as regressor fitted on the pilot data for both cubic and heteroskedasticity models.",
          style = "default")

```

```{r pilotglm, fig.cap="Logistic regressions with detection rate as response variable and natural logarithm of the effect size as regressor fitted on the pilot data for both cubic and heteroskedasticity models. Points are jittered to reduce overplotting using the `ggbeeswarm` package. The smooth curves are the fitted regression curves. The ribbons around the curves are the 95% confidence intervals."}
list(p1, p2) %>% patchwork::wrap_plots(ncol = 2)
```

Lineups were classified into three categories - easy, medium and hard. Easy lineups were those of which predicted detection rates were higher than $80$%, while medium lineups were between $40$% to $80$% and hard lineups were less than $40$%. 

For the first experiment, we would like to have at least 20 detection with chance greater than 99.99% in each category. Thus, we prepared $68$ easy lineups, $120$ medium lineups and $388$ hard lineups with five replications. Every participant would get at least $8$ easy or medium lineups. Two out of $10$ extremely lineups with predicted detection rate over $90$% were randomly given to every participant as attention checks. We initially planned to recruit $(68+120+388) \times 5 / (20-2) = 160$ participants but eventually decided to only recruit $20$ participants because it was the first time we launched an experiment on Prolific where factors like payment method and quality of participants were unclear at the moment. As a consequence, most of the lineups had only two evaluations and not all the lineups were used. However, data collected from the first experiment was sufficient for building a more accurate logistic regression to predict detection rate for the second experiment.

```{r}
process_small_responses <- function(k, lineup_dat, lineup_ord, survey_folder) {

  dat <- tibble(id = 1:length(unlist(lineup_ord)),
                lineup_dat_id = unlist(lineup_ord)) %>%
    mutate(set = (id-1) %/% k + 1) %>%
    mutate(num = (id-1) %% k + 1)

  user_info_dict <- list(c("18-24", "25-39", "40-54", "55-64", "65 or above"),
                         c("High School or below", "Diploma and Bachelor Degree", "Honours Degree", "Masters Degree", "Doctoral Degree"),
                         c("He", "She", "They", "Other"),
                         c("Yes", "No"))

  get_responses <- function(user_info_dict, uuid) {
    tmp_survey <- jsonlite::fromJSON(here::here(glue::glue("{survey_folder}/{uuid}.txt")),
                                     simplifyVector = FALSE)
    response_time <- map_dbl(tmp_survey, ~.x$rt)
    user_information <- str_split(tmp_survey[[3]]$response[[1]], ",")[[1]]

    user_information[2:5] <- imap_chr(user_information[-1], ~user_info_dict[[.y]][as.integer(.x)])
    names(user_information) <- c("prolific_id", "age_group", "education", "pronoun", "previous_experience")


    lineup_respone <- str_split(map_chr(tmp_survey[-c(1,2,3,4)], ~.x$response$response), ",")
    # handle selections
    selections <- map_chr(lineup_respone, ~.x[1])
    # handle reasons
    reasons_dict <- c("Outlier(s)", "Cluster(s)", "Shape")

    reasons <- map_chr(lineup_respone, ~if(.x[2] %in% c("1", "2", "3")) {
      reasons_dict[as.integer(.x[2])]
    } else {
      .x[2]
    })

    # handle confidence
    confidence_dict <- c("Not at all", "Slightly", "Moderately", "Very", "Extremely")
    confidence <- map_chr(lineup_respone, ~if(.x[3] %in% c("1", "2", "3", "4", "5")) {
      confidence_dict[as.integer(.x[3])]
    } else {
      .x[3]
    })

    tibble(page = 1:length(response_time),
           response_time = response_time,
           set = uuid,
           num = c(rep(NA, 4), 1:k),
           selection = c(rep(NA, 4), selections),
           num_selection = c(rep(NA, 4), str_count(selections, "_") + 1),
           reason = c(rep(NA, 4), reasons),
           confidence = c(rep(NA, 4), confidence),
           prolific_id = unname(user_information['prolific_id']),
           age_group = unname(user_information['age_group']),
           education = unname(user_information['education']),
           pronoun = unname(user_information['pronoun']),
           previous_experience = unname(user_information['previous_experience'])
    ) %>%
      mutate(reason = ifelse(selection == "NA", NA, reason)) %>%
      mutate(confidence = ifelse(selection == "NA", NA, confidence)) %>%
      mutate(selection = ifelse(selection == "NA", "0", selection)) %>%
      mutate(num_selection = ifelse(selection == "0", 0, num_selection))
  }

  response_dat <- NULL
  file_names <- list.files(here::here(survey_folder))

  for (i in sort(as.integer(str_replace(file_names[grep("*.txt", file_names)], ".txt", "")))) {
    if (is.null(response_dat)) {
      response_dat <- get_responses(user_info_dict, i)
    } else {
      response_dat <- bind_rows(response_dat, get_responses(user_info_dict, i))
    }
  }

  correct_or_not <- function(selection, ans) {
    tmp <- imap_lgl(selection, ~ans[.y] %in% str_split(.x, "_")[[1]])
    tmp[is.na(ans)] <- NA
    tmp
  }

  extract_p_value <- function(lineup_id, lineup_dat, null = FALSE) {
    if (is.na(lineup_id)) {
      return(NA)
    }

    tmp <- lineup_dat[[lineup_id]]$data %>%
      group_by(k) %>%
      summarise(null = first(null), pvalue = first(pvalue)) %>%
      ungroup()

    data_p <- tmp$pvalue[tmp$null == FALSE]
    null_min_p <- min(tmp$pvalue[tmp$null == TRUE])
    if (null) {
      return(null_min_p)
    } else {
      return(data_p)
    }
  }

  response_dat %>%
    rowwise() %>%
    mutate(lineup_id = dat$lineup_dat_id[dat$set == set & dat$num == num][1]) %>%
    mutate(metadata = list(lineup_dat[[lineup_id]]$metadata)) %>%
    ungroup() %>%
    unnest_wider(metadata) %>%
    mutate(age_group = factor(age_group,
                              levels = user_info_dict[[1]])) %>%
    mutate(education = factor(education,
                              levels = user_info_dict[[2]])) %>%
    mutate(confidence = factor(confidence,
                               levels = c("Not at all", "Slightly", "Moderately", "Very", "Extremely"))) %>%
    mutate(detected = correct_or_not(selection, ans)) %>%
    rowwise() %>%
    mutate(data_p_value = extract_p_value(lineup_id, lineup_dat, null = FALSE)) %>%
    mutate(null_min_p_value = extract_p_value(lineup_id, lineup_dat, null = TRUE)) %>%
    ungroup()
}
```


```{r}
lineup_dat <- readRDS(here::here("data/small_study/all_data.RDS"))
lineup_ord <- readRDS(here::here("data/small_study/lineup_order.RDS"))

# Correct the effect size
for (i in 1:length(lineup_dat)) {
  if (lineup_dat[[i]]$metadata$type != "higher order") next
  
  tmp_eff <- CUBIC_MODEL$effect_size(select(filter(lineup_dat[[i]]$data, null == FALSE), x, z), 
                          a = lineup_dat[[i]]$metadata$a, 
                          b = lineup_dat[[i]]$metadata$b,
                          c = lineup_dat[[i]]$metadata$c,
                          sigma = lineup_dat[[i]]$metadata$e_sigma)
  
  lineup_dat[[i]]$metadata$effect_size <- tmp_eff 
}

small_survey_dat <- process_small_responses(20, 
                                            lineup_dat = lineup_dat, 
                                            lineup_ord = lineup_ord, 
                                            survey_folder = "data/small_study/survey") %>%
  filter(page > 4) %>%
  mutate(difficulty = str_replace_all(name, "(high_|heter_|_\\d+)", "")) %>%
  mutate(difficulty = factor(difficulty, levels = c("extremely_easy", "easy", "medium", "hard")))

# Delete set 18

small_survey_dat <- filter(small_survey_dat, set != 18)
```

```{r}
small_survey_dat$detect <- small_survey_dat$detected

e1_high <- glm(detect ~ log(effect_size),
                data = filter(small_survey_dat, type == "higher order"),
                family = binomial)
e1_heter <- glm(detect ~ log(effect_size),
                 data = filter(small_survey_dat, type == "heteroskedasticity"),
                 family = binomial)
```

```{r e1glmplot, fig.cap="Logistic regressions with detection rate as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I for both cubic and heteroskedasticity models. Points are jittered to reduce overplotting using the `ggbeeswarm` package. The smooth curves are the fitted regression curves. The ribbons around the curves are the 95% confidence intervals."}
filter(small_survey_dat, type == "higher order") %>%
  mutate(detect = as.numeric(detect)) %>%
  ggplot() +
  ggbeeswarm::geom_quasirandom(aes(log(effect_size),
                                   detect),
                               groupOnX = FALSE) +
  geom_smooth(aes(log(effect_size), detect, group = 1),
              method = "glm", method.args = list(family = binomial)) +
  xlab("Log of effect size") +
  ylab("Detection rate") +
  ggtitle("Cubic Model") -> p1

filter(small_survey_dat, type == "heteroskedasticity") %>%
  mutate(detect = as.numeric(detect)) %>%
  ggplot() +
  ggbeeswarm::geom_quasirandom(aes(log(effect_size),
                                   detect),
                               groupOnX = FALSE) +
  geom_smooth(aes(log(effect_size), detect, group = 1),
              method = "glm", method.args = list(family = binomial)) +
  xlab("Log of effect size") +
  ylab("Detection rate") +
  ggtitle("Heteroskedasticity Model") -> p2

list(p1, p2) %>% patchwork::wrap_plots(ncol = 2)
```


```{r results="asis"}
stargazer(e1_high, e1_heter,
          header = FALSE,
          label = "tab:e1glm",
          column.labels = c("Cubic", "Heteroskedasticity"),
          title = "Logistic regressions with detection rate as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I for both cubic and heteroskedasticity models.",
          style = "default")
```

According to the new logistic regression given in Table \@ref(tab:e1glm) and Figure \@ref(fig:e1glmplot), it was found that the first experiment might be too difficult for participants as the detection rate was lower than what we estimated from the pilot study, especially for the heteroskedasticity model. Thus, for the second experiments, we relaxed the sample size requirements and increased the proportion of easy lineups. In addition, due to budget constraint, the maximum number of participants was $50$. With the new logistic regression fitted with data collected from the first experiment, $28$ easy lineups, $36$ medium lineups and $100$ hard lineups were simulated for the second experiment. $50$ participants were recruited but $6$ were withdrew.


### $P$-value and Power Estimation of Visual Test Allowed for Multiple Selections

The $p$-value calculation for multiple selection is an extension of Equation (\ref{eq:pvaluesingle}). The assumptions about the independence still needed. Let $K$ be the number of independent evaluations of a visual test, $s_i$, $i = 1,...,K$ be the number of selections of the evaluations, and $X$ be the random variable denoting the number of detections. Then, the $p$-value of the visual test is given as:
\begin{equation} \label{eq:pvaluemulti}
P(X\geq x) = \sum_{j=x}^{K}Pr(j|s_1,...,s_K).
\end{equation}
The distribution of $X$ given $s_1,...s_K$ can not be derived trivially, as it is a sampling without replacement problem. In practice, this distribution can be approximated by computer simulation. 

Function `sim_dist()` from package `visage` is designed to approximate the distribution of number of detections of a lineup via Monte Carlo method. It takes the number of evaluations `n_eval`, the number of selections `n_sel`, and the number of plots in a lineup `n_plot` as inputs, then outputs a discrete distribution.

```{r echo = TRUE}
sim_dist(n_eval = 3, n_sel = c(2, 2, 3), n_plot = 20)
```

Function `calc_p_value()` from the same package calculates the $p$-value using the distribution returned by `sim_dist()`. It takes an additional argument `n_detect`, which is the number of detections.

```{r echo = TRUE}
calc_p_value(n_detect = 2, n_eval = 3, n_sel = c(2, 2, 3), n_plot = 20)
```

The above example shows that a visual test with two detections and three independent evaluations in which three observers select two, two, and three plots out of $20$ plots respectively yields a $p$-value smaller than $0.05$.

Assume there is a visual test $V_K$, where $K$ denoting the number of evaluations. The corresponding  $p$-value of $V_K$ can be computed by using Equation (\ref{eq:pvaluemulti}). Meanwhile, if one evaluation is randomly deleted from $V_K$, the remaining evaluations can still be used to form another valid visual test $V_{K-1}$. In fact, considering all the possibilities, $K$ different outcomes for $V_{K-1}$ can be obtained. Since all outcomes occur with equal probability, the proportion of outcomes which reject the null hypothesis can be used as an estimate of the power of the visual test $V_{K-1}$. Similarly, if we would like to estimate the power of the visual test $V_{K-j}$ given the evaluations of $V_{K}$, for $j < K$, we could find all the possible combinations of $K$ elements, taken $k-j$ at a time to obtain $K\choose{K-j}$ different outcomes for $V_{K-j}$. Then, the estimated power is given as $R/{K\choose{K-j}}$, where $R$ is the number of outcomes which reject the null hypothesis.

Function `calc_p_value_comb()` from the package `visage` can be used to compute $p$-values of $V_{K-j}$. The first argument is `detected`, which needs to be a vector of Boolean values denoting whether the observer detects the actual data plot. Desired number of evaluations and number of selections need to be provided via argument `n_eval` and `n_sel`.

For example, given a visual test $V_3$ where the first observer selects one plot and gets correct, the second observer selects one plot but misses, and the third observer selects two plots and gets correct. The $p$-value of all possible outcomes of $V_2$ can be computed using the following code: 

```{r}
set.seed(10086)
```


```{r echo = TRUE}
calc_p_value_comb(detected = c(TRUE, FALSE, TRUE), 
                  n_eval = 2, 
                  n_sel = c(1, 1, 2))
```

`calc_p_value_comb()` returns a vector of $p$-value along with an attribute which is a matrix representing elements being used by the specific outcome. Elements in the first column indicates that the first observer and the second observer are involved in the visual test, and the corresponding $p$-value is $0.09352$. 


### Results



```{r}
lineup_dat_big <- readRDS(here::here("data/big_study/all_data_big.RDS"))
lineup_ord_big <- readRDS(here::here("data/big_study/lineup_order_big.RDS"))

# Correct the effect size
for (i in 1:length(lineup_dat_big)) {
  if (lineup_dat_big[[i]]$metadata$type != "higher order") next
  
  tmp_eff <- CUBIC_MODEL$effect_size(select(filter(lineup_dat_big[[i]]$data, null == FALSE), x, z), 
                          a = lineup_dat_big[[i]]$metadata$a, 
                          b = lineup_dat_big[[i]]$metadata$b,
                          c = lineup_dat_big[[i]]$metadata$c,
                          sigma = lineup_dat_big[[i]]$metadata$e_sigma)
  
  lineup_dat_big[[i]]$metadata$effect_size <- tmp_eff 
}

big_survey_dat <- process_small_responses(20, 
                                            lineup_dat = lineup_dat_big, 
                                            lineup_ord = lineup_ord_big, 
                                            survey_folder = "data/big_study/survey") %>%
  filter(page > 4) %>%
  mutate(difficulty = str_replace_all(name, "(high_|heter_|_\\d+)", "")) %>%
  mutate(difficulty = factor(difficulty, levels = c("extremely_easy", "easy", "medium", "hard")))

big_survey_dat$detect <- big_survey_dat$detected
```

```{r eval= FALSE}
# Cubic 1
bind_rows(
small_survey_dat %>%
  filter(type == "higher order") %>%
  mutate(p_hat = predict(mod1, newdata = data.frame(effect_size), type = "response")) %>%
  mutate(new_diff = ifelse(p_hat < 0.4, "hard", ifelse(p_hat > 0.8, "easy", "medium"))),

# Heter 1
small_survey_dat %>%
  filter(type != "higher order") %>%
  mutate(p_hat = predict(mod2, newdata = data.frame(effect_size), type = "response")) %>%
  mutate(new_diff = ifelse(p_hat < 0.4, "hard", ifelse(p_hat > 0.8, "easy", "medium")))
) %>%
  ggplot() +
  geom_bar(aes(new_diff)) +
  facet_wrap(~set)
  # mutate(me = new_diff == "easy" | new_diff == "medium") %>%
  # count(set, me) %>%
  # filter(me == TRUE) %>%
  # arrange(n)

# total
# 94, 184, 122
# easy, hard, medium

# unique lineups
# 50, 147, 90
# easy, hard, medium
  
# at least 8 medium or easy
```


```{r eval = FALSE}
e1_heter_2 <- glm(detect ~ effect_size,
                 data = filter(small_survey_dat, type == "heteroskedasticity"),
                 family = binomial)

bind_rows(
# Cubic 2
big_survey_dat %>%
  filter(type == "higher order") %>%
  mutate(p_hat = predict(e1_high, newdata = data.frame(effect_size), type = "response")) %>%
  mutate(new_diff = ifelse(p_hat < 0.4, "hard", ifelse(p_hat > 0.8, "easy", "medium"))),

# Heter 2
big_survey_dat %>%
  filter(type != "higher order") %>%
  mutate(p_hat = predict(e1_heter_2, newdata = data.frame(effect_size), type = "response")) %>%
  mutate(new_diff = ifelse(p_hat < 0.4, "hard", ifelse(p_hat > 0.8, "easy", "medium")))
) %>%
  mutate(me = new_diff == "easy" | new_diff == "medium") %>%
  count(set, me) %>%
  filter(me == TRUE) %>%
  arrange(n)
```


```{r}
merged_survey <- bind_rows(mutate(small_survey_dat, exp = 1), mutate(big_survey_dat, exp = 2))
merged_survey <- merged_survey %>%
  mutate(type = ifelse(type == "higher order", "cubic", type))
```

We collected $400$ lineup evaluations made by $20$ participants in experiment I and $880$ lineup evaluations made by $44$ participants in experiment II. In total, $442$ unique lineups were evaluated by $64$ subjects. In experiment I, one of the participants skipped all $20$ lineups. Hence, the submission was rejected and removed from the dataset. In experiment II, there was a participant failed one of the two attention checks, but there was no further evidence of low-effort throughout the experiment. Therefore, the submission was kept. 

#### Power Estimation

It was expected that with larger effect size, the power of the visual test would be greater for both cubic model and heteroskedasticity model. Using the power calculation method discussed in Section \@ref(p-value-and-power-estimation-of-visual-test-allowed-for-multiple-selections), power of each lineup from one to five evaluations was estimated. The estimated power was further used in fitting a quasi-binomial generalized linear model with natural logarithm of effect size as the only regressor. The result of the fitted models are given in Table \@ref(tab:powerglmcubic), Table \@ref(tab:powerglmheter), and Figure \@ref(fig:powerglm). 

As expected, coefficients of natural logarithm of effect size of all five models were positive. From Figure \@ref(fig:powerglm), it can be observed that the fitted power of visual test increased as the number of evaluations increased for both cubic and heteroskedasticity model. 

For heteroskedasticity model, this phenomenon was more obvious as the curves of visual tests with evaluations greater than two were always above the curves of visual test with evaluations smaller than two. However, this only held for large enough effect size. For small effect size, visual tests with fewer evaluations might have greater power. Note that, the expected power of visual test derived by @majumder_validation_2013 followed by the assumption that human has the ability to select the plot with the highest t-statistic from a lineup under the classical linear model regression setting showed similar properties, where visual tests with fewer evaluations were expected to perform better when the parameter values close to the null hypothesis. 

For cubic model, the separation between curves was small. Fitted power of visual test with three to five evaluations were almost identical to each other in regards of effect size. In addition, all five curves peaked at one as effect size increased, suggesting that identification of nonlinearity as a visual task can be completed reliably by human when the effect size is large enough.  

As shown in Figure \@ref(fig:powerglm), both F-test and Breusch–Pagan test generally possessed greater power than visual test. Since the settings we were using were not ideal scenarios for visual inference, it was not expected that the visual test would be as good as the conventional test. Our goal was to quantify the ability of human reading residual plot, such that the comparison between human and computer vision model can be made.

```{r}
env <- new.env()

comb_pvalue <- merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  calc_p_value_multi(lineup_id = "exp_lineup_id", 
                     detected = "detected", 
                     n_sel = "num_selection",
                     comb = TRUE,
                     n_eval = 1:5,
                     n_sim = 100000,
                     cache_env = env) %>%
  rowwise() %>%
  mutate(hat_power = mean(eval_p_value(p_value, tol = 1e-2))) %>%
  ungroup() %>%
  select(exp_lineup_id = lineup_id, p_value, n_eval, hat_power)
```

```{r results="asis"}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  select(exp_lineup_id, type, exp, effect_size) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = factor(n_eval)) -> tmp_dat

map(1:5, function(x) glm(hat_power ~ log(effect_size),
                         family = quasibinomial(),
                         data = filter(tmp_dat, n_eval == {{x}}, type == "cubic"))) %>%
  stargazer(header = FALSE,
            label = "tab:powerglmcubic",
            title = "Quasi-binomial logistic regressions with estimated power as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for cubic model. Five regressions were fitted for different number of evaluations.",
            style = "default")

```

```{r results="asis"}
map(1:5, function(x) glm(hat_power ~ log(effect_size),
                         family = quasibinomial(),
                         data = filter(tmp_dat, n_eval == {{x}}, type == "heteroskedasticity"))) %>%
  stargazer(header = FALSE,
            label = "tab:powerglmheter",
            title = "Quasi-binomial logistic regressions with estimated power as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for heteroskedasticity model. Five regression were fitted for different number of evaluations.",
            style = "default")

```

```{r}
conv_test_cubic <- function() {
  para_list1 <- list(a = runif(1, -3, 3),
                     b = runif(1, -3, 3),
                     c = runif(1, 0, 2),
                     x_dist = sample(c("uniform", "normal", "lognormal", "neglognormal"), 1),
                     x_mu = 0,
                     x_n = NA,
                     x_sigma = sample(c(0.3, 0.6), 1),
                     z_discrete = sample(c(TRUE, FALSE), 1),
                     z_discrete_dist = "discrete_uniform",
                     z_n = sample(3:10, 1),
                     e_dist = "normal",
                     e_df = NA,
                     e_sigma = sample(c(0.25, 0.5, 1, 2), 1),
                     n = sample(c(50, 100, 300), 1))

  if (para_list1$x_dist == "normal") {para_list1$x_sigma <- 0.3} else {para_list1$x_sigma <- 0.6}

  # Init X and Z based on the parameter list
  x <- switch (para_list1$x_dist ,
               "uniform" = rand_uniform(-1, 1),
               "normal" = rand_normal(0, para_list1$x_sigma),
               "lognormal" = rand_lognormal(0, para_list1$x_sigma),
               "neglognormal" = {rl <- rand_lognormal(0, para_list1$x_sigma); closed_form(~-rl)}
  )

  if (para_list1$z_discrete) {z <- rand_uniform(-1, 1)} else {z <- rand_uniform_d(-1, 1, para_list1$z_n)}


  # Build the cubic model
  mod <- cubic_model(a = para_list1$a, b = para_list1$b, c = para_list1$c,
                     sigma = para_list1$e_sigma,
                     x = x, z = z)

  # Generate data from the model, let k = 2 (50% true data plot, 50% null plot)
  dat <- mod$gen(para_list1$n, fit_model = TRUE, test = TRUE)
  
  c(mod$effect_size(dat), dat$p_value[1])
}

conv_result_cubic <- map(1:5000, ~conv_test_cubic())

conv_result_cubic <- data.frame(effect_size = map_dbl(conv_result_cubic, ~.x[1]), 
                          p_value = map_dbl(conv_result_cubic, ~.x[2])) %>%
  mutate(reject = p_value < 0.05) %>%
  mutate(reject = as.numeric(reject))

conv_result_cubic <- conv_result_cubic %>%
  filter(log(effect_size) > -5) %>%
  mutate(type = "cubic")
```

```{r}
conv_test_heter <- function() {
  para_list1 <- list(a = sample(c(-1, 0, 1), 1),
                     b = runif(1, 0, 32),
                     c = NA,
                     x_dist = sample(c("uniform", "normal", "lognormal", "neglognormal", "discrete_uniform"), 1),
                     x_mu = 0,
                     x_n = sample(10:20, 1),
                     x_sigma = sample(c(0.3, 0.6), 1),
                     z_discrete = FALSE,
                     z_discrete_dist = NA,
                     z_n = NA,
                     e_dist = NA,
                     e_df = NA,
                     e_sigma = NA,
                     n = sample(c(50, 100, 300), 1))

  if (para_list1$x_dist == "normal") {para_list1$x_sigma <- 0.3} else {para_list1$x_sigma <- 0.6}

  # Init X and Z based on the parameter list
  x <- switch (para_list1$x_dist ,
               "uniform" = rand_uniform(-1, 1),
               "discrete_uniform" = rand_uniform_d(-1, 1, para_list1$x_n),
               "normal" = rand_normal(0, para_list1$x_sigma),
               "lognormal" = rand_lognormal(0, para_list1$x_sigma),
               "neglognormal" = {rl <- rand_lognormal(0, para_list1$x_sigma); closed_form(~-rl)}
  )

  # Build the cubic model
  mod <- heter_model(a = para_list1$a, b = para_list1$b,
                     x = x)

  # Generate data from the model, let k = 2 (50% true data plot, 50% null plot)
  dat <- mod$gen(para_list1$n, fit_model = TRUE, test = TRUE)
  
  c(mod$effect_size(dat), dat$p_value[1])
}

conv_result_heter <- map(1:50000, ~conv_test_heter())

conv_result_heter <- data.frame(effect_size = map_dbl(conv_result_heter, ~.x[1]), 
                          p_value = map_dbl(conv_result_heter, ~.x[2])) %>%
  mutate(reject = p_value < 0.05) %>%
  mutate(reject = as.numeric(reject))

conv_result_heter <- conv_result_heter %>%
  filter(log(effect_size) > -3) %>%
  mutate(type = "heteroskedasticity")
# 
# tt <- tapply(conv_result_heter$reject, cut(log(conv_result_heter$effect_size), 100), mean)
# tt %>%
#   names() %>%
#   stringr::str_remove_all("\\(|\\]") %>%
#   stringr::str_split(",") %>%
#   map_dbl(~mean(as.numeric(.x))) -> ttt
# 
# data.frame(log_effect_size = as.numeric(ttt), reject = tt) %>%
#   ggplot() +
#   geom_point(aes(log_effect_size, reject)) +
#   geom_smooth(aes(log_effect_size, reject), span = 0.4, col = "red") +
#   geom_smooth(data = conv_result_heter, aes(log(effect_size), reject), method = "glm",
#               method.args = list(family = binomial()))
```

```{r}
conv_result <- bind_rows(conv_result_cubic, conv_result_heter)
```


```{r powerglm, fig.cap="Quasi-binomial logistic regressions with estimated power as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for both cubic and heteroskedasticity model. Five regression were fitted for different number of evaluations shown in five different colours. Estimated power are drawn in black. The dashed curvses are the power of F-test and Breusch–Pagan test respectively."}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  select(exp_lineup_id, type, exp, effect_size) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = factor(n_eval)) %>%
  filter(log(effect_size) < 20) %>%
  ggplot() +
  geom_point(aes(log(effect_size), hat_power), alpha = 0.1, size = 1) +
  geom_smooth(aes(log(effect_size), reject, group = "conventional"),
              method = "glm", method.args = list(family = binomial), data = conv_result, 
              se = FALSE, linetype = 2, size = 1, col = "grey80") +
  geom_smooth(aes(log(effect_size), hat_power, group = n_eval, col = n_eval),
              method = "glm", method.args = list(family = quasibinomial), size = 0.5, se = FALSE) +
  facet_wrap(~type, scales = "free_x") +
  labs(col = "Number of evaluations") +
  theme_light(base_size = 5) +
  theme(legend.position = "bottom") +
  guides(col = guide_legend(nrow = 1, byrow=TRUE)) +
  scale_colour_viridis_d(option = "B", begin = 0.5, end = 0.9) +
  xlab("Log of effect size") +
  ylab("Estimated power")
```

Another method we used to model the power was to collect all decisions made by $K\choose{K-j}$ different outcomes for $V_{K-j}$, and directly fit a binomial generalized linear model with natural logarithm of effect size as the regressor. This method skipped the calculation of the estimated power $R/{K\choose{K-j}}$, and regressed on the binary outcome with larger sample size. Results of the fitted model is given in Figure \@ref(fig:powerglmsecond), Table \@ref(tab:powerglmhetersecond) and Table \@ref(tab:powerglmcubicsecond). According to Figure \@ref(fig:powerglmsecond), this method gave clearer separation between curves for heteroskedasticity model. Noticeably, fitted power of visual test with five evaluations matched the power of conventional test at large effect size.   


```{r results="asis"}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  select(exp_lineup_id, type, exp, effect_size) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = factor(n_eval)) %>%
  unnest_longer(col = p_value) %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) -> tmp_dat2

map(1:5, function(x) glm(hat_power ~ log(effect_size),
                         family = binomial(),
                         data = filter(tmp_dat2, n_eval == {{x}}, type == "cubic"))) %>%
  stargazer(header = FALSE,
            label = "tab:powerglmcubicsecond",
            title = "Logistic regressions with rejection as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for cubic model. Five regression were fitted for different number of evaluations.",
            style = "default")
```


```{r results="asis"}
map(1:5, function(x) glm(hat_power ~ log(effect_size),
                         family = binomial(),
                         data = filter(tmp_dat2, n_eval == {{x}}, type == "heteroskedasticity"))) %>%
  stargazer(header = FALSE,
            label = "tab:powerglmhetersecond",
            title = "Logistic regressions with rejection as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for heteroskedasticity model. Five regression were fitted for different number of evaluations.",
            style = "default")
```



```{r powerglmsecond, fig.cap="Logistic regressions with rejection as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for both cubic and heteroskedasticity model. Five regression were fitted for different number of evaluations shown in five different colours. Estimated power are drawn in black. The dashed curvses are the power of F-test and Breusch–Pagan test respectively."}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  select(exp_lineup_id, type, exp, effect_size) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = factor(n_eval)) %>%
  unnest_longer(col = p_value) %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) %>%
  mutate(reject = as.numeric(reject)) %>%
  ggplot() +
  geom_point(aes(log(effect_size), reject), alpha = 0.1, size = 1) +
  geom_smooth(aes(log(effect_size), reject, group = "conventional"),
              method = "glm", method.args = list(family = binomial), data = conv_result, 
              se = FALSE, size = 1, linetype = 2, col = "grey80") +
  geom_smooth(aes(log(effect_size), reject, group = n_eval, col = n_eval),
              method = "glm", method.args = list(family = binomial), size = 0.5, se = FALSE) +
  facet_wrap(~type, scales = "free_x") +
  labs(col = "Number of evaluations") +
  theme_light(base_size = 5) +
  theme(legend.position = "bottom") +
  scale_colour_viridis_d(option = "B", begin = 0.5, end = 0.9) +
  guides(col = guide_legend(nrow = 1, byrow=TRUE)) +
  xlab("Log of effect size") +
  ylab("Rejected")
```


```{r eval = FALSE}
merged_survey %>%
  filter(exp == 2, lineup_id == 1)
```

#### Effect of Factors on Power of the Visual Test

The previous section focuses on the change of effect size relative to the power of the visual test. Individual factor embedded in the data simulation also needs be analysed. Two logistic regressions were fitted with decisions made by $V_{K-j}$ as response variable. For the cubic model, regressors were $|a|$, $|b|$, $|c-1|$, $\sigma$, distribution of $\boldsymbol{X}$, distribution of $\boldsymbol{Z}$, number of observations and number of evaluations. For the heteroskedasticity model, regressors were $|a|$, $b$, distribution of $\boldsymbol{X}$, number of observations and number of evaluations.

According to Table \@ref(tab:rejectfactorcubic), for cubic model, both coefficients of $|a|$ and $|b|$ were positive, which suggested increasing the coefficients of quadric and cubic terms would result in higher visual power. The coefficient of $|c-1|$ was also positive indicating joint effect of multiple regressors on the residual plot decreased the visual power. As expected, the coefficient of $\sigma$ was negative as it controlled the noise level. Lineup simulated with uniform distribution of $\boldsymbol{X}$ had the highest power, followed by negative lognormal, normal, and lognormal. Discreteness in $\boldsymbol{Z}$ decreased the visual power. The coefficient of number of observations was positive, which means larger sample size would lead to higher visual power.  

Results of logistic regression for heteroskedasticity is given in Table \@ref(tab:rejectfactorheter). For heteroskedasticity model, the coefficient of $|a|$ was negative, which suggested "butter fly" shape was easier to identify than "triangle" shape. As expected, the coefficient of $b$ was positive. For different distributions of $\boldsymbol{X}$, the one with highest power was uniform distribution, followed by normal, discrete uniform, lognormal and negative lognormal. The coeffient of the number of observations was positive, but it was not significant. 

In overall, we did not have any unexpected findings except the visual power related to different distributions of $\boldsymbol{X}$ in cubic model. We expected lineup simulated with uniform distribution would have the highest power, followed by normal, lognormal and negative lognormal, since we observed similar results in pilot study.  

```{r results="asis"}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = n_eval) %>%
  unnest_longer(col = p_value) %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) -> single_merged

glm(reject ~ abs(a) + abs(b) + abs(c - 1) + e_sigma + x_dist + z_discrete + n + n_eval, data = filter(single_merged, type == "cubic"), family = binomial()) -> mod1_factor

```


```{r results="asis"}
glm(reject ~ abs(a) + b + x_dist + n + n_eval, data = filter(single_merged, type == "heteroskedasticity"), family = binomial()) -> mod2_factor

stargazer(mod1_factor, header = FALSE,
          label = "tab:rejectfactorcubic",
          title = "Logistic regressions with rejection as response variable and parameter settings as regressors fitted on the data collected from experiment I and II for cubic model.",
          style = "default",
          covariate.labels = c("|a|", "|b|", "|c-1|", 
                               "sigma",
                               "Distribution of X: Negative Lognormal",
                               "Distribution of X: Normal", 
                               "Distribution of X: Uniform", 
                               "Distribution of Z: Discrete",
                               "n",
                               "Number of evaluations",
                               "Constant"))

```

```{r results="asis"}
stargazer(mod2_factor, header = FALSE,
          label = "tab:rejectfactorheter",
          title = "Logistic regressions with rejection as response variable and parameter settings as regressors fitted on the data collected from experiment I and II for heteroskedasticity model.",
          style = "default",
          covariate.labels = c("|a|", "b",
                               "Distribution of X: Lognormal", 
                               "Distribution of X: Negative Lognormal",
                               "Distribution of X: Normal", 
                               "Distribution of X: Uniform", 
                               "n",
                               "Number of evaluations",
                               "Constant"))
```


<!-- ### Visual $p$-value -->

<!-- ### Response Time -->

<!-- ### Reason -->

<!-- ### Distribution of selections -->


<!-- (power against different parameters) -->

<!-- (power against conventional p-value) -->

<!-- (conventional power) -->

<!-- (visual p-value vs. conventional p-value) -->

<!-- (response time) -->

<!-- (confidence level) -->

<!-- (relative freq of picks) -->

<!-- (all variables in the merged dataset) -->


## Automatic Visual Inference System

Lineup protocol can be used to make valid inference for visual discoveries, but one of the drawbacks of this protocol is it requires human evaluation of data plots. Such requirement is unattainable when the number of data plots need for testing is large. Even if it can be fulfilled, the cost of labour would be exceedingly high. Hence, there is a need of building an automatic visual inference system to read data plots, evaluate lineups and provide meaningful decisions. Since this desired automatic system is technically an agent intended to play the role of evaluators as humans in lineup protocol, some degree of intelligence needs to be demonstrated. This brings us to Artificial Intelligence (AI). 

### Artificial Intelligence: Four Approaches {#se:ai}

AI is the field of research concerned with understanding and building machines who can demonstrate intelligence. As discussed in @russell_artificial_2002, historically, there are disagreements among researchers about the definition of intelligence, which is caused by two critical questions:

1. Should AI act and think humanly or rationally? 
2. Without the thought process and reasoning, are behaviours sufficient to demonstrate intelligence?

Based on the answer to the above questions, four major approaches to pursue AI have been established. These approaches can be summarized into a two by two table as shown in Figure \@ref(fig:two-dim-ai), where the row is "Human" vs. "Rational", and the column is "Behaviour" vs. "Thought". Positioning at the top right cell, the rational agent approach aims to build agent that perform mathematically perfect acts such that the best expected outcome can always be achieved. In contrast, the "laws of thought" approach focus on understanding the logic behind the rationality. Closely related to cognitive science, the cognitive modelling approach attempts to express theories of human cognition as computer program to mimic the thought process of human. Lastly, the Turing test approach is built upon the famous Turing test proposed by @turing_computing_1950. The test can be roughly described as, whether a human can distinguish another human from a computer with written communications only. To pass the test, several capabilities of computer are required. This includes natural language processing for communication with human, knowledge representation for encoding knowledge, automated reasoning for derivation of conclusions and machine learning for improving AI automatically through experience and data. Some researchers argued that written communication is insufficient to demonstrate intelligence, and some degree of physical simulation of a person is still necessary. One such example is the total Turing test proposed by @harnad_other_1991. It adds three new requirements to the list, including computer vision, speech recognition and robotics, which are response for interactions with the physical world. Notably, all seven required capabilities have become major subfields of AI today. And their development has made AI one of the fastest-growing fields in the 21st century [@russell_artificial_2002].

```{r two-dim-ai, fig.cap="Four possible approaches to pursue AI based on the two dimensions in AI research - human vs. rational and thought vs. behavior [@russell_artificial_2002]."}
ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey") +
  geom_vline(aes(xintercept = 1), col = "grey") +
  geom_text(aes(x = 0.5 , y = 1.5, label = "The Turing test approach")) +
  geom_text(aes(x = 1.5 , y = 1.5, label = "The rational agent approach")) +
  geom_text(aes(x = 0.5 , y = 0.5, label = "The cognitive modelling approach")) +
  geom_text(aes(x = 1.5 , y = 0.5, label = 'The "laws of thought" approach')) +
  theme_light() +
  xlab("") +
  ylab("") +
  scale_x_continuous(breaks = c(0.5, 1.5), labels = c("Human", "Rational"), limits = c(0, 2)) +
  scale_y_continuous(breaks = c(0.5, 1.5), labels = c("Thought", "Behaviour"), limits = c(0, 2)) +
  theme(panel.grid = element_blank(), axis.ticks = element_blank())
```

Considering there are different approches of AI, it is necessary to clarify the type of AI we aimed to build. Understanding the human thought process or mimicking the human vision mechanism were not of interest in this research. We believed humanly behaviours was enough for demonstrating AI for the problem we concerned. Hence, two of the capacities required by the total Turing test, namely machine learning and computer vision were adopted.

Another possible method under the rational agent approach was to define distance metrics to measure difference between data plots for making mathematically prefect decisions. This method would work if such metrics can be obtained manually or approximated algorithmically. However, we considered it was too complex to fit into our prototype automatic system. But we will revisited this method in our second project. 

### Design of the Automatic System

Depended on the inputs and outputs of the automatic system, various designs arised.


A system can be summarised 




For an automatic visual inference system, types of inputs can be classified by two decisions: (i) whether to accept multiple image files as inputs, and (ii) whether to accept fitted model.

single image file
multiple image files

fitted model




Depend on the inputs and outputs, computer vision model can be integrated into an automatic visual inference system.  





### different approaches of AI

- Not aim for understanding the thought process
- Not aim for mimicking the human vision mechanism
- May be able to define distance metrics to measure difference between data plots for making mathematically prefect decisions
- May be able to use computer vision model to approximate how people evaluate lineups

computer vision model:

- Use human data to train model with human selection as target -> mimic the human behaviour
- Use simulated lineup to train model with actual data plot as target -> assume the actual data plot is the most different one
- Use simulated data plot to train model with null or not null as target -> limited to the alternatives given to the model, and Type I error can not be controlled
- Use simulated data plot to train model, then let the model evaluate each plot of a lineup -> leads to multiple selections in a lineup, and the model does not compare different plots in a lineup
  - Use a very large $m$ to ensure the null hypothesis can be rejected by a classifier slightly better than a random selector. The problem becomes how large the $m$ should be
  - Use multiple models as multiple individuals to evaluate lineup -> Which models should be used? How many models should be used?
- Let the model select multiple plots from a simulated lineup while comparing different plots in a lineup -> how to build such a system? 
  - The input is $m$ plots, what is the output? The probability of being the most different one? 
  - Then how many plots should be selected? Top 5? Or cumulative probability greater than a threshold? 
  - How to select the threshold? 

The primary drawbacks of visual inference are 

 infeasible in a large scale
2. unfriendly to vision-impaired people 3. high finical cost and human cost
4. time consuming

Visual inference is a valid useful in diagnostics of predictive models 

To relieve human analysts of the job of diagnosing predictive models in the context of visual inference, computer vision 

Hence, to relieve human analysts of the job of diagnosing, automatically improve predictive models and assist vision-impaired people, this research aims to develop computer vision model to automate the reading
1
of data plots with application to predictive model diagnostics. The success of this research will enrich the applications of artificial intelligence in data visualization and improve the debugging process of artificial intelligence models.

### Model Architecture



Five computer vision models were trained to predict the probability of the given result plot being a null residual plot.  




### Performance

```{r}
null_true_eval_dat <- data.frame()

model_name <- c("VGG16", "VGG19", "Xception", "ResNet152V2", "InceptionV3")

for (i in 1:5) {
  mid_dat <- read_csv(here::here(glue::glue("data/cv/prediction/mod_{i}_null_both.csv")))
  null_true_eval_dat <- bind_rows(null_true_eval_dat, mid_dat)
  mid_dat <- read_csv(here::here(glue::glue("data/cv/prediction/mod_{i}_true_both.csv")))
  null_true_eval_dat <- bind_rows(null_true_eval_dat, mid_dat)
} 
```

```{r}
null_true_eval_dat %>%
  mutate(type = ifelse(id <= 5000, "Cubic", "Heteroskedasticity")) %>% 
  group_by(model, type, null) %>%
  summarise(rate = mean(prediction == null)) %>%
  pivot_wider(names_from = "null", values_from = "rate") %>%
  ungroup() %>%
  mutate(avg_rate = (`FALSE` + `TRUE`)/2) %>%
  select(-model) %>%
  knitr::kable(col.names = c("", "Actual data plot detection rate", "Null plot detection rate", "Average detection rate"), 
             booktabs = TRUE, 
             caption  = 'Performance of computer vision models on null residual plot and acutal data plot.', 
             label = "null-true-eval" ) %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::pack_rows("VGG16", 1, 2, bold = TRUE, italic = TRUE, underline = TRUE) %>%
  kableExtra::pack_rows("VGG19", 3, 4, bold = TRUE, italic = TRUE, underline = TRUE) %>%
  kableExtra::pack_rows("Xception", 5, 6, bold = TRUE, italic = TRUE, underline = TRUE) %>%
  kableExtra::pack_rows("ResNet152V2", 7, 8, bold = TRUE, italic = TRUE, underline = TRUE) %>%
  kableExtra::pack_rows("InceptionV3", 9, 10, bold = TRUE, italic = TRUE, underline = TRUE)
```

```{r}
lineup_eval_dat <- data.frame()

for (i in 1:5) {
  mid_dat <- read_csv(here::here(glue::glue("data/cv/prediction/mod_{i}_fixed_both.csv"))) %>%
    mutate(model = i) 
  lineup_eval_dat <- bind_rows(lineup_eval_dat, mid_dat)
}

eval_data_para <- readRDS(here::here("data/cv/eval_data_para.rds"))
eval_data_para <- reduce(map(eval_data_para, as.data.frame), bind_rows)
eval_original_data <- readRDS(here::here("data/cv/eval_original_data.rds"))
```

```{r}
eval_original_data %>%
  group_by(id, k) %>%
  summarise(ans = first(null)) -> eval_lineup_ans
```

```{r}
lineup_eval_dat %>%
  left_join(eval_lineup_ans, by = c("set" = "id", "plot" = "k")) %>%
  group_by(model, set) %>%
  summarise(n_sel = sum(!predict), detect = sum(ans == FALSE & predict == FALSE)) -> lineup_eval_summary
```

```{r}
cache_env <- new.env()
lineup_eval_summary %>%
  calc_p_value_multi(lineup_id = "set", detected = "detect", n_sel = "n_sel", cache_env = cache_env) -> cv_results
```


```{r}
cv_results %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) %>%
  left_join(select(eval_original_data, id, effect_size) %>%
              group_by(id) %>%
              summarise(effect_size = first(effect_size)),
            by = c("lineup_id" = "id")) -> cv_results

cv_results %>%
  mutate(type = ifelse(lineup_id <= 2500, "cubic", "heteroskedasticity")) -> cv_results
```

```{r}
merged_survey %>%
  mutate(exp_lineup_id = paste0(exp, "_", lineup_id)) %>%
  select(exp_lineup_id, type, exp, effect_size) %>%
  right_join(comb_pvalue) %>%
  mutate(n_eval = factor(n_eval)) %>%
  unnest_longer(col = p_value) %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) %>%
  mutate(reject = as.numeric(reject)) %>%
  ggplot() +
  geom_point(aes(log(effect_size), reject), alpha = 0.1, size = 1) +
  geom_smooth(aes(log(effect_size), reject, group = "conventional"),
              method = "glm", method.args = list(family = binomial), data = conv_result, 
              se = FALSE, size = 1, linetype = 2, col = "grey80") +
  geom_smooth(aes(log(effect_size), reject, group = n_eval, col = n_eval),
              method = "glm", method.args = list(family = binomial), size = 0.5, se = FALSE) +
  geom_smooth(aes(log(effect_size), as.numeric(reject), group = "cv"), data = cv_results,
              se = FALSE, size = 1, method = "glm", method.args = list(family = binomial)) +
  facet_wrap(~type, scales = "free_x") +
  labs(col = "Number of evaluations") +
  theme_light(base_size = 5) +
  theme(legend.position = "bottom") +
  scale_colour_viridis_d(option = "B", begin = 0.5, end = 0.9) +
  guides(col = guide_legend(nrow = 1, byrow=TRUE)) +
  xlab("Log of effect size") +
  ylab("Rejected")
```



```{r}
lineup_eval_dat %>%
  left_join(eval_lineup_ans, by = c("set" = "id", "plot" = "k")) %>%
  filter(ans == TRUE) %>%
  group_by(set, model) %>%
  summarise(pseudo_predict = sample(predict, 1)) -> pseudo_predict_dat

lineup_eval_dat %>%
  left_join(eval_lineup_ans, by = c("set" = "id", "plot" = "k")) %>%
  left_join(pseudo_predict_dat) %>%
  mutate(predict = ifelse(ans == FALSE, pseudo_predict, predict)) %>%
  group_by(model, set) %>%
  summarise(n_sel = sum(!predict), detect = sum(ans == FALSE & predict == FALSE))  -> lineup_pseudo_summary

lineup_pseudo_summary %>%
  calc_p_value_multi(lineup_id = "set", detected = "detect", n_sel = "n_sel", cache_env = cache_env) -> pseudo_results

pseudo_results %>%
  mutate(reject = eval_p_value(p_value, tol = 1e-2)) %>%
  mutate(type = ifelse(lineup_id <= 2500, "Cubic", "Heteroskedasticity")) %>%
  group_by(type) %>%
  summarise(rate = mean(reject))
```


# General Purpose Automatic Visual Statistical Inference System 

# Visual Diagnostics for Predictive Models with ... 

# Timeline

\begin{table} \label{tab:timeline}
\centering
\begin{tabular}[t]{ll}
\toprule
\addlinespace[0.5em]
Date & Event\\
\addlinespace[0.5em]
\midrule
\addlinespace[0.5em]
\multicolumn{2}{l}{\textit{\textbf{2022}}}\\
\hspace{4em}March & Extend the scope of violations of classical normal linear\\ 
\addlinespace[0.5em]
\hspace{4em} & regression considered by the computer vision model\\
\addlinespace[0.5em]
\hspace{4em}April & Conduct human subject experiment on the extended model\\ 
\addlinespace[0.5em]
\hspace{4em} & defects\\
\addlinespace[0.5em]
\hspace{4em}May & Conclude the first project: draft and submit the first paper\\
\addlinespace[0.5em]
\hspace{4em}June & Review previous studies in image comparison\\
\addlinespace[0.5em]
\hspace{4em}July & Propose a new design of the automatic visual inference\\
\addlinespace[0.5em]
\hspace{4em} & system based on image comparison\\
\addlinespace[0.5em]
\hspace{4em}August - November & Develop the proposed system\\
\addlinespace[0.5em]
\hspace{4em}November - December & Conduct human subject experiment for validating the\\
\addlinespace[0.5em]
\hspace{4em} & proposed system\\
\addlinespace[0.5em]
\hspace{4em}December & Attend the Australasian Applied Statistics Conference\\
\addlinespace[0.5em]
\hline
\addlinespace[0.5em]
\multicolumn{2}{l}{\textit{\textbf{2023}}}\\
\hspace{4em}January - March & Finish the second project: prepare milestone material and\\
\addlinespace[0.5em]
\hspace{4em} & submit the second paper\\
\addlinespace[0.5em]
\hspace{4em}April - December & Apply the automatic visual inference system to predictive\\
\addlinespace[0.5em]
\hspace{4em} & model diagnostics\\
\addlinespace[0.5em]
\hline
\addlinespace[0.5em]
\multicolumn{2}{l}{\textit{\textbf{2024}}}\\
\hspace{4em}January - March & Finish the third project: finalize the thesis\\
\addlinespace[0.5em]
\bottomrule
\end{tabular}
\end{table}
