
@article{goldstein_peeking_2015,
	title = {Peeking {Inside} the {Black} {Box}: {Visualizing} {Statistical} {Learning} {With} {Plots} of {Individual} {Conditional} {Expectation}},
	volume = {24},
	issn = {1061-8600},
	shorttitle = {Peeking {Inside} the {Black} {Box}},
	url = {https://doi.org/10.1080/10618600.2014.907095},
	doi = {10.1080/10618600.2014.907095},
	abstract = {This article presents individual conditional expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the PDP by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data-generating model. Through simulated examples and real datasets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
	number = {1},
	urldate = {2021-12-13},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
	month = jan,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2014.907095},
	keywords = {Exploratory data analysis, Graphical method, Model visualization},
	pages = {44--65},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/4LRXJS42/Goldstein et al. - 2015 - Peeking Inside the Black Box Visualizing Statisti.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/X2YKSRSC/10618600.2014.html:text/html},
}

@article{gupta_detecting_2021,
	title = {Detecting distributional differences between temporal granularities for exploratory time series analysis},
	abstract = {Cyclic temporal granularities are temporal deconstructions of a time period into units such as hour-of-theday and work-day/weekend. They can be useful for measuring repetitive patterns in large univariate time series data, and feed new approaches to exploring time series data. One use is to take pairs of granularities, and make plots of response values across the categories induced by the temporal deconstruction. However, when there are many granularities that can be constructed for a time period, there will also be too many possible displays to decide which might be the more interesting to display. This work proposes a new distance metric to screen and rank the possible granularities, and hence choose the most interesting ones to plot. The distance measure is computed for a single or pairs of cyclic granularities and can be compared across different cyclic granularities or on a collection of time series. The methods are implemented in the open-source R package hakear.},
	language = {en},
	author = {Gupta, Sayani and Hyndman, Rob J and Cook, Dianne},
	year = {2021},
	pages = {25},
	file = {Gupta et al. - 2021 - Detecting distributional differences between tempo.pdf:/Users/patrickli/Zotero/storage/X9I6CNH8/Gupta et al. - 2021 - Detecting distributional differences between tempo.pdf:application/pdf},
}

@article{krishnan_hierarchical_2021,
	title = {Hierarchical {Decision} {Ensembles}- {An} inferential framework for uncertain {Human}-{AI} collaboration in forensic examinations},
	url = {http://arxiv.org/abs/2111.01131},
	abstract = {Forensic examination of evidence like firearms and toolmarks, traditionally involves a visual and therefore subjective assessment of similarity of two questioned items. Statistical models are used to overcome this subjectivity and allow specification of error rates. These models are generally quite complex and produce abstract results at different levels of the analysis. Presenting such metrics and complicated results to examiners is challenging, as examiners generally do not have substantial statistical training to accurately interpret results. This creates distrust in statistical modelling and lowers the rate of acceptance of more objective measures that the discipline at large is striving for. We present an inferential framework for assessing the model and its output. The framework is designed to calibrate trust in forensic experts by bridging the gap between domain specific knowledge and predictive model results, allowing forensic examiners to validate the claims of the predictive model while critically assessing results.},
	journal = {arXiv:2111.01131 [cs, stat]},
	author = {Krishnan, Ganesh and Hofmann, Heike},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/BP4H9VRT/Krishnan and Hofmann - 2021 - Hierarchical Decision Ensembles- An inferential fr.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/84H58CXJ/2111.html:text/html},
}

@article{korting_visual_2021,
	title = {Visual {Inference} and {Graphical} {Representation} in {Regression} {Discontinuity} {Designs}},
	url = {http://arxiv.org/abs/2112.03096},
	abstract = {Despite the widespread use of graphs in empirical research, little is known about readers' ability to process the statistical information they are meant to convey ("visual inference"). We study visual inference within the context of regression discontinuity (RD) designs by measuring how accurately readers identify discontinuities in graphs produced from data generating processes calibrated on 11 published papers from leading economics journals. First, we assess the effects of different graphical representation methods on visual inference using randomized experiments. We find that bin widths and fit lines have the largest impacts on whether participants correctly perceive the presence or absence of a discontinuity. Incorporating the experimental results into two decision theoretical criteria adapted from the recent economics literature, we find that using small bins with no fit lines to construct RD graphs performs well and recommend it as a starting point to practitioners. Second, we compare visual inference with widely used econometric inference procedures. We find that visual inference achieves similar or lower type I error rates and complements econometric inference.},
	urldate = {2022-01-20},
	journal = {arXiv:2112.03096 [econ]},
	author = {Korting, Christina and Lieberman, Carl and Matsudaira, Jordan and Pei, Zhuan and Shen, Yi},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.03096},
	keywords = {Economics - Econometrics},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/CC46XXWR/Korting et al. - 2021 - Visual Inference and Graphical Representation in R.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/IBYHEKYW/2112.html:text/html},
}

@article{hecksteden_dealing_2021,
	title = {Dealing with small samples in football research},
	volume = {0},
	issn = {2473-3938},
	url = {https://doi.org/10.1080/24733938.2021.1978106},
	doi = {10.1080/24733938.2021.1978106},
	abstract = {In football research, ‘small’ trials with low statistical power are common. On the elite level, the inherently low number of participants obviously conflicts with the relevance of even tiny effects. However, general characteristics of football also contribute (e.g. multifactorially influenced and/or complex outcomes). Importantly, small sample sizes are problematic regardless of the study outcome with issues ranging from inconclusive results and low precision to unrepeatable ‘discoveries’ and overestimation of effect sizes. Therefore, meeting the calculated, target sample size is the first priority. If a suboptimal sample size must be accepted, a range of tools can improve insights. To begin with, some general aspects of data collection and analysis become more important and should be optimally implemented (e.g. reliability of measures). Building on this foundation, specific amendments are available on the levels of data collection (e.g. aggregated single-subject designs) and data analysis (e.g. Bayesian methods). The present commentary aims to give an overview of selected, practical tools for dealing with small sample sizes in football research and provide recommendations for their application in scenarios typical for the field. Importantly, versatility and adaptability are mirrored by the need for utmost transparency including a predetermined (ideally preregistered) study plan. Collaboration or counselling with an expert statistician is strongly encouraged.},
	number = {0},
	urldate = {2022-01-20},
	journal = {Science and Medicine in Football},
	author = {Hecksteden, Anne and Kellner, Ralf and Donath, Lars},
	month = sep,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/24733938.2021.1978106},
	keywords = {small population, statistical power, statistics, Study design, underpowered, winners curse},
	pages = {1--9},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/39Z6BQKB/Hecksteden et al. - 2021 - Dealing with small samples in football research.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/Y5RXBKX9/24733938.2021.html:text/html},
}

@article{cook_foundation_2021,
	title = {The {Foundation} is {Available} for {Thinking} about {Data} {Visualization} {Inferentially}},
	url = {https://hdsr.mitpress.mit.edu/pub/mpdasaqt/release/1},
	doi = {10.1162/99608f92.8453435d},
	language = {en},
	urldate = {2022-01-20},
	journal = {Harvard Data Science Review},
	author = {Cook, Dianne and Reid, Nancy and Tanaka, Emi},
	month = jul,
	year = {2021},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/HPL2EWAS/Cook et al. - 2021 - The Foundation is Available for Thinking about Dat.pdf:application/pdf},
}

@article{hullman_designing_2021,
	title = {Designing for {Interactive} {Exploratory} {Data} {Analysis} {Requires} {Theories} of {Graphical} {Inference}},
	url = {https://hdsr.mitpress.mit.edu/pub/w075glo6/release/2},
	doi = {10.1162/99608f92.3ab8a587},
	abstract = {Research and development in computer science and statistics have produced increasingly sophisticated software interfaces for interactive and exploratory analysis, optimized for easy pattern finding and data exposure. But design philosophies that emphasize exploration over other phases of analysis risk confusing a need for flexibility with a conclusion that exploratory visual analysis is inherently “model free” and cannot be formalized. We describe how without a grounding in theories of human statistical inference, research in exploratory visual analysis can lead to contradictory interface objectives and representations of uncertainty that can discourage users from drawing valid inferences. We discuss how the concept of a model check in a Bayesian statistical framework unites exploratory and confirmatory analysis, and how this understanding relates to other proposed theories of graphical inference. Viewing interactive analysis as driven by model checks suggests new directions for software and empirical research around exploratory and visual analysis. For example, systems might enable specifying and explicitly comparing data to null and other reference distributions and better representations of uncertainty. Implications of Bayesian and other theories of graphical inference can be tested against outcomes of interactive analysis by people to drive theory development.},
	language = {en},
	urldate = {2022-01-20},
	journal = {Harvard Data Science Review},
	author = {Hullman, Jessica and Gelman, Andrew},
	month = jul,
	year = {2021},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/P9XUMXEB/Hullman and Gelman - 2021 - Designing for Interactive Exploratory Data Analysi.pdf:application/pdf},
}

@article{johnstone_statistical_2009,
	title = {Statistical challenges of high-dimensional data},
	volume = {367},
	url = {https://royalsocietypublishing-org.ezproxy.lib.monash.edu.au/doi/full/10.1098/rsta.2009.0159},
	doi = {10.1098/rsta.2009.0159},
	abstract = {Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.},
	number = {1906},
	urldate = {2022-01-20},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Johnstone, Iain M. and Titterington, D. Michael},
	month = nov,
	year = {2009},
	note = {Publisher: Royal Society},
	keywords = {Bayesian analysis, classification, cluster analysis, high-dimensional data, regression, sparsity},
	pages = {4237--4253},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/U79X6P5X/Johnstone and Titterington - 2009 - Statistical challenges of high-dimensional data.pdf:application/pdf},
}

@article{wickham_graphical_2010,
	title = {Graphical inference for infovis},
	volume = {16},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2010.161},
	abstract = {How do we know if what we see is really there? When visualizing data, how do we avoid falling into the trap of apophenia where we see patterns in random noise? Traditionally, infovis has been concerned with discovering new relationships, and statistics with preventing spurious relationships from being reported. We pull these opposing poles closer with two new techniques for rigorous statistical inference of visual discoveries. The "Rorschach" helps the analyst calibrate their understanding of uncertainty and "line-up" provides a protocol for assessing the significance of visual discoveries, protecting against the discovery of spurious structure.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike and Buja, Andreas},
	month = nov,
	year = {2010},
	keywords = {permutation tests, Accuracy, data plot, Histograms, null hypotheses, Protocols, Statistics, Tag clouds, Testing, visual testing, Visualization},
	pages = {973--979},
	file = {IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/Y6LK373G/5613434.html:text/html;IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/GZAMZJP6/Wickham et al. - 2010 - Graphical inference for infovis.pdf:application/pdf},
}

@article{wickham_ggplot2_2011,
	title = {ggplot2},
	volume = {3},
	issn = {1939-0068},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/wics.147},
	doi = {10.1002/wics.147},
	abstract = {This article discusses ggplot2, an open source R package, based on a grammatical theory of graphics. The underlying theory has been discussed in depth elsewhere so this article illustrates some of the consequences of the theory for creating new graphics, the importance of programmable graphics, and the rich ecosystem that has grown up around ggplot2. WIREs Comp Stat 2011 3 180–185 DOI: 10.1002/wics.147 This article is categorized under: Software for Computational Statistics {\textgreater} Software/Statistical Software Statistical and Graphical Methods of Data Analysis {\textgreater} Statistical Graphics and Visualization},
	language = {en},
	number = {2},
	journal = {WIREs Computational Statistics},
	author = {Wickham, Hadley},
	year = {2011},
	keywords = {statistical graphics, R, visualization},
	pages = {180--185},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/MJ8SVVXG/Wickham - 2011 - ggplot2.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/HJ9HEFUN/wics.html:text/html},
}

@article{hofmann_delayed_2011,
	title = {Delayed, {Canceled}, on {Time}, {Boarding}… {Flying} in the {USA}},
	volume = {20},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/jcgs.2011.3de},
	doi = {10.1198/jcgs.2011.3de},
	abstract = {The short paper describes the major findings of the ISU Statistical Graphics working group on airline traffic in the USA. Flight volumes at major airports are increasing. Delays decreased after structural changes in 2002–2003 but have been increasing again since and delays build up during the day reaching a peak in the early evening hours. There is some hint of wind direction and strength affecting delays. This article has supplementary material online.},
	number = {2},
	urldate = {2022-01-20},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hofmann, Heike and Cook, Dianne and Kielion, Chris and Schloerke, Barret and Hobbs, Jon and Loy, Adam and Mosley, Lawrence and Rockoff, David and Huang, Yuanyuan and Wrolstad, Danielle and Yin, Tengfei},
	month = jan,
	year = {2011},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/jcgs.2011.3de},
	keywords = {Exploratory data analysis, Data mining, Visual analytics},
	pages = {287--290},
	file = {Snapshot:/Users/patrickli/Zotero/storage/U6MWSQ7T/jcgs.2011.html:text/html;Full Text PDF:/Users/patrickli/Zotero/storage/ZSJTD8LY/Hofmann et al. - 2011 - Delayed, Canceled, on Time, Boarding… Flying in th.pdf:application/pdf},
}

@misc{majumder_visual_2010,
	title = {Visual {Statistical} {Inference} for {Regression} {Parameters}},
	abstract = {Statistical graphics play a crucial role in exploratory data analysis, model checking and diagnosis. Until recently there were no formal visual methods in place for determining statistical significance of findings. This changed, when Buja et al. [2009] conceptually introduced two protocols for formal tests of visual findings. In this paper we take this a step further by comparing the lineup protocol [Buja et al., 2009] against classical statistical testing of the significance of regression model parameters. A human subjects experiment is conducted using simulated data to provide controlled conditions. Results suggest that the lineup protocol provides results equivalent to the uniformly most powerful (UMP) test and for some scenarios yields better power than the UMP test. 1},
	author = {Majumder, Mahbubul and Hofmann, Heike and Cook, Dianne},
	year = {2010},
	file = {Citeseer - Snapshot:/Users/patrickli/Zotero/storage/PU7CIPLH/summary.html:text/html;Citeseer - Full Text PDF:/Users/patrickli/Zotero/storage/7F9LECLD/Majumder et al. - 2010 - Visual Statistical Inference for Regression Parame.pdf:application/pdf},
}

@article{brentnall_method_2011,
	title = {A {METHOD} {FOR} {EXPLORATORY} {REPEATED}-{MEASURES} {ANALYSIS} {APPLIED} {TO} {A} {BREAST}-{CANCER} {SCREENING} {STUDY}},
	volume = {5},
	issn = {1932-6157},
	url = {http://www.jstor.org/stable/23069337},
	abstract = {When a model may be fitted separately to each individual statistical unit, inspection of the point estimates may help the statistician to understand between-individual variability and to identify possible relationships. However, some information will be lost in such an approach because estimation uncertainty is disregarded. We present a comparative method for exploratory repeated-measures analysis to complement the point estimates that was motivated by and is demonstrated by analysis of data from the CADET II breast-cancer screening study. The approach helped to flag up some unusual reader behavior, to assess differences in performance, and to identify potential random-effects models for further analysis.},
	number = {4},
	urldate = {2022-01-20},
	journal = {The Annals of Applied Statistics},
	author = {Brentnall, Adam R. and Duffy, Stephen W. and Crowder, Martin J. and Gillan, Maureen G. C. and Astley, Susan M. and Wallis, Matthew G. and James, Jonathan and Boggis, Caroline R. M. and Gilbert, Fiona J.},
	year = {2011},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2448--2469},
	file = {JSTOR Full Text PDF:/Users/patrickli/Zotero/storage/82IHP9RR/Brentnall et al. - 2011 - A METHOD FOR EXPLORATORY REPEATED-MEASURES ANALYSI.pdf:application/pdf},
}

@article{hofmann_graphical_2012,
	title = {Graphical {Tests} for {Power} {Comparison} of {Competing} {Designs}},
	volume = {18},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2012.230},
	abstract = {Lineups [4, 28] have been established as tools for visual testing similar to standard statistical inference tests, allowing us to evaluate the validity of graphical findings in an objective manner. In simulation studies [12] lineups have been shown as being efficient: the power of visual tests is comparable to classical tests while being much less stringent in terms of distributional assumptions made. This makes lineups versatile, yet powerful, tools in situations where conditions for regular statistical tests are not or cannot be met. In this paper we introduce lineups as a tool for evaluating the power of competing graphical designs. We highlight some of the theoretical properties and then show results from two studies evaluating competing designs: both studies are designed to go to the limits of our perceptual abilities to highlight differences between designs. We use both accuracy and speed of evaluation as measures of a successful design. The first study compares the choice of coordinate system: polar versus cartesian coordinates. The results show strong support in favor of cartesian coordinates in finding fast and accurate answers to spotting patterns. The second study is aimed at finding shift differences between distributions. Both studies are motivated by data problems that we have recently encountered, and explore using simulated data to evaluate the plot designs under controlled conditions. Amazon Mechanical Turk (MTurk) is used to conduct the studies. The lineups provide an effective mechanism for objectively evaluating plot designs.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hofmann, Heike and Follett, Lendie and Majumder, Mahbubul and Cook, Dianne},
	month = dec,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Accuracy, Visual analytics, Data models, Efficiency of displays, Inference mechanisms, Lineups, Observers, Power comparison, Statistical analysis, Visual inference},
	pages = {2441--2448},
	file = {IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/UYEZKLDF/6327249.html:text/html;IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/HMNP2STA/Hofmann et al. - 2012 - Graphical Tests for Power Comparison of Competing .pdf:application/pdf},
}

@article{yifan_zhao_mind_2013,
	title = {Mind {Reading}: {Using} an {Eye}-{Tracker} to {See} {How} {People} are {Looking} at {Lineups}},
	volume = {6},
	issn = {19985010},
	shorttitle = {Mind {Reading}},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=asn&AN=93607180&site=ehost-live&scope=site},
	doi = {10.6148/IJITAS.2013.0604.05},
	abstract = {Graphical statistics plays a very important role in research and industry. As a statistician, it is very useful to know how analysts make decisions based on plots, so that the plots can be improved for better performance. With the help of eye-tracking equipment, researchers can assess this. This paper describes the use of an eye-tracker for assessing how people look at lineup plots that are used in visual inference. The experiment included a comparison of several plot types, histogram, density, scatterplot and dotplot for two group data, and the use of side-by-side boxplots and scatterplots for assessing the importance of predictors in a regression model. Results are compared with those from a larger visual inference study using Amazon's Mechanical Turk. The eye-tracker can be manageably used only on a small sample of people, to give detailed information plot about reading behavior, so it is ideally used prior to a large human subjects study to help refine the experimental procedures, or as a post-processing step to better understand any surprises in the results.},
	number = {4},
	urldate = {2022-01-21},
	journal = {International Journal of Intelligent Technologies \& Applied Statistics},
	author = {{Yifan Zhao} and Cook, Dianne and Hofmann, Heike and Majumder, Mahbubul and Chowdhury, Niladri Roy},
	month = dec,
	year = {2013},
	note = {Publisher: Airiti Press Inc.},
	keywords = {AMAZON.COM Inc., Eye-tracking, Graphical inference, Graphical statistics, HUMAN research subjects, Machine learning, MACHINE learning, REGRESSION analysis, STATISTICIANS, STATISTICS},
	pages = {393--413},
	file = {EBSCO Full Text:/Users/patrickli/Zotero/storage/CEEXEQXQ/Yifan Zhao et al. - 2013 - Mind Reading Using an Eye-Tracker to See How Peop.pdf:application/pdf},
}

@article{gelman_infovis_2013,
	title = {Infovis and {Statistical} {Graphics}: {Different} {Goals}, {Different} {Looks}},
	volume = {22},
	issn = {1061-8600},
	shorttitle = {Infovis and {Statistical} {Graphics}},
	url = {https://doi.org/10.1080/10618600.2012.761137},
	doi = {10.1080/10618600.2012.761137},
	abstract = {The importance of graphical displays in statistical practice has been recognized sporadically in the statistical literature over the past century, with wider awareness following Tukey's Exploratory Data Analysis and Tufte's books in the succeeding decades. But statistical graphics still occupy an awkward in-between position: within statistics, exploratory and graphical methods represent a minor subfield and are not well integrated with larger themes of modeling and inference. Outside of statistics, infographics (also called information visualization or Infovis) are huge, but their purveyors and enthusiasts appear largely to be uninterested in statistical principles.We present here a set of goals for graphical displays discussed primarily from the statistical point of view and discuss some inherent contradictions in these goals that may be impeding communication between the fields of statistics and Infovis. One of our constructive suggestions, to Infovis practitioners and statisticians alike, is to try not to cram into a single graph what can be better displayed in two or more. We recognize that we offer only one perspective and intend this article to be a starting point for a wide-ranging discussion among graphic designers, statisticians, and users of statistical methods. The purpose of this article is not to criticize but to explore the different goals that lead researchers in different fields to value different aspects of data visualization.},
	number = {1},
	urldate = {2022-01-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gelman, Andrew and Unwin, Antony},
	month = jan,
	year = {2013},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2012.761137},
	keywords = {Visualization, Graphics, Infovis, Statistical communication},
	pages = {2--28},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/ZN8VABAV/Gelman and Unwin - 2013 - Infovis and Statistical Graphics Different Goals,.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/EQHU42TW/10618600.2012.html:text/html},
}

@article{majumder_validation_2013,
	title = {Validation of {Visual} {Statistical} {Inference}, {Applied} to {Linear} {Models}},
	volume = {108},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2013.808157},
	doi = {10.1080/01621459.2013.808157},
	abstract = {Statistical graphics play a crucial role in exploratory data analysis, model checking, and diagnosis. The lineup protocol enables statistical significance testing of visual findings, bridging the gulf between exploratory and inferential statistics. In this article, inferential methods for statistical graphics are developed further by refining the terminology of visual inference and framing the lineup protocol in a context that allows direct comparison with conventional tests in scenarios when a conventional test exists. This framework is used to compare the performance of the lineup protocol against conventional statistical testing in the scenario of fitting linear models. A human subjects experiment is conducted using simulated data to provide controlled conditions. Results suggest that the lineup protocol performs comparably with the conventional tests, and expectedly outperforms them when data are contaminated, a scenario where assumptions required for performing a conventional test are violated. Surprisingly, visual tests have higher power than the conventional tests when the effect size is large. And, interestingly, there may be some super-visual individuals who yield better performance and power than the conventional test even in the most difficult tasks. Supplementary materials for this article are available online.},
	number = {503},
	journal = {Journal of the American Statistical Association},
	author = {Majumder, Mahbubul and Hofmann, Heike and Cook, Dianne},
	month = sep,
	year = {2013},
	keywords = {Exploratory data analysis, Visualization, Data mining, Effect size, Lineup, Nonparametric test, Practical significance, Statistical graphics},
	pages = {942--956},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/H8V45DUB/Majumder et al. - 2013 - Validation of Visual Statistical Inference, Applie.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/59BGSAGD/01621459.2013.html:text/html},
}

@article{loy_diagnostic_2013,
	title = {Diagnostic tools for hierarchical linear models},
	volume = {5},
	issn = {1939-0068},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1238},
	doi = {10.1002/wics.1238},
	abstract = {Hierarchical structures are omnipresent in today's society—this is reflected in the data that we collect on all aspects of this society. Hierarchical linear models allow a representation of structural levels in a statistical modeling framework. Diagnostic tools are used to assess the quality of model estimation and explore features of the data not well described by the model. Residual and influence diagnostics are familiar tools for the classical regression model with independent observations. For hierarchical linear models, these diagnostic tools must be adjusted to reflect the dependence introduced by the nested data structure. Residual analysis now includes the assessment of distributional assumptions at each level of the model. This requires the use of level-dependent residual quantities. Similarly, the parameter estimates may be influenced at each level of the model, requiring influence diagnostics that can pinpoint specific levels of the model, as well as specific aspects of the model. We present an overview of the diagnostic tools available for hierarchical linear models that are familiar from linear models. Additionally, we discuss the utility of the lineup protocol for residual analysis with complex models. WIREs Comput Stat 2013, 5:48–61. doi: 10.1002/wics.1238 This article is categorized under: Statistical Models {\textgreater} Bayesian Models Statistical and Graphical Methods of Data Analysis {\textgreater} Bayesian Methods and Theory Statistical and Graphical Methods of Data Analysis {\textgreater} Modeling Methods and Algorithms Statistical and Graphical Methods of Data Analysis {\textgreater} Statistical Graphics and Visualization},
	language = {en},
	number = {1},
	journal = {WIREs Computational Statistics},
	author = {Loy, Adam and Hofmann, Heike},
	year = {2013},
	keywords = {diagnostics, hierarchical linear models, influence analysis, outlier detection, residual analysis},
	pages = {48--61},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/6FHKL4DR/Loy and Hofmann - 2013 - Diagnostic tools for hierarchical linear models.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/MNKB49FF/wics.html:text/html},
}

@article{healy_data_2014,
	title = {Data {Visualization} in {Sociology}},
	volume = {40},
	issn = {0360-0572},
	url = {https://www-annualreviews-org.ezproxy.lib.monash.edu.au/doi/citedby/10.1146/annurev-soc-071312-145551},
	doi = {10.1146/annurev-soc-071312-145551},
	number = {1},
	urldate = {2022-01-21},
	journal = {Annual Review of Sociology},
	author = {Healy, Kieran and Moody, James},
	month = jul,
	year = {2014},
	note = {Publisher: Annual Reviews},
	pages = {105--128},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/2L522FR8/Healy and Moody - 2014 - Data Visualization in Sociology.pdf:application/pdf},
}

@article{loy_hlmdiag_2014,
	title = {{HLMdiag}: {A} {Suite} of {Diagnostics} for {Hierarchical} {Linear} {Models} in {R}},
	volume = {56},
	copyright = {Copyright (c) 2012 Adam Loy, Heike Hofmann},
	issn = {1548-7660},
	shorttitle = {{HLMdiag}},
	url = {https://doi.org/10.18637/jss.v056.i05},
	doi = {10.18637/jss.v056.i05},
	abstract = {Over the last twenty years there have been numerous developments in diagnostic pro- cedures for hierarchical linear models; however, these procedures are not widely imple- mented in statistical software packages, and those packages that do contain a complete framework for model assessment are not open source. The lack of availability of diagnostic procedures for hierarchical linear models has limited their adoption in statistical practice. The R package HLMdiag provides diagnostic tools targeting all aspects and levels of continuous response hierarchical linear models with strictly nested dependence structures fit using the lmer() function in the lme4 package. In this paper we discuss the tools implemented in HLMdiag for both residual and influence analysis.},
	language = {en},
	urldate = {2022-01-21},
	journal = {Journal of Statistical Software},
	author = {Loy, Adam and Hofmann, Heike},
	month = jan,
	year = {2014},
	pages = {1--28},
	file = {Full Text:/Users/patrickli/Zotero/storage/2HBNQJMS/Loy and Hofmann - 2014 - HLMdiag A Suite of Diagnostics for Hierarchical L.pdf:application/pdf},
}

@article{buja_discussion_2014,
	title = {Discussion: “{A} significance test for the lasso”},
	volume = {42},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Discussion},
	url = {http://projecteuclid.org/journals/annals-of-statistics/volume-42/issue-2/Discussion-A-significance-test-for-the-lasso/10.1214/14-AOS1175F.full},
	doi = {10.1214/14-AOS1175F},
	abstract = {The Annals of Statistics},
	number = {2},
	urldate = {2022-01-21},
	journal = {The Annals of Statistics},
	author = {Buja, A. and Brown, L.},
	month = apr,
	year = {2014},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {509--517},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/UR77UX4S/Buja and Brown - 2014 - Discussion “A significance test for the lasso”.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/MH2L5GQW/14-AOS1175F.html:text/html},
}

@article{majumder_human_2014,
	title = {Human {Factors} {Influencing} {Visual} {Statistical} {Inference}},
	url = {http://arxiv.org/abs/1408.1974},
	abstract = {Visual statistical inference is a way to determine significance of patterns found while exploring data. It is dependent on the evaluation of a lineup, of a data plot among a sample of null plots, by human observers. Each individual is different in their cognitive psychology and judiciousness, which can affect the visual inference. The usual way to estimate the effectiveness of a statistical test is its power. The estimate of power of a lineup can be controlled by combining evaluations from multiple observers. Factors that may also affect the power of visual inference are the observers' demographics, visual skills, and experience, the sample of null plots taken from the null distribution, the position of the data plot in the lineup, and the signal strength in the data. This paper examines these factors. Results from multiple visual inference studies using Amazon's Mechanical Turk are examined to provide an assessment of these. The experiments suggest that individual skills vary substantially, but demographics do not have a huge effect on performance. There is evidence that a learning effect exists but only in that observers get faster with repeated evaluations, but not more often correct. The placement of data plot in the lineup does not affect the inference.},
	journal = {arXiv:1408.1974 [stat]},
	author = {Majumder, Mahbubul and Hofmann, Heike and Cook, Dianne},
	month = aug,
	year = {2014},
	keywords = {Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/ICGTP8PL/Majumder et al. - 2014 - Human Factors Influencing Visual Statistical Infer.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/5CV5X3EL/1408.html:text/html},
}

@article{chowdhury_utilizing_2014,
	title = {Utilizing {Distance} {Metrics} on {Lineups} to {Examine} {What} {People} {Read} {From} {Data} {Plots}},
	url = {http://arxiv.org/abs/1408.1889},
	abstract = {Graphics play a crucial role in statistical analysis and data mining. This paper describes metrics developed to assist the use of lineups for making inferential statements. Lineups embed the plot of the data among a set of null plots, and engage a human observer to select the plot that is most different from the rest. If the data plot is selected it corresponds to the rejection of a null hypothesis. Metrics are calculated in association with lineups, to measure the quality of the lineup, and help to understand what people see in the data plots. The null plots represent a finite sample from a null distribution, and the selected sample potentially affects the ease or difficulty of a lineup. Distance metrics are designed to describe how close the true data plot is to the null plots, and how close the null plots are to each other. The distribution of the distance metrics is studied to learn how well this matches to what people detect in the plots, the effect of null generating mechanism and plot choices for particular tasks. The analysis was conducted on data that has already been collected from Amazon Turk studies conducted with lineups for studying an array of data analysis tasks.},
	urldate = {2022-01-21},
	journal = {arXiv:1408.1889 [stat]},
	author = {Chowdhury, Niladri Roy and Cook, Dianne and Hofmann, Heike and Majumder, Mahbubul and Zhao, Yifan},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.1889},
	keywords = {Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/V7JQW2YD/Chowdhury et al. - 2014 - Utilizing Distance Metrics on Lineups to Examine W.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/28FC7MS7/1408.html:text/html},
}

@article{hullman_hypothetical_2015,
	title = {Hypothetical {Outcome} {Plots} {Outperform} {Error} {Bars} and {Violin} {Plots} for {Inferences} about {Reliability} of {Variable} {Ordering}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444},
	doi = {10.1371/journal.pone.0142444},
	abstract = {Many visual depictions of probability distributions, such as error bars, are difficult for users to accurately interpret. We present and study an alternative representation, Hypothetical Outcome Plots (HOPs), that animates a finite set of individual draws. In contrast to the statistical background required to interpret many static representations of distributions, HOPs require relatively little background knowledge to interpret. Instead, HOPs enables viewers to infer properties of the distribution using mental processes like counting and integration. We conducted an experiment comparing HOPs to error bars and violin plots. With HOPs, people made much more accurate judgments about plots of two and three quantities. Accuracy was similar with all three representations for most questions about distributions of a single quantity.},
	language = {en},
	number = {11},
	urldate = {2022-01-23},
	journal = {PLOS ONE},
	author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
	month = nov,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Normal distribution, Probability density, Probability distribution, Random variables, Sea water, Statistical distributions, Statistical inference, Vision},
	pages = {e0142444},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/7J9AWNM3/Hullman et al. - 2015 - Hypothetical Outcome Plots Outperform Error Bars a.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/W2VV4FRC/article.html:text/html},
}

@article{anand_automatic_2016,
	title = {Automatic {Selection} of {Partitioning} {Variables} for {Small} {Multiple} {Displays}},
	volume = {22},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2015.2467323},
	abstract = {Effective small multiple displays are created by partitioning a visualization on variables that reveal interesting conditional structure in the data. We propose a method that automatically ranks partitioning variables, allowing analysts to focus on the most promising small multiple displays. Our approach is based on a randomized, non-parametric permutation test, which allows us to handle a wide range of quality measures for visual patterns defined on many different visualization types, while discounting spurious patterns. We demonstrate the effectiveness of our approach on scatterplots of real-world, multidimensional datasets.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Anand, Anushka and Talbot, Justin},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Histograms, Visual analytics, Algorithm design and analysis, Data visualization, Employment, Multidimensional data, Partitioning algorithms, Small multiple displays, Visualization selection},
	pages = {669--677},
	file = {IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/AKXCGD9Q/Anand and Talbot - 2016 - Automatic Selection of Partitioning Variables for .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/T7HSDUGS/7192658.html:text/html},
}

@article{ver_hoef_iterating_2015,
	title = {Iterating on a single model is a viable alternative to multimodel inference},
	volume = {79},
	issn = {1937-2817},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/jwmg.891},
	doi = {10.1002/jwmg.891},
	abstract = {Multimodel inference accommodates uncertainty when selecting or averaging models, which seems logical and natural. However, there are costs associated with multimodel inferences, so they are not always appropriate or desirable. First, we present statistical inference in the big picture of data analysis and the deductive–inductive process of scientific discovery. Inferences on fixed states of nature, such as survey sampling methods, generally use a single model. Multimodel inferences are used primarily when modeling processes of nature, when there is no hope of knowing the true model. However, even in these cases, iterating on a single model may meet objectives without introducing additional complexity. Additionally, discovering new features in the data through model diagnostics is easier when considering a single model. There are costs for multimodel inferences, including the coding, computing, and summarization time on each model. When cost is included, a reasonable strategy may often be iterating on a single model. We recommend that researchers and managers carefully examine objectives and cost when considering multimodel inference methods. Published 2015. This article is a U.S. Government work and is in the public domain in the USA.},
	language = {en},
	number = {5},
	urldate = {2022-01-23},
	journal = {The Journal of Wildlife Management},
	author = {Ver Hoef, Jay M. and Boveng, Peter L.},
	year = {2015},
	note = {\_eprint: https://wildlife.onlinelibrary.wiley.com/doi/pdf/10.1002/jwmg.891},
	keywords = {information criteria, model averaging, model diagnostics, model selection, philosophy, scientific method},
	pages = {719--729},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/D6PPMTJD/Ver Hoef and Boveng - 2015 - Iterating on a single model is a viable alternativ.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/QBXD7LNW/jwmg.html:text/html},
}

@article{vanderplas_spatial_2016,
	title = {Spatial {Reasoning} and {Data} {Displays}},
	volume = {22},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2015.2469125},
	abstract = {Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {VanderPlas, Susan and Hofmann, Heike},
	month = jan,
	year = {2016},
	keywords = {Protocols, Visualization, Statistical graphics, Data visualization, Atmospheric measurements, Cognition, Data visualization,, Particle measurements, Perception, Sociology, Statistical computing},
	pages = {459--468},
	file = {IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/FPS6UKUY/7217849.html:text/html;IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/QZAZNV55/VanderPlas and Hofmann - 2016 - Spatial Reasoning and Data Displays.pdf:application/pdf},
}

@article{roy_chowdhury_using_2015,
	title = {Using visual statistical inference to better understand random class separations in high dimension, low sample size data},
	volume = {30},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-014-0534-x},
	doi = {10.1007/s00180-014-0534-x},
	abstract = {Statistical graphics play an important role in exploratory data analysis, model checking and diagnosis. With high dimensional data, this often means plotting low-dimensional projections, for example, in classification tasks projection pursuit is used to find low-dimensional projections that reveal differences between labelled groups. In many contemporary data sets the number of observations is relatively small compared to the number of variables, which is known as a high dimension low sample size (HDLSS) problem. This paper explores the use of visual inference on understanding low-dimensional pictures of HDLSS data. Visual inference helps to quantify the significance of findings made from graphics. This approach may be helpful to broaden the understanding of issues related to HDLSS data in the data analysis community. Methods are illustrated using data from a published paper, which erroneously found real separation in microarray data, and with a simulation study conducted using Amazon’s Mechanical Turk.},
	language = {en},
	number = {2},
	journal = {Computational Statistics},
	author = {Roy Chowdhury, Niladri and Cook, Dianne and Hofmann, Heike and Majumder, Mahbubul and Lee, Eun-Kyung and Toth, Amy L.},
	month = jun,
	year = {2015},
	pages = {293--316},
	file = {Springer Full Text PDF:/Users/patrickli/Zotero/storage/GYZEGWDX/Roy Chowdhury et al. - 2015 - Using visual statistical inference to better under.pdf:application/pdf},
}

@article{loy_are_2015,
	title = {Are {You} {Normal}? {The} {Problem} of {Confounded} {Residual} {Structures} in {Hierarchical} {Linear} {Models}},
	volume = {24},
	issn = {1061-8600},
	shorttitle = {Are {You} {Normal}?},
	url = {https://doi.org/10.1080/10618600.2014.960084},
	doi = {10.1080/10618600.2014.960084},
	abstract = {We encounter hierarchical data structures in a wide range of applications. Regular linear models are extended by random effects to address correlation between observations in the same group. Inference for random effects is sensitive to distributional misspecifications of the model, making checks for (distributional) assumptions particularly important. The investigation of residual structures is complicated by the presence of different levels and corresponding dependencies. Ignoring these dependencies leads to erroneous conclusions using our familiar tools, such as Q–Q plots or normal tests. We first show the extent of the problem, then we introduce the fraction of confounding as a measure of the level of confounding in a model and finally introduce rotated random effects as a solution to assessing distributional model assumptions. This article has supplementary materials online.},
	number = {4},
	urldate = {2022-01-23},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Loy, Adam and Hofmann, Heike},
	month = oct,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2014.960084},
	keywords = {Diagnostic, Multilevel model, Q–Q plot, Random effects distribution},
	pages = {1191--1209},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/JMLF2BBY/Loy and Hofmann - 2015 - Are You Normal The Problem of Confounded Residual.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/9V6JK2ZH/10618600.2014.html:text/html},
}

@article{franke_statistical_2016,
	title = {Statistical {Inference}, {Learning} and {Models} in {Big} {Data}},
	volume = {84},
	issn = {1751-5823},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12176},
	doi = {10.1111/insr.12176},
	abstract = {The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application and including some examples of applications to make these challenges and strategies more concrete.},
	language = {en},
	number = {3},
	urldate = {2022-01-23},
	journal = {International Statistical Review},
	author = {Franke, Beate and Plante, Jean-François and Roscher, Ribana and Lee, En-shiun Annie and Smyth, Cathal and Hatefi, Armin and Chen, Fuqi and Gil, Einat and Schwing, Alexander and Selvitella, Alessandro and Hoffman, Michael M. and Grosse, Roger and Hendricks, Dieter and Reid, Nancy},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12176},
	keywords = {high-dimensional data, aggregation, computational complexity, dimension reduction, networks, streaming data},
	pages = {371--389},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/8R8LPXSE/Franke et al. - 2016 - Statistical Inference, Learning and Models in Big .pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/6KBB8XET/insr.html:text/html},
}

@article{loy_variations_2016,
	title = {Variations of {Q}–{Q} {Plots}: {The} {Power} of {Our} {Eyes}!},
	volume = {70},
	issn = {0003-1305},
	shorttitle = {Variations of {Q}–{Q} {Plots}},
	url = {https://doi.org/10.1080/00031305.2015.1077728},
	doi = {10.1080/00031305.2015.1077728},
	abstract = {In statistical modeling, we strive to specify models that resemble data collected in studies or observed from processes. Consequently, distributional specification and parameter estimation are central to parametric models. Graphical procedures, such as the quantile–quantile (Q–Q) plot, are arguably the most widely used method of distributional assessment, though critics find their interpretation to be overly subjective. Formal goodness of fit tests are available and are quite powerful, but only indicate whether there is a lack of fit, not why there is lack of fit. In this article, we explore the use of the lineup protocol to inject rigor into graphical distributional assessment and compare its power to that of formal distributional tests. We find that lineup tests are considerably more powerful than traditional tests of normality. A further investigation into the design of Q–Q plots shows that de-trended Q–Q plots are more powerful than the standard approach as long as the plot preserves distances in x and y to be the same. While we focus on diagnosing nonnormality, our approach is general and can be directly extended to the assessment of other distributions.},
	number = {2},
	journal = {The American Statistician},
	author = {Loy, Adam and Follett, Lendie and Hofmann, Heike},
	month = apr,
	year = {2016},
	keywords = {Visual inference, Statistical graphics, Lineup protocol, Normality test, Quantile–Quantile plot},
	pages = {202--214},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/MN4EYZAE/Loy et al. - 2016 - Variations of Q–Q Plots The Power of Our Eyes!.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/6T2YP5SC/00031305.2015.html:text/html},
}

@article{valverde-albacete_supporting_2016,
	title = {Supporting scientific knowledge discovery with extended, generalized {Formal} {Concept} {Analysis}},
	volume = {44},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417415006442},
	doi = {10.1016/j.eswa.2015.09.022},
	abstract = {In this paper we fuse together the Landscapes of Knowledge of Wille’s and Exploratory Data Analysis by leveraging Formal Concept Analysis (FCA) to support data-induced scientific enquiry and discovery. We use extended FCA first by allowing K-valued entries in the incidence to accommodate other, non-binary types of data, and second with different modes of creating formal concepts to accommodate diverse conceptualizing phenomena. With these extensions we demonstrate the versatility of the Landscapes of Knowledge metaphor to help in creating new scientific and engineering knowledge by providing several successful use cases of our techniques that support scientific hypothesis-making and discovery in a range of domains: semiring theory, perceptual studies, natural language semantics, and gene expression data analysis. While doing so, we also capture the affordances that justify the use of FCA and its extensions in scientific discovery.},
	language = {en},
	urldate = {2022-01-23},
	journal = {Expert Systems with Applications},
	author = {Valverde-Albacete, Francisco J. and González-Calabozo, José María and Peñas, Anselmo and Peláez-Moreno, Carmen},
	month = feb,
	year = {2016},
	keywords = {-Formal Concept Analysis, Confusion matrix, Exploratory Data Analysis, Extended Formal Concept Analysis, Formal Concept Analysis, Gene expression data, Landscapes of Knowledge, Metaphor theory, Relation extraction, Scientific knowledge discovery, Semiring theory},
	pages = {198--216},
	file = {ScienceDirect Snapshot:/Users/patrickli/Zotero/storage/NK9GAWKR/S0957417415006442.html:text/html;Accepted Version:/Users/patrickli/Zotero/storage/2M6T8SU9/Valverde-Albacete et al. - 2016 - Supporting scientific knowledge discovery with ext.pdf:application/pdf},
}

@article{vohl_large-scale_2016,
	title = {Large-scale comparative visualisation of sets of multidimensional data},
	volume = {2},
	issn = {2376-5992},
	url = {http://arxiv.org/abs/1610.00760},
	doi = {10.7717/peerj-cs.88},
	abstract = {We present encube \$-\$ a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: 1) the integration of comparative visualisation and analysis into a unified system; 2) the documentation of the discovery process; and 3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios.},
	urldate = {2022-01-23},
	journal = {PeerJ Computer Science},
	author = {Vohl, Dany and Barnes, David G. and Fluke, Christopher J. and Poudel, Govinda and Georgiou-Karistianis, Nellie and Hassan, Amr H. and Benovitski, Yuri and Wong, Tsz Ho and Kaluza, Owen and Nguyen, Toan D. and Bonnington, C. Paul},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.00760},
	keywords = {Computer Science - Human-Computer Interaction, Astrophysics - Instrumentation and Methods for Astrophysics},
	pages = {e88},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/U3B56PTC/Vohl et al. - 2016 - Large-scale comparative visualisation of sets of m.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/B33ZQ6QP/1610.html:text/html},
}

@article{wheeler_tables_2016,
	title = {Tables and graphs for monitoring temporal crime trends: {Translating} theory into practical crime analysis advice},
	volume = {18},
	issn = {1461-3557},
	shorttitle = {Tables and graphs for monitoring temporal crime trends},
	url = {https://doi.org/10.1177/1461355716642781},
	doi = {10.1177/1461355716642781},
	abstract = {This article is a practical review on how to construct tables and graphs to monitor temporal crime trends. Such advice is mostly applicable to crime analysts to improve the readability of their products, but is also useful to general consumers of crime statistics in trying to identify crime trends in reported data. First, the use of percent change to identify significant changes in crime trends is critiqued, and an alternative metric based on the Poisson distribution is provided. Second, visualization principles for constructing tables are provided, and a practical example of remaking a poor table using these guidelines is shown. Finally, the utility of using time series charts to easily identify short- and long-term increases, as well as outliers in seasonal data using examples with actual crime data is illustrated.},
	language = {en},
	number = {3},
	urldate = {2022-01-23},
	journal = {International Journal of Police Science \& Management},
	author = {Wheeler, Andrew P},
	month = sep,
	year = {2016},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {crime analysis, crime statistics, crime trends, Tables, time series},
	pages = {159--172},
	file = {SAGE PDF Full Text:/Users/patrickli/Zotero/storage/UR38GPT4/Wheeler - 2016 - Tables and graphs for monitoring temporal crime tr.pdf:application/pdf},
}

@article{oldford_self-calibrating_2016,
	title = {Self-{Calibrating} {Quantile}–{Quantile} {Plots}},
	volume = {70},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2015.1090338},
	doi = {10.1080/00031305.2015.1090338},
	abstract = {Quantile–quantile plots, or qqplots, are an important visual tool for many applications but their interpretation requires some care and often more experience. This apparent subjectivity is unnecessary. By drawing on the computational and display facilities now widely available, qqplots are easily enriched to help with their interpretation. An overview of quantile functions and quantile–quantile plots is presented against the backdrop of their early historical development. Strengths and shortcomings of the traditional display are described. A new enhanced qqplot, the self-calibrating qqplot, is introduced and demonstrated on a variety of examples—both synthetic and real. Real examples include normal qqplots, log-normal plots, half-normal plots for factorial experiments, qqplots for and s in process improvement applications, detection of multivariate outliers, and the comparison of empirical distributions. Self-calibration is had by visually incorporating sampling variation in the qqplot display in a variety of ways. The new qqplot is available through the function and R package qqtest.[Received December 2014. Revised August 2015.]},
	number = {1},
	urldate = {2022-01-23},
	journal = {The American Statistician},
	author = {Oldford, R. Wayne},
	month = jan,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2015.1090338},
	keywords = {{\textless}img src="/na101/home/literatum/publisher/tandf/journals/content/utas20/2016/utas20.v070.i01/00031305.2015.1090338/20161121/images/utas\_a\_1090338\_ilm0002.gif" alt="" /{\textgreater} and s charts, Daniel plots, Half-normal plots, Multivariate outlier detection, Ogive, Visual hypothesis testing.},
	pages = {74--90},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/NHBEKGYD/Oldford - 2016 - Self-Calibrating Quantile–Quantile Plots.pdf:application/pdf},
}

@techreport{davies_stylized_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Stylized {Facts} and {Simulating} {Long} {Range} {Financial} {Data}},
	url = {http://papers.ssrn.com/abstract=2763508},
	abstract = {We propose a new method (implemented in an R-program) to simulate long-range daily stock-price data. The program reproduces various stylized facts much better than various parametric models from the extended GARCH-family. In particular, the empirically observed changes in unconditional variance are truthfully mirrored in the simulated data.},
	language = {en},
	number = {ID 2763508},
	urldate = {2022-01-23},
	institution = {Social Science Research Network},
	author = {Davies, Laurie and Kraemer, Walter},
	month = mar,
	year = {2016},
	doi = {10.2139/ssrn.2763508},
	keywords = {empirical economics, GARCH modelling, long-range daily stock-price, stylized facts},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/X5CRQ8G8/Davies and Kraemer - 2016 - Stylized Facts and Simulating Long Range Financial.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/F6G6AEJ9/papers.html:text/html},
}

@article{widen_graphical_2016,
	title = {Graphical {Inference} in {Geographical} {Research}},
	volume = {48},
	issn = {1538-4632},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/gean.12085},
	doi = {10.1111/gean.12085},
	abstract = {Graphical inference, a process refined by Buja et al., can be a useful tool for geographers as it provides a visual and spatial method to test null hypotheses. The core idea is to generate sample datasets from a null hypothesis to visually compare with the actual dataset. The comparison is performed from a line-up of graphs where a single graph of the actual data is hidden among multiple graphs of sample data. If the real data is discernible, the null hypothesis can be rejected. Here, we illustrate the utility of graphical inference using examples from climatology, biogeography, and health geography. The examples include inferences about location of the mean, change across space and time, and clustering. We show that graphical inference is a useful technique to answer a broad range of common questions in geographical datasets. This approach is needed to avoid the common pitfalls of “straw man” hypotheses and “p-hacking” as datasets become increasingly larger and more complex.},
	language = {en},
	number = {2},
	journal = {Geographical Analysis},
	author = {Widen, Holly M. and Elsner, James B. and Pau, Stephanie and Uejio, Christopher K.},
	year = {2016},
	pages = {115--131},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/7TYYPFC9/Widen et al. - 2016 - Graphical Inference in Geographical Research.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/EKWTELLQ/gean.html:text/html},
}

@article{buja_statistical_2009,
	title = {Statistical inference for exploratory data analysis and model diagnostics},
	volume = {367},
	url = {https://royalsocietypublishing-org.ezproxy.lib.monash.edu.au/doi/full/10.1098/rsta.2009.0120},
	doi = {10.1098/rsta.2009.0120},
	abstract = {We propose to furnish visual statistical methods with an inferential framework and protocol, modelled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests. Statistical significance of ‘discoveries’ is measured by having the human viewer compare the plot of the real dataset with collections of plots of simulated datasets. A simple but rigorous protocol that provides inferential validity is modelled after the ‘lineup’ popular from criminal legal procedures. Another protocol modelled after the ‘Rorschach’ inkblot test, well known from (pop-)psychology, will help analysts acclimatize to random variability before being exposed to the plot of the real data. The proposed protocols will be useful for exploratory data analysis, with reference datasets simulated by using a null assumption that structure is absent. The framework is also useful for model diagnostics in which case reference datasets are simulated from the model in question. This latter point follows up on previous proposals. Adopting the protocols will mean an adjustment in working procedures for data analysts, adding more rigour, and teachers might find that incorporating these protocols into the curriculum improves their students’ statistical thinking.},
	number = {1906},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Buja, Andreas and Cook, Dianne and Hofmann, Heike and Lawrence, Michael and Lee, Eun-Kyung and Swayne, Deborah F. and Wickham, Hadley},
	month = nov,
	year = {2009},
	keywords = {cognitive perception, permutation tests, rotation tests, simulation, statistical graphics, visual data mining},
	pages = {4361--4383},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/7IRGYLNU/Buja et al. - 2009 - Statistical inference for exploratory data analysi.pdf:application/pdf},
}

@article{donoho_50_2017,
	title = {50 {Years} of {Data} {Science}},
	volume = {26},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2017.1384734},
	doi = {10.1080/10618600.2017.1384734},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Donoho, David},
	year = {2017},
	keywords = {Statistics, Correction, Cross-study analysis, Data analysis, Data science, Meta analysis, Predictive modeling, Quantitative programming environments},
	pages = {745--766},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/LZZPKHMV/Donoho - 2017 - 50 Years of Data Science.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/EWMUZX9K/10618600.2017.html:text/html},
}

@article{cleveland_graphical_1984,
	title = {Graphical {Perception}: {Theory}, {Experimentation}, and {Application} to the {Development} of {Graphical} {Methods}},
	volume = {79},
	issn = {0162-1459},
	shorttitle = {Graphical {Perception}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478080},
	doi = {10.1080/01621459.1984.10478080},
	abstract = {The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction: Graphs should employ elementary tasks as high in the ordering as possible. This principle is applied to a variety of graphs, including bar charts, divided bar charts, pie charts, and statistical maps with shading. The conclusion is that radical surgery on these popular graphs is needed, and as replacements we offer alternative graphical forms—dot charts, dot charts with grouping, and framed-rectangle charts.},
	number = {387},
	journal = {Journal of the American Statistical Association},
	author = {Cleveland, William S. and McGill, Robert},
	month = sep,
	year = {1984},
	keywords = {Computer graphics, Psychophysics},
	pages = {531--554},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/LQKKWI3U/Cleveland and McGill - 1984 - Graphical Perception Theory, Experimentation, and.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/8EZK4LGD/01621459.1984.html:text/html},
}

@inproceedings{wilkinson_graph-theoretic_2005,
	title = {Graph-theoretic scagnostics},
	doi = {10.1109/INFVIS.2005.1532142},
	abstract = {We introduce Tukey and Tukey scagnostics and develop graph-theoretic methods for implementing their procedure on large datasets.},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization}, 2005. {INFOVIS} 2005.},
	author = {Wilkinson, L. and Anand, A. and Grossman, R.},
	month = oct,
	year = {2005},
	note = {ISSN: 1522-404X},
	keywords = {Visualization, Area measurement, Computer interfaces, Covariance matrix, Density measurement, Displays, Kernel, Length measurement, Scattering, Symmetric matrices},
	pages = {157--164},
	file = {IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/33Y3W5Y6/Wilkinson et al. - 2005 - Graph-theoretic scagnostics.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/3BLIFLN6/1532142.html:text/html},
}

@article{laib_spatial_2016,
	series = {European {Geosciences} {Union} {General} {Assembly} 2016, {EGU} {Division} {Energy}, {Resources} \& the {Environment} ({ERE})},
	title = {Spatial {Modelling} of {Extreme} {Wind} {Speed} {Distributions} in {Switzerland}},
	volume = {97},
	issn = {1876-6102},
	url = {https://www.sciencedirect.com/science/article/pii/S1876610216309778},
	doi = {10.1016/j.egypro.2016.10.029},
	abstract = {The understanding of wind phenomena is of primary importance, especially regarding the renewable energy. One traditional way of wind modelling concerns the use of parametric probability distributions. This work presents a method in two steps to model this phenomena. The first of which studies and compares different extreme distributions by modelling data collected from the Swiss meteorological service. This comparison should serve as a basis for the second step, which applies spatial modelling of distribution parameters throughout the country. The modelling is performed in a high dimensional input space with the help of extreme learning machine. The knowledge of probability distributions allows us to give a comprehensive information about a wind speed distribution.},
	language = {en},
	urldate = {2022-01-30},
	journal = {Energy Procedia},
	author = {Laib, Mohamed and Kanevski, Mikhail},
	month = nov,
	year = {2016},
	keywords = {Extreme values, Machine learning algorithms, Spatial modelling, Switzerland., Wind speed},
	pages = {100--107},
	file = {ScienceDirect Full Text PDF:/Users/patrickli/Zotero/storage/2MUE3GFF/Laib and Kanevski - 2016 - Spatial Modelling of Extreme Wind Speed Distributi.pdf:application/pdf;ScienceDirect Snapshot:/Users/patrickli/Zotero/storage/JWT9GUIF/S1876610216309778.html:text/html},
}

@article{russell_artificial_2002,
	title = {Artificial intelligence: a modern approach},
	author = {Russell, Stuart and Norvig, Peter},
	year = {2002},
	file = {Russell and Norvig - 2002 - Artificial intelligence a modern approach.pdf:/Users/patrickli/Zotero/storage/GFC6FBPS/Russell and Norvig - 2002 - Artificial intelligence a modern approach.pdf:application/pdf},
}

@article{turing_computing_1950,
	title = {Computing machinery and intelligence},
	journal = {The Turing Test: Verbal Behavior as the Hallmark of Intelligence},
	author = {Turing, Alan M and Haugeland, J},
	year = {1950},
	pages = {29--56},
}

@article{harnad_other_1991,
	title = {Other bodies, other minds: {A} machine incarnation of an old philosophical problem},
	volume = {1},
	number = {1},
	journal = {Minds and Machines},
	author = {Harnad, Stevan},
	year = {1991},
	note = {Publisher: Springer},
	pages = {43--54},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	language = {en},
	number = {6245},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	pages = {255--260},
	file = {Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:/Users/patrickli/Zotero/storage/T2C8QUCE/Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf},
}

@article{silver_general_2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aar6404},
	doi = {10.1126/science.aar6404},
	abstract = {The game of chess is the longest-studied domain in the history of artiﬁcial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-speciﬁc adaptations, and handcrafted evaluation functions that have been reﬁned by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.},
	language = {en},
	number = {6419},
	urldate = {2022-02-09},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2018},
	pages = {1140--1144},
	file = {Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:/Users/patrickli/Zotero/storage/VUFPGF55/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	shorttitle = {Statistical {Modeling}},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199--231},
	file = {Full Text:/Users/patrickli/Zotero/storage/GUAW8G9R/Breiman - 2001 - Statistical Modeling The Two Cultures (with comme.pdf:application/pdf},
}

@misc{noauthor_full_nodate,
	title = {Full article: {Bringing} {Visual} {Inference} to the {Classroom}},
	url = {https://www-tandfonline-com.ezproxy.lib.monash.edu.au/doi/full/10.1080/26939169.2021.1920866},
	urldate = {2022-02-10},
	file = {Full article\: Bringing Visual Inference to the Classroom:/Users/patrickli/Zotero/storage/R5UN6INQ/26939169.2021.html:text/html},
}

@book{r_core_team_r_2021,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2021},
}

@book{van_rossum_python_2009,
	address = {Scotts Valley, CA},
	title = {Python 3 {Reference} {Manual}},
	isbn = {1-4414-1269-7},
	publisher = {CreateSpace},
	author = {Van Rossum, Guido and Drake, Fred L.},
	year = {2009},
}

@book{grinberg_flask_2018,
	title = {Flask web development: developing web applications with python},
	publisher = {" O'Reilly Media, Inc."},
	author = {Grinberg, Miguel},
	year = {2018},
}

@book{flanagan_javascript_2006,
	title = {{JavaScript}: the definitive guide},
	publisher = {" O'Reilly Media, Inc."},
	author = {Flanagan, David},
	year = {2006},
}

@article{de_leeuw_jspsych_2015,
	title = {{jsPsych}: {A} {JavaScript} library for creating behavioral experiments in a {Web} browser},
	volume = {47},
	number = {1},
	journal = {Behavior research methods},
	author = {De Leeuw, Joshua R},
	year = {2015},
	pages = {1--12},
}

@article{palan_prolificacsubject_2018,
	title = {Prolific.ac—{A} subject pool for online experiments},
	volume = {17},
	issn = {22146350},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214635017300989},
	doi = {10.1016/j.jbef.2017.12.004},
	abstract = {The number of online experiments conducted with subjects recruited via online platforms has grown considerably in the recent past. While one commercial crowdworking platform – Amazon’s Mechanical Turk – basically has established and since dominated this field, new alternatives offer services explicitly targeted at researchers. In this article, we present www.prolific.ac and lay out its suitability for recruiting subjects for social and economic science experiments. After briefly discussing key advantages and challenges of online experiments relative to lab experiments, we trace the platform’s historical development, present its features, and contrast them with requirements for different types of social and economic experiments.},
	language = {en},
	journal = {Journal of Behavioral and Experimental Finance},
	author = {Palan, Stefan and Schitter, Christian},
	month = mar,
	year = {2018},
	pages = {22--27},
	file = {Palan and Schitter - 2018 - Prolific.ac—A subject pool for online experiments.pdf:/Users/patrickli/Zotero/storage/78FGXP8T/Palan and Schitter - 2018 - Prolific.ac—A subject pool for online experiments.pdf:application/pdf},
}

@misc{prolific_prolific_2022,
	title = {Prolific},
	url = {https://www.prolific.co},
	author = {Prolific},
	year = {2022},
}

@misc{pythonanywhere_pythonanywhere_2022,
	title = {{PythonAnywhere}},
	url = {https://www.pythonanywhere.com},
	author = {PythonAnywhere},
	year = {2022},
}

@misc{gtihub_github_2022,
	title = {Github},
	url = {https://www.github.com/},
	author = {Gtihub},
	year = {2022},
}

@misc{gtihub_github_2022-1,
	title = {Github {Pages}},
	url = {https://pages.github.com/},
	author = {Gtihub},
	year = {2022},
}

@article{breusch_simple_1979,
	title = {A simple test for heteroscedasticity and random coefficient variation},
	journal = {Econometrica: Journal of the econometric society},
	author = {Breusch, Trevor S and Pagan, Adrian R},
	year = {1979},
	note = {Publisher: JSTOR},
	pages = {1287--1294},
}

@inproceedings{buja_inference_1999,
	title = {Inference for data visualization},
	booktitle = {Joint {Statistics} {Meetings}, {August}},
	author = {Buja, Andreas and Cook, Dianne and Swayne, D},
	year = {1999},
	file = {Buja et al. - 1999 - Inference for data visualization.pdf:/Users/patrickli/Zotero/storage/CJ6NIMQP/Buja et al. - 1999 - Inference for data visualization.pdf:application/pdf},
}

@article{gelman_exploratory_2004,
	title = {Exploratory {Data} {Analysis} for {Complex} {Models}},
	volume = {13},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186004X11435},
	doi = {10.1198/106186004X11435},
	language = {en},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gelman, Andrew},
	month = dec,
	year = {2004},
	pages = {755--779},
	file = {Gelman - 2004 - Exploratory Data Analysis for Complex Models.pdf:/Users/patrickli/Zotero/storage/2V8MXHK5/Gelman - 2004 - Exploratory Data Analysis for Complex Models.pdf:application/pdf},
}

@article{gelman_bayesian_2003,
	title = {A {Bayesian} {Formulation} of {Exploratory} {Data} {Analysis} and {Goodness}-of-fit {Testing}*},
	volume = {71},
	issn = {1751-5823},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2003.tb00203.x},
	doi = {10.1111/j.1751-5823.2003.tb00203.x},
	abstract = {Exploratory data analysis (EDA) and Bayesian inference (or, more generally, complex statistical modeling)—which are generally considered as unrelated statistical paradigms—can be particularly effective in combination. In this paper, we present a Bayesian framework for EDA based on posterior predictive checks. We explain how posterior predictive simulations can be used to create reference distributions for EDA graphs, and how this approach resolves some theoretical problems in Bayesian data analysis. We show how the generalization of Bayesian inference to include replicated data yrep and replicated parameters θrep follows a long tradition of generalizations in Bayesian theory. On the theoretical level, we present a predictive Bayesian formulation of goodness-of-fit testing, distinguishing between p-values (posterior probabilities that specified antisymmetric discrepancy measures will exceed 0) and u-values (data summaries with uniform sampling distributions). We explain that p-values, unlike u-values, are Bayesian probability statements in that they condition on observed data. Having reviewed the general theoretical framework, we discuss the implications for statistical graphics and exploratory data analysis, with the goal being to unify exploratory data analysis with more formal statistical methods based on probability models. We interpret various graphical displays as posterior predictive checks and discuss how Bayesian inference can be used to determine reference distributions. The goal of this work is not to downgrade descriptive statistics, or to suggest they be replaced by Bayesian modeling, but rather to suggest how exploratory data analysis fits into the probability-modeling paradigm. We conclude with a discussion of the implications for practical Bayesian inference. In particular, we anticipate that Bayesian software can be generalized to draw simulations of replicated data and parameters from their posterior predictive distribution, and these can in turn be used to calibrate EDA graphs.},
	language = {en},
	number = {2},
	urldate = {2022-03-06},
	journal = {International Statistical Review},
	author = {Gelman, Andrew},
	year = {2003},
	keywords = {Graphics, “p-value”, “u-value”, Bootstrap, Fisher's exact test, Graphiques, Mixture model, Model checking, Modéles de mèlange, Multiple imputation, p-value, Posterior predictive check, Prior predictive check, u-value, Vérification de modéle, Vérification prédictive a posteriori, Vérification prédictive antérieur},
	pages = {369--382},
	file = {Full Text PDF:/Users/patrickli/Zotero/storage/T7FIV3XH/Gelman - 2003 - A Bayesian Formulation of Exploratory Data Analysi.pdf:application/pdf;Snapshot:/Users/patrickli/Zotero/storage/CNVUT2KX/j.1751-5823.2003.tb00203.html:text/html},
}

@incollection{fukushima_neocognitron_1982,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of visual pattern recognition},
	booktitle = {Competition and cooperation in neural nets},
	publisher = {Springer},
	author = {Fukushima, Kunihiko and Miyake, Sei},
	year = {1982},
	pages = {267--285},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	volume = {1},
	number = {4},
	journal = {Neural computation},
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	year = {1989},
	note = {Publisher: MIT Press},
	pages = {541--551},
}

@article{lee_image_2015,
	title = {Image based computer aided diagnosis system for cancer detection},
	volume = {42},
	number = {12},
	journal = {Expert Systems with Applications},
	author = {Lee, Howard and Chen, Yi-Ping Phoebe},
	year = {2015},
	pages = {5356--5365},
}

@article{brunetti_computer_2018,
	title = {Computer vision and deep learning techniques for pedestrian detection and tracking: {A} survey},
	volume = {300},
	journal = {Neurocomputing},
	author = {Brunetti, Antonio and Buongiorno, Domenico and Trotta, Gianpaolo Francesco and Bevilacqua, Vitoantonio},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {17--33},
}

@article{emami_facial_2012,
	title = {Facial recognition using {OpenCV}},
	volume = {4},
	number = {1},
	journal = {Journal of Mobile, Embedded and Distributed Systems},
	author = {Emami, Shervin and Suciu, Valentin Petrut},
	year = {2012},
	pages = {38--43},
}

@inproceedings{ojeda_multivariate_2020,
	title = {Multivariate time series imaging for short-term precipitation forecasting using convolutional neural networks},
	booktitle = {2020 {International} {Conference} on {Artificial} {Intelligence} in {Information} and {Communication} ({ICAIIC})},
	publisher = {IEEE},
	author = {Ojeda, Sun Arthur A and Solano, Geoffrey A and Peramo, Elmer C},
	year = {2020},
	pages = {33--38},
}

@article{chu_automatic_2019,
	title = {An automatic classification method of well testing plot based on convolutional neural network ({CNN})},
	volume = {12},
	number = {15},
	journal = {Energies},
	author = {Chu, Hongyang and Liao, Xinwei and Dong, Peng and Chen, Zhiming and Zhao, Xiaoliang and Zou, Jiandong},
	year = {2019},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {2846},
}

@article{hailesilassie_financial_2019,
	title = {Financial {Market} {Prediction} {Using} {Recurrence} {Plot} and {Convolutional} {Neural} {Network}},
	author = {Hailesilassie, Tameru},
	year = {2019},
}

@inproceedings{hatami_classification_2018,
	title = {Classification of time-series images using deep convolutional neural networks},
	volume = {10696},
	booktitle = {Tenth international conference on machine vision ({ICMV} 2017)},
	publisher = {International Society for Optics and Photonics},
	author = {Hatami, Nima and Gavet, Yann and Debayle, Johan},
	year = {2018},
	pages = {106960Y},
}

@article{zhang_encoding_2020,
	title = {Encoding time series as multi-scale signed recurrence plots for classification using fully convolutional networks},
	volume = {20},
	number = {14},
	journal = {Sensors},
	author = {Zhang, Ye and Hou, Yi and Zhou, Shilin and Ouyang, Kewei},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3818},
}

@article{chen_convolutional_2020,
	title = {Convolutional neural network analysis of recurrence plots for anomaly detection},
	volume = {30},
	number = {01},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Chen, Yun and Su, Shijie and Yang, Hui},
	year = {2020},
	note = {Publisher: World Scientific},
	pages = {2050002},
}

@article{singh_deep_2017,
	title = {Deep convolutional neural networks for pairwise causality},
	journal = {arXiv preprint arXiv:1701.00597},
	author = {Singh, Karamjit and Gupta, Garima and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
	year = {2017},
}

@article{rawat_deep_2017,
	title = {Deep convolutional neural networks for image classification: {A} comprehensive review},
	volume = {29},
	number = {9},
	journal = {Neural computation},
	author = {Rawat, Waseem and Wang, Zenghui},
	year = {2017},
	pages = {2352--2449},
}

@incollection{araujo_large_2009,
	address = {Berlin, Heidelberg},
	title = {Large {Scale} {Online} {Learning} of {Image} {Similarity} through {Ranking}},
	volume = {5524},
	isbn = {978-3-642-02171-8 978-3-642-02172-5},
	url = {http://link.springer.com/10.1007/978-3-642-02172-5_2},
	abstract = {Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
	editor = {Araujo, Helder and Mendonça, Ana Maria and Pinho, Armando J. and Torres, María Inés},
	year = {2009},
	doi = {10.1007/978-3-642-02172-5_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {11--14},
	file = {Chechik et al. - 2009 - Large Scale Online Learning of Image Similarity th.pdf:/Users/patrickli/Zotero/storage/2J3PV8ZW/Chechik et al. - 2009 - Large Scale Online Learning of Image Similarity th.pdf:application/pdf},
}

@inproceedings{rafiee_review_2010,
	title = {A review of content-based image retrieval},
	doi = {10.1109/CSNDSP16145.2010.5580313},
	abstract = {A comprehensive survey on patch recognition, which is a crucial part of content-based image retrieval (CBIR), is presented. CBIR can be viewed as a methodology in which three correlated modules including patch sampling, characterizing, and recognizing are employed. This paper aims to evaluate meaningful models for one of the most challenging problems in image understanding, specifically, for the effective and efficient mapping between image visual features and high-level semantic concepts. To achieve this, the latest classification, clustering, and interactive methods have been meticulously discussed. Finally, several recommendations for future research issues have been suggested based on the weaknesses of recent technologies.},
	booktitle = {2010 7th {International} {Symposium} on {Communication} {Systems}, {Networks} {Digital} {Signal} {Processing} ({CSNDSP} 2010)},
	author = {Rafiee, G. and Dlay, S.S. and Woo, W.L.},
	month = jul,
	year = {2010},
	keywords = {content-based image retrieval (CBIR), Databases, effective mapping, Fires, Image segmentation, patch characterizing, patch recognizing, patch sampling, Review, semantic concept, Testing},
	pages = {775--779},
	file = {IEEE Xplore Full Text PDF:/Users/patrickli/Zotero/storage/2YSS4SJA/Rafiee et al. - 2010 - A review of content-based image retrieval.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/patrickli/Zotero/storage/CX2WASPM/5580313.html:text/html},
}

@inproceedings{kato_database_1992,
	title = {Database architecture for content-based image retrieval},
	volume = {1662},
	booktitle = {image storage and retrieval systems},
	publisher = {International Society for Optics and Photonics},
	author = {Kato, Toshikazu},
	year = {1992},
	pages = {112--123},
}

@article{zhou_recent_2017,
	title = {Recent {Advance} in {Content}-based {Image} {Retrieval}: {A} {Literature} {Survey}},
	shorttitle = {Recent {Advance} in {Content}-based {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1706.06064},
	abstract = {The explosive increase and ubiquitous accessibility of visual data on the Web have led to the prosperity of research activity in image search or retrieval. With the ignorance of visual content as a ranking clue, methods with text search techniques for visual retrieval may suffer inconsistency between the text words and visual content. Content-based image retrieval (CBIR), which makes use of the representation of visual content to identify relevant images, has attracted sustained attention in recent two decades. Such a problem is challenging due to the intention gap and the semantic gap problems. Numerous techniques have been developed for content-based image retrieval in the last decade. The purpose of this paper is to categorize and evaluate those algorithms proposed during the period of 2003 to 2016. We conclude with several promising directions for future research.},
	journal = {arXiv:1706.06064 [cs]},
	author = {Zhou, Wengang and Li, Houqiang and Tian, Qi},
	month = sep,
	year = {2017},
	keywords = {Computer Science - Information Retrieval, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/U2IECWGE/Zhou et al. - 2017 - Recent Advance in Content-based Image Retrieval A.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/G68GGGKW/1706.html:text/html},
}

@misc{google_google_2022,
	title = {Google {Image}},
	url = {https://www.google.com/imghp},
	author = {Google},
	year = {2022},
}

@misc{tineye_tineye_2022,
	title = {{TinEye}},
	url = {https://www.tineye.com},
	author = {TinEye},
	year = {2022},
}

@article{weiss_survey_2016,
	title = {A survey of transfer learning},
	volume = {3},
	issn = {2196-1115},
	url = {http://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
	doi = {10.1186/s40537-016-0043-6},
	abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
	language = {en},
	number = {1},
	journal = {Journal of Big Data},
	author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
	month = dec,
	year = {2016},
	pages = {9},
	file = {Weiss et al. - 2016 - A survey of transfer learning.pdf:/Users/patrickli/Zotero/storage/Q6FDTEHG/Weiss et al. - 2016 - A survey of transfer learning.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {Imagenet: {A} large-scale hierarchical image database},
	booktitle = {2009 {IEEE} conference on computer vision and pattern recognition},
	publisher = {Ieee},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	pages = {248--255},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/QKKATYW4/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/7G95SPTD/1409.html:text/html},
}

@article{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	journal = {arXiv:1610.02357 [cs]},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv: 1610.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/J7VFNIBH/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/82TM6PGQ/1610.html:text/html},
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jul,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/3N3JP9YI/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/TMTIYU9L/1603.html:text/html},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/6TMG3P2W/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/F46TST8Y/1512.html:text/html},
}

@article{lin_network_2014,
	title = {Network {In} {Network}},
	url = {http://arxiv.org/abs/1312.4400},
	abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	journal = {arXiv:1312.4400 [cs]},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	month = mar,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/472FTKLM/Lin et al. - 2014 - Network In Network.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/FF5VS4V9/1312.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/ZGMBSS6L/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/DU9UFB26/1412.html:text/html},
}

@article{hoi_online_2018,
	title = {Online {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Online {Learning}},
	url = {http://arxiv.org/abs/1802.02871},
	abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	urldate = {2022-03-10},
	journal = {arXiv:1802.02871 [cs]},
	author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.02871},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/patrickli/Zotero/storage/5KN97W8L/Hoi et al. - 2018 - Online Learning A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/patrickli/Zotero/storage/PMCUTFUA/1802.html:text/html},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	number = {1},
	journal = {The journal of machine learning research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}
