---
title: "Supplmentary material - A plot is worth a thousand tests: assessing residual diagnostics with visual inference"
output: 
  pdf_document:
    number_sections: true
author: "Weihao Li, Dianne Cook, Emi Tanaka and Susan VanderPlas"
bibliography: "paper_comparison/paper.bib"
header-includes:
  - \renewcommand\thesection{\Alph{section}}
  - \renewcommand\thesubsection{\thesection.\arabic{subsection}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(visage)
library(tidyverse)
```

```{r}
# OOP supports needed by `visage`
# remotes::install_github("TengMCing/bandicoot")
# 
# Visual inference models and p-value calculation
# remotes::install_github("TengMCing/visage")

library(tidyverse)
library(visage)

# To control the simulation in this file
set.seed(10086)
```

```{r get-lineup-data}
# Create the dir folder
if (!dir.exists(here::here("data/"))) dir.create(here::here("data/"))

# The lineup data used to draw residual plot needed to be downloaded
# from the github repo. Cache it.
if (!file.exists(here::here("data/vi_lineup.rds"))) {
  vi_lineup <- get_vi_lineup()
  saveRDS(vi_lineup, here::here("data/vi_lineup.rds"))
} else {
  vi_lineup <- readRDS(here::here("data/vi_lineup.rds"))
}
```

```{r poly-conventional-simulation}
# Ensure the support of the predictor is [-1, 1]
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

# Run simulations to get the behaviours of conventional tests
if (!file.exists(here::here("data/poly_conventional_simulation.rds"))) {
  poly_conv_sim <- list()
  
  # 100 bootstrap samples
  for (i in 1:100)
  {
    poly_conv_sim[[i]] <- 
      # Every sample contains 2000 lineups
      map(1:2000, function(i) {
        
        # Sample a set of parameters
        shape <- sample(1:4, 1)
        e_sigma <- sample(c(0.5, 1, 2, 4), 1)
        x_dist <- sample(c("uniform", 
                           "normal", 
                           "lognormal", 
                           "even_discrete"), 1)
        x <- switch(x_dist,
                    uniform = rand_uniform(-1, 1),
                    normal = {
                      raw_x <- rand_normal(sigma = 0.3)
                      closed_form(~stand_dist(raw_x))
                      },
                    lognormal = {
                      raw_x <- rand_lognormal(sigma = 0.6)
                      closed_form(~stand_dist(raw_x/3 - 1))
                      },
                    even_discrete = rand_uniform_d(k = 5, even = TRUE))
        
        # Build the model
        mod <- poly_model(shape, x = x, sigma = e_sigma)
        
        # Sample the number of observations
        n <- sample(c(50, 100, 300), 1)
        
        # Generate data from the model 
        tmp_dat <- mod$gen(n)
        
        # Return a data frame containing p-values of
        # F, RESET, BP and SW tests
        tibble(shape = shape,
               e_sigma = e_sigma,
               x_dist = x_dist,
               n = n,
               F_p_value = mod$test(tmp_dat)$p_value,
               RESET3_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:3, 
                                         power_type = "fitted")$p_value,
               RESET4_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:4, 
                                         power_type = "fitted")$p_value,
               RESET5_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:5, 
                                         power_type = "fitted")$p_value,
               RESET6_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:6, 
                                         power_type = "fitted")$p_value,
               RESET7_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:7, 
                                         power_type = "fitted")$p_value,
               RESET8_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:8, 
                                         power_type = "fitted")$p_value,
               RESET9_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:9, 
                                         power_type = "fitted")$p_value,
               RESET10_p_value = mod$test(tmp_dat, 
                                          test = "RESET", 
                                          power = 2:10, 
                                          power_type = "fitted")$p_value,
               BP_p_value = HETER_MODEL$test(tmp_dat)$p_value,
               SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
               boot_id = i)
        }) %>%
  reduce(bind_rows)
  }
  
  poly_conv_sim <- poly_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(poly_conv_sim, here::here("data/poly_conventional_simulation.rds"))
} else {
  poly_conv_sim <- readRDS(here::here("data/poly_conventional_simulation.rds"))
}
```

```{r heter-conventional-simulation}
# Run simulations to get the behaviours of conventional tests
if (!file.exists(here::here("data/heter_conventional_simulation.rds"))) {
  heter_conv_sim <- list()
  
  # 100 bootstrap samples
  for (i in 1:100)
  {
    heter_conv_sim[[i]] <-
      # Every sample contains 2000 lineups
      map(1:2000, function(x) {
        
        # Sample a set of parameters
        a <- sample(c(-1, 0, 1), 1)
        b <- sample(c(0.25, 1, 4, 16, 64), 1)
        x_dist <- sample(c("uniform", 
                           "normal", 
                           "lognormal", 
                           "even_discrete"), 1)
        x <- switch(x_dist,
                    uniform = rand_uniform(-1, 1),
                    normal = {
                      raw_x <- rand_normal(sigma = 0.3)
                      closed_form(~stand_dist(raw_x))
                      },
                    lognormal = {
                      raw_x <- rand_lognormal(sigma = 0.6)
                      closed_form(~stand_dist(raw_x/3 - 1))
                      },
                    even_discrete = rand_uniform_d(-1, 1, k = 5, even = TRUE))
        
        # Build the model
        mod <- heter_model(a = a, b = b, x = x)
        
        # Sample the number of observations
        n <- sample(c(50, 100, 300), 1)
        
        # Generate data from the model 
        tmp_dat <- mod$gen(n)
        
        # Return a data frame containing p-values of
        # F, RESET, BP and SW tests
        tibble(a = a,
               b = b,
               x_dist = x_dist,
               n = n,
               F_p_value = POLY_MODEL$test(
                 
                 # Create a pseudo z to be able to use F-test
                 tmp_dat %>%
                   mutate(z = poly_model()$
                            gen(n, computed = select(tmp_dat, x)) %>%
                            pull(z))
                 )$p_value,
               RESET3_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:3, 
                                                power_type = "fitted")$p_value,
               RESET4_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:4, 
                                                power_type = "fitted")$p_value,
               RESET5_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:5, 
                                                power_type = "fitted")$p_value,
               RESET6_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:6, 
                                                power_type = "fitted")$p_value,
               RESET7_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:7, 
                                                power_type = "fitted")$p_value,
               RESET8_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:8, 
                                                power_type = "fitted")$p_value,
               RESET9_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:9, 
                                                power_type = "fitted")$p_value,
               RESET10_p_value = POLY_MODEL$test(tmp_dat, 
                                                 test = "RESET", 
                                                 power = 2:10, 
                                                 power_type = "fitted")$p_value,
               BP_p_value = mod$test(tmp_dat)$p_value,
               SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
               boot_id = i)
        }) %>%
  reduce(bind_rows)
  }
  
  heter_conv_sim <- heter_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(heter_conv_sim, here::here("data/heter_conventional_simulation.rds"))
} else {
  heter_conv_sim <- readRDS(here::here("data/heter_conventional_simulation.rds"))
}
```

```{r}
# Borrow effect size from the survey
poly_conv_sim <- poly_conv_sim %>%
  left_join(select(filter(vi_survey, type == "polynomial"), shape, e_sigma, n, x_dist, effect_size))

heter_conv_sim <- heter_conv_sim %>%
  left_join(select(filter(vi_survey, type == "heteroskedasticity"), a, b, n, x_dist, effect_size))
```


# Appendix

## Experiment setup

### Mapping of subjects to experimental factors

Mapping of subjects to experimental factors is an important part of experiment design. Essentially, we want to maximum the difference in factors exposed to a subject. For this purpose, we design an algorithm to conduct subject allocation. Let $L$ be a set of available lineups and $S$ be a set of available subjects. According to the experimental design, the availability of a lineup is associated with the number of subjects it can assign to. For lineups with uniform fitted value distribution, this value is 11. And other lineups can be allocated to at most five different subjects. The availability of a subject is associated with the number of lineups that being allocated to this subject. A subject can view at most 18 different lineups. 

The algorithm starts from picking a random subject $s \in S$ with the minimum number of allocated lineups. It then tries to find a lineup $l \in L$ that can maximise the distance metric $D$ and allocate it to subject $s$. Set $L$ and $S$ will be updated and the picking process will be repeated until there is no available lineups or subjects. 

Let $F_1,...,F_q$ be $q$ experimental factors, and $f_1, ...,f_q$ be the corresponding factor values. We say $f_i$ exists in $L_{s}$ if any lineup in $L_{s}$ has this factor value. Similarly, $f_if_j$ exists in $L_{s}$ if any lineup in $L_{s}$ has this pair of factor values. And $f_if_jf_k$ exists in $L_{s}$ if any lineup in $L_{s}$ has this trio of factor values. The distance metric $D$ is defined between a lineup $l$ and a set of lineups $L_{s}$ allocated to a subject $s$ if $L_{s}$ is non-empty: 

\footnotesize

\begin{equation*}
D =
C - \sum_{1\leq i \leq q}I(f_i\text{ exists in }L_{s}) - \sum_{\substack{1\leq i \leq q-1 \\ i < j \leq q}}I(f_if_j\text{ exists in }L_{s}) - \sum_{\substack{1\leq i \leq q - 2 \\ i < j \leq q - 1 \\ j < k \leq q}}I(f_if_jf_k\text{ exists in }L_{s})
\end{equation*}

\normalsize
where $C$ is a sufficiently large constant such that $D > 0$. If $L_{s}$ is empty, we define $D = 0$.

The distance measures how different a lineup is from the set of lineups allocated to the subject in terms of factor values. Thus, the algorithm will try to allocate the most different lineup to a subject at each step.



### Data collection process


The survey data is collected via a self-hosted website designed by us. 

(desc of the tech layers)

Once the participant is recruited from Prolific (ref here), it will be redirected to the entry page of our study website. An image of the entry page is provided in Figure \ref{fig:entry-page}. Then, the participant needs to submit the online consent form and fill in the demographic information as shown in \ref{fig:consent_form} and \ref{fig:metadata} respectively. Before evaluating lineups, participant also need to read the training page as provide in Figure \ref{fig:training-page} to understand the process. An example of the lineup page is given in Figure \ref{fig:lineup-page}. A half of the page is taken by the lineup image to attract participant's attention. The button to skip the selections for the current lineup is intentionally put in the corner of the bounding box with smaller font size, such that participants will not misuse this functionality.

```{r tech, out.width = "100%", fig.cap = "Diagram of online experiment setup. The server-side of the study website uses Flask as backend hosted on PythonAnywhere. And the client-side uses jsPsych to run experiment."}
knitr::include_graphics("img/experiment_tech.pdf")
```


```{r entry-page, out.width = "100%", fig.cap = "The entry page of the study website."}
knitr::include_graphics("img/message.png")
```

```{r consent-form, out.width = "100%", fig.cap = "The consent form provided in the study website."}
knitr::include_graphics("img/consent_form.png")
```

```{r metadata, out.width = "100%", fig.cap = "The form to provide demographic information."}
knitr::include_graphics("img/metadata.png")
```

```{r training-page, out.width = "100%", fig.cap = "The training page of the study website."}
knitr::include_graphics("img/training.png")
```

```{r lineup-page, out.width = "100%", fig.cap = "The lineup page of the study website."}
knitr::include_graphics("img/lineup1.png")
```

## Demographics

Along with the responses to lineups, we have collected a series of demographic information including age, pronoun, education background and previous experience in studies involved data visualization. Figure \ref{fig:education} and \ref{fig:experience} provide summary of the demographic data. 

As shown in Figure \ref{fig:education}, most participants have Diploma or Bachelor degrees, followed by High school or below.
The survey data is gender balanced and the majority of participants are between 18 to 39 years old.

```{r education, fig.cap = "\\label{fig:education}Education distribution of subjects recuritted in this study group by age and pronoun.", fig.height = 4}
vi_survey %>%
  mutate(pronoun = ifelse(pronoun == "They", "Other", pronoun)) %>%
  mutate(pronoun = factor(pronoun, levels = c("He", "She", "Other"))) %>%
  group_by(exp, set) %>%
  summarise(across(everything(), first)) %>%
  ggplot() +
  geom_bar(aes(age_group, fill = pronoun), position = "dodge") +
  facet_wrap(~fct_relevel(education, "High School or below", "Diploma and Bachelor Degree", "Honours Degree", "Masters Degree")) +
  xlab("Age group") +
  ylab("Count") +
  theme_light(base_size = 10)
```

Figure \ref{fig:experience} shows no particular difference between the number of participants who have previous experience and the number of those who haven't. Age distributions are also similar for these two groups.

```{r experience, fig.cap = "\\label{fig:experience}Status of pervious experience of subjects recuritted in this study group by age and pronoun."}
vi_survey %>%
  mutate(pronoun = ifelse(pronoun == "They", "Other", pronoun)) %>%
  mutate(pronoun = factor(pronoun, levels = c("He", "She", "Other"))) %>%
  group_by(exp, set) %>%
  summarise(across(everything(), first)) %>%
  ggplot() +
  geom_bar(aes(age_group, fill = pronoun), position = "dodge") +
  facet_wrap(~previous_experience) +
  xlab("Age group") +
  ylab("Count") +
  theme_light()
```



## Effect size derivation

Effect size can be defined as the difference of a parameter for a particular model or distribution,  or a statistic derived from a sample. Importantly, it needs to reflect the treatment we try to measure. Centred on a conventional statistical test, we usually can deduce the effect size from the test statistic by substituting the null parameter value. When considering the diagnostics of residual departures, there exist many possibilities of test statistics for a variety of model assumptions. Meanwhile, diagnostic plots such as the residual plot have no general agreement on measuring how strong a model violation pattern is. To build a bridge between various residual-based tests, and the visual test, we focus on the shared information embedded in the testing procedures, which is the distribution of residuals. When comes to comparison of distribution, Kullback-Leibler divergence [@@kullback1951information] is a classical way to represent the information loss or entropy increase caused by the approximation to the true distribution, which in our case, the inefficiency due to the use of false model assumptions.

Following the terminology introduced by @kullback1951information, $P$ represents the measured probability distribution, and $Q$ represents the assumed probability distribution. The Kullback-Leibler divergence is defined as $\int_{-\infty}^{\infty}log(p(x)/q(x))p(x)dx$, where $p(.)$ and $q(.)$ denote probability densities of $P$ and $Q$. 

Let $\boldsymbol{X}_a = (\boldsymbol{1}, \boldsymbol{X})$ denotes the $p$ regressors with $n$ observations, $\boldsymbol{R}_a = \boldsymbol{I} -\boldsymbol{X}(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'$ denotes the residual operator, and let $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0},\sigma^2\boldsymbol{I})$ denotes the error. Using the Frisch–Waugh–Lovell theorem, residuals $\boldsymbol{e} = \boldsymbol{R}_a\boldsymbol{\varepsilon}$. Because $rank(\boldsymbol{R}_a) = n - p < n$, $e$ follows a degenerate multivariate normal distribution and does not have a density. Since the Kullback-Leibler divergence requires a proper density function, we need to simplify the covariance matrix of $\boldsymbol{e}$ by setting all the off-diagonal elements to 0. Then, the residuals will assumed to follow $N(\boldsymbol{0}, diag(\boldsymbol{R}_a\sigma^2))$ under the null hypothesis that the model is correctly specified. If the model is however misspecified due to omitted variables $\boldsymbol{Z}$, or a non-constant variance $\boldsymbol{V}$, the distribution of residuals can be derived as $N(\boldsymbol{R}_a\boldsymbol{Z}\boldsymbol{\beta}_z, diag(\boldsymbol{R}_a\sigma^2))$ and $N(\boldsymbol{0}, diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a'))$ respectively.

By assuming both $P$ and $Q$ are multivariate normal density functions, the Kullback-Leibler divergence can be rewritten as 
$$KL = \frac{1}{2}\left(log\frac{|\Sigma_p|}{|\Sigma_q|} - n + tr(\Sigma_p^{-1}\Sigma_q) + (\mu_p - \mu_q)'\Sigma_p^{-1}(\mu_p - \mu_q)\right).$$

Then, we can combine the two residual departures into one formula

\begin{equation}
\label{eq:effect-size}
KL = \frac{1}{2}\left(log\frac{|diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')|}{|diag(\boldsymbol{R}_a\sigma^2)|} - n + tr(diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')^{-1}diag(\boldsymbol{R}_a\sigma^2)) + \boldsymbol{\mu}_z^{T}(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')^{-1}\boldsymbol{\mu}_z\right).
\end{equation}

When there are omitted variables but constant error variance, the formula can be reduced to

$$KL = \frac{1}{2}\left(\boldsymbol{\mu}_z^{T}(diag(\boldsymbol{R}_a\sigma^2))^{-1}\boldsymbol{\mu}_z\right).$$

And when the model equation is correctly specified but the error variance is non-constant, the formula can be reduced to

$$KL = \frac{1}{2}\left(log\frac{|diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')|}{|diag(\boldsymbol{R}_a\sigma^2)|} - n + tr(diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')^{-1}diag(\boldsymbol{R}_a\sigma^2))\right).$$
Since we assume $\sigma = 1$ for the heteroskedasticity model, the final form of the formula is 

$$KL = \frac{1}{2}\left(log\frac{|diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')|}{|diag(\boldsymbol{R}_a)|} - n + tr(diag(\boldsymbol{R}_a\boldsymbol{V}\boldsymbol{R}_a')^{-1}diag(\boldsymbol{R}_a))\right).$$


## Batch effect

```{r poly-boxplot-lineup, fig.cap = '\\label{fig:poly-boxplot-lineup}A lineup of "letter-value" boxplots of weighted propotion of detect for lineups over different data collection periods for non-linearity model. Can you find the most different boxplot? The data plot is positioned in panel $2^3 - 1$.'}
library(lvplot)
sample_from_null <- function(dat, true_position) {
  dat %>%
    mutate(k = true_position) %>%
    bind_rows(map_df((1:20)[-true_position], function(k) {
    dat %>%
      mutate(exp = sample(exp),
             k = k)
  }))
}

vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(exp = exp - 2) %>%
  group_by(exp, unique_lineup_id) %>%
  mutate(prop_detect = mean(weighted_detect)) %>%
  group_by(exp, unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  ungroup() %>%
  filter(type == "non-linearity") %>%
  select(type, exp, prop_detect) %>%
  mutate(exp = as.character(exp)) %>%
  sample_from_null(7) %>%
  ggplot() +
  geom_lv(aes(x = exp, y = prop_detect, fill = after_stat(LV)), varwidth = TRUE) +
  # scale_fill_lv() +
  facet_wrap(~k, ncol = 5) +
  theme_light() +
  scale_x_discrete(labels = c("I", "III")) +
  ylab("Weighted propotion of detect") +
  xlab("Data collection period")
```

```{r heter-boxplot-lineup, fig.cap = '\\label{fig:heter-boxplot-lineup}A lineup of "letter-value" boxplots of weighted propotion of detect for lineups over different data collection periods for heteroskedasticity model. Can you find the most different boxplot? The data plot is positioned in panel $2^4 - 2$.'}
vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(exp = exp - 2) %>%
  group_by(exp, unique_lineup_id) %>%
  mutate(prop_detect = mean(weighted_detect)) %>%
  group_by(exp, unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  ungroup() %>%
  filter(type != "non-linearity") %>%
  select(type, exp, prop_detect) %>%
  mutate(exp = as.character(exp)) %>%
  sample_from_null(14) %>%
  ggplot() +
  geom_lv(aes(x = exp, y = prop_detect, fill = after_stat(LV)), varwidth = TRUE) +
  # scale_fill_lv() +
  facet_wrap(~k, ncol = 5) +
  theme_light() +
  scale_x_discrete(labels = c("II", "III")) +
  ylab("Weighted propotion of detect") +
  xlab("Data collection period")
```

```{r batch, fig.cap = '\\label{fig:batch}A "letter-value" boxplot of weighted propotion of detect for lineups over different data collection periods.'}
vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(exp = exp - 2) %>%
  group_by(exp, unique_lineup_id) %>%
  mutate(prop_detect = mean(weighted_detect)) %>%
  group_by(exp, unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  ungroup() %>%
ggplot() +
  geom_lv(aes(x = factor(exp), y = prop_detect, fill = after_stat(LV)), varwidth = TRUE) +
  # scale_fill_lv() +
  facet_grid(~type) +
  theme_light()
```


```{r}
t_test_poly <- vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(exp = exp - 2) %>%
  group_by(exp, unique_lineup_id) %>%
  mutate(prop_detect = mean(weighted_detect)) %>%
  group_by(exp, unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "non-linearity") %>%
  t.test(prop_detect ~ exp, data = .)
```


```{r}
t_test_heter <- vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(exp = exp - 2) %>%
  group_by(exp, unique_lineup_id) %>%
  mutate(prop_detect = mean(weighted_detect)) %>%
  group_by(exp, unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type != "non-linearity") %>%
  t.test(prop_detect ~ exp, data = .)
```

We have the same type of model collected over different data collection periods, that may lead to unexpected batch effect.
Figure \ref{fig:poly-boxplot-lineup} and \ref{fig:heter-boxplot-lineup} provide two lineups to examine whether there is an actual difference across data collection periods for non-linearity model and heteroskedasticity model respectively. To emphasize the tail behaviour and display fewer outliers, we use the "letter-value" boxplot (ref here) which is an extension of the number of "letter value" statistics (ref here) to check the weighed proportion of detect over different data collection period. The weighted proportion of detect is calculated by taking the average of $c_i$ of a lineup over a data collection period. Within our research team, we can not identify the data plot from the null plots for these two lineups, result in $p$-values much greater than $5$%. 

Figure \ref{fig:batch} shows the data plot, where the distribution for the heteroskedasticity patterns are almost identical in period II and III, while the distribution for the non-linearity patterns have a shift in the median. The $p$-values of the t-test for testing the mean difference across data periods are `r round(t_test_poly$p.value, 2)` and `r round(t_test_heter$p.value, 2)` for non-linearity and heteroskedasticity patterns respectively. Both are much greater than 5%.
Therefore, both the visual test and the conventional test, there is no clear evidence of batch effect.

## Sensitivity analysis for $\alpha$

The parameter $\alpha$ used for the $p$-value calculation needs to be estimated from responses to null lineups. However, The way we generate Rorschach lineup is not strictly the same as what suggested in @vanderplas2021statistical and @buja_statistical_2009. Therefore, we conduct a sensitivity analysis in this section to examine the impact of the variation of the estimator $\alpha$ on our primary findings.

The analysis is conducted by setting up two relatively extreme scenarios, where the $\alpha$ is under or overestimated by 50%. Using the inflated and deflated $\hat{\alpha}$, we recalculate the $p$-value for every lineup and show the results in Figure \ref{fig:sensitive}. It can be observed that there are some changes to $p$-values, especially when the $\hat{\alpha}$ is deflated. However, Figure \ref{fig:sensitive-barplot} shows that adjusting $\hat{\alpha}$ will not result in a huge difference in rejection decisions. There are only a small number of cases (12 out of 558) where the rejection decision changes. It is very unlikely the downstream findings will be affected because of the estimate of $\alpha$. 


```{r}
vi_survey_upper <- calc_p_value_multi(vi_survey %>%
                                        mutate(alpha_upper = alpha * 1.5,
                                               alpha_lower = alpha * 0.5),
                                      lineup_id = unique_lineup_id,
                                      detect = detect,
                                      n_sel = num_selection,
                                      alpha = alpha_upper) %>%
  right_join(select(vi_survey, -p_value))
  

vi_survey_lower <- calc_p_value_multi(vi_survey %>%
                                        mutate(alpha_upper = alpha * 1.5,
                                               alpha_lower = alpha * 0.5),
                                      lineup_id = unique_lineup_id,
                                      detect = detect,
                                      n_sel = num_selection,
                                      alpha = alpha_lower) %>%
  right_join(select(vi_survey, -p_value))
```


```{r cache = TRUE, eval = FALSE}
# Define the minimum and maximum effect size
min_poly_es <- vi_survey_upper %>% 
  filter(!null_lineup, 
         !attention_check,
         type == "polynomial", 
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  min()
max_poly_es <- vi_survey_upper %>% 
  filter(!null_lineup, 
         !attention_check,
         type == "polynomial", 
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  max()

# The glm model for RESET test
reset_poly_glmmod <- poly_conv_sim %>%
  filter(x_dist == "uniform") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(reject = as.numeric(RESET4_p_value <= 0.05)) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  select(x_dist, effect_size, log_effect_size, offset0, reject) %>%
  glm(reject ~ effect_size - 1, family = binomial(), data = ., offset = offset0)

# The prediction for all conventional tests on poly model
pred_conv_poly <- poly_conv_sim %>%
  filter(x_dist == "uniform") %>%
  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value:SW_p_value) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value <= 0.05) %>%
  select(effect_size, name, reject) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  nest(dat = c(effect_size, offset0, reject)) %>%
  mutate(mod = map(dat, 
                   ~glm(reject ~ effect_size - 1, 
                        family = binomial(), 
                        data = .x,
                        offset = offset0))) %>%
  mutate(power = map(mod, function(mod) {
    result <- data.frame(effect_size = seq(min_poly_es, max_poly_es, 0.1),
                         offset0 = log(0.05/0.95))
    result$power <- predict(mod, type = "response", newdata = result)
    result
  })) %>%
  select(-dat, -mod) %>%
  unnest(power) %>%
  mutate(log_effect_size = log(effect_size))

# Data needed by the visual test
visual_poly_dat <- vi_survey_upper %>%
  filter(!null_lineup, 
         !attention_check, 
         type == "polynomial", 
         x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  select(effect_size, offset0, reject)

# The glm model for visual test
visual_poly_glmmod <- glm(reject ~ effect_size - 1, 
                          family = binomial(), 
                          data = visual_poly_dat, 
                          offset = offset0)

# The prediction of the visual test
pred_visual_poly <- data.frame(effect_size = seq(min_poly_es, max_poly_es, 0.1),
                               offset0 = log(0.05/0.95)) %>%
  mutate(power = predict(visual_poly_glmmod, 
                         type = "response",
                         newdata = data.frame(effect_size = effect_size,
                                              offset0 = offset0))) %>%
  mutate(log_effect_size = log(effect_size))

# The prediction of the boot visual test
pred_visual_poly_boot <- map_df(1:500, function(boot_id) {
  slice_sample(visual_poly_dat, n = nrow(visual_poly_dat), replace = TRUE) %>%
    nest(dat = c(effect_size, offset0, reject)) %>%
    mutate(mod = map(dat, 
                     ~glm(reject ~ effect_size - 1, 
                          family = binomial(), 
                          data = .x,
                          offset = offset0))) %>%
    mutate(power = map(mod, function(mod) {
      result <- data.frame(effect_size = seq(min_poly_es, max_poly_es, 0.1),
                           offset0 = log(0.05/0.95))
      result$power <- predict(mod, type = "response", newdata = result)
      result
    })) %>%
    select(-dat, -mod) %>%
    unnest(power) %>%
    mutate(log_effect_size = log(effect_size)) %>%
    mutate(boot_id = boot_id)
  })
```

```{r eval = FALSE}
ggplot() +
  
  geom_point(data = visual_poly_dat,
             aes(log(effect_size), reject),
             alpha = 0.15) +
  
  # Draw conventional test power curve
  geom_line(data = pred_conv_poly,
            aes(log_effect_size, power, col = name),
            size = 1) +
  
  # Draw boot visual test curves
  geom_line(data = pred_visual_poly_boot,
            aes(log_effect_size, power, col = "Visual test", group = boot_id),
            size = 1,
            alpha = 0.01) +
  
  # Draw visual test power curve
  geom_line(data = pred_visual_poly,
            aes(log_effect_size, power, col = "Visual test"), 
            size = 1) +
  
  # Theme
  theme_light(base_size = 5) +
  theme(panel.grid.major.x =  element_line(colour = "grey70", linewidth = 0.5)) +
  scale_color_manual(values = rev(rcartocolor::carto_pal(4, "Vivid"))) +
  scale_x_continuous(breaks = seq(0, 5, 0.5), minor_breaks = NULL) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```


```{r sensitive, fig.cap = "\\label{fig:sensitive}Change of $p$-values with $\\hat{\\alpha}$ inflated by 50% and deflated by 50%."}
vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value = first(p_value)) %>%
  left_join(
vi_survey_upper %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value_upper = first(p_value))) %>%
  left_join(
vi_survey_lower %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value_lower = first(p_value))) %>%
  pivot_longer(p_value_upper:p_value_lower) %>%
  mutate(name = ifelse(name == "p_value_upper", "+50%", "-50%")) %>%
  ggplot() +
  geom_point(aes(p_value, value), alpha = 0.2) +
  geom_abline() +
  scale_x_sqrt() +
  scale_y_sqrt() +
  facet_wrap(~name) +
  ylab("Adjusted p-value") +
  xlab("p-value") +
  coord_fixed() +
  theme_light()
```

```{r sensitive-barplot, fig.cap = "\\label{fig:sensitive-barplot}Change of rejection of the null hypothesis with $\\hat{\\alpha}$ inflated by 50% and deflated by 50%."}
vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value = first(p_value)) %>%
  left_join(
vi_survey_upper %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value_upper = first(p_value))) %>%
  left_join(
vi_survey_lower %>%
  group_by(unique_lineup_id) %>%
  summarise(p_value_lower = first(p_value))) %>%
  pivot_longer(p_value_upper:p_value_lower) %>%
  mutate(name = ifelse(name == "p_value_upper", "+50%", "-50%")) %>%
  ggplot() +
  geom_bar(aes(p_value < 0.05, fill = value < 0.05), position = "dodge") +
  theme_light() +
  xlab("p-value < 0.05") +
  labs(fill = "Adjusted p-value < 0.05")
```


```{r cache = TRUE, eval = FALSE}
# Define the minimum and maximum effect size
min_heter_es <- vi_survey_upper %>% 
  filter(!null_lineup, 
         !attention_check,
         type == "heteroskedasticity", 
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  min()
max_heter_es <- vi_survey_upper %>% 
  filter(!null_lineup, 
         !attention_check,
         type == "heteroskedasticity", 
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  max()

# The prediction for all conventional tests on heter model
pred_conv_heter <- heter_conv_sim %>%
  filter(x_dist == "uniform") %>%
  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value:SW_p_value) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value <= 0.05) %>%
  select(effect_size, name, reject) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  nest(dat = c(effect_size, offset0, reject)) %>%
  mutate(mod = map(dat, 
                   ~glm(reject ~ effect_size - 1, 
                        family = binomial(), 
                        data = .x,
                        offset = offset0))) %>%
  mutate(power = map(mod, function(mod) {
    result <- data.frame(effect_size = seq(min_heter_es, max_heter_es, 1),
                         offset0 = log(0.05/0.95))
    result$power <- predict(mod, type = "response", newdata = result)
    result
  })) %>%
  select(-dat, -mod) %>%
  unnest(power) %>%
  mutate(log_effect_size = log(effect_size))

# Data needed by the visual test
visual_heter_dat <- vi_survey_upper %>%
  filter(!null_lineup, 
         !attention_check, 
         type == "heteroskedasticity", 
         x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  select(effect_size, offset0, reject)

# The glm model for visual test
visual_heter_glmmod <- glm(reject ~ effect_size - 1, 
                           family = binomial(), 
                           data = visual_heter_dat, 
                           offset = offset0)

# The prediction of the visual test
pred_visual_heter <- data.frame(effect_size = seq(min_heter_es, max_heter_es, 0.1),
                                offset0 = log(0.05/0.95)) %>%
  mutate(power = predict(visual_heter_glmmod, 
                         type = "response",
                         newdata = data.frame(effect_size = effect_size,
                                              offset0 = offset0))) %>%
  mutate(log_effect_size = log(effect_size))

# The prediction of the boot visual test
pred_visual_heter_boot <- map_df(1:500, function(boot_id) {
  slice_sample(visual_heter_dat, n = nrow(visual_heter_dat), replace = TRUE) %>%
    nest(dat = c(effect_size, offset0, reject)) %>%
    mutate(mod = map(dat, 
                     ~glm(reject ~ effect_size - 1, 
                          family = binomial(), 
                          data = .x,
                          offset = offset0))) %>%
    mutate(power = map(mod, function(mod) {
      result <- data.frame(effect_size = seq(min_heter_es, max_heter_es, 0.1),
                           offset0 = log(0.05/0.95))
      result$power <- predict(mod, type = "response", newdata = result)
      result
    })) %>%
    select(-dat, -mod) %>%
    unnest(power) %>%
    mutate(log_effect_size = log(effect_size)) %>%
    mutate(boot_id = boot_id)
  })
```


```{r eval = FALSE}
# Analyse uniform heteroskedasticity data
ggplot() +
  
  geom_point(data = visual_heter_dat,
             aes(log(effect_size), reject),
             alpha = 0.15) +
  
  # Draw conventional test power curve
  geom_line(data = pred_conv_heter,
            aes(log_effect_size, power, col = name),
            size = 1) +
  
  # Draw boot visual test curves
  geom_line(data = pred_visual_heter_boot,
            aes(log_effect_size, power, col = "Visual test", group = boot_id),
            size = 1,
            alpha = 0.01) +
  
  # Draw visual test power curve
  geom_line(data = pred_visual_heter,
            aes(log_effect_size, power, col = "Visual test"), 
            size = 1) +
  
  # Theme
  theme_light(base_size = 5) +
  theme(panel.grid.major.x =  element_line(colour = "grey70", linewidth = 0.5)) +
  scale_color_manual(values = rev(rcartocolor::carto_pal(4, "Vivid"))) +
  # scale_size_manual(values = seq(1, 9)) +
  scale_x_continuous(breaks = seq(0, 5, 0.5), minor_breaks = NULL) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```  


## Curious effect of predictor distribution on conventional test power

## Effect of number of evaluations on the power of a visual test

## Power of a RESET test under different auxiliary regression formulas

1. put #eval in appendix
3. compute the power of RESET test with different orders (considering dof) (focus on simple pattern possibly?)

\newpage

## References
