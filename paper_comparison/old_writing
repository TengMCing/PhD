<!-- check the policy -->

<!-- problem: residual plot diagnostics -->

<!-- conventional test: too sensitive -->

<!-- background:  -->

<!-- week 1 -->

<!-- 1. residual plot for model diagnostics -->

<!-- a. residual is widely used -->

<!-- b. what are the types of residual plots -->

<!-- c. comparison -->

<!-- week 2 -->

<!-- 2. conventional test: F, BP -->

<!-- 3. visual test: lineup, theory -->

<!-- week 4 -->

<!-- desc of experiment: -->

<!-- 1. simulation setup -->

<!-- 2. experimental design -->

<!-- 3. result -->

<!-- week 3 -->

<!-- comparison of conventional tests: -->

<!-- 1. power (visual test vs. conventional test) -->

<!-- (visual test most different one (everything test, any departure)) -->

<!-- plot figure in a paper, desc, exp -->

<!-- 2. investigate the difference (gap), give examples -->

<!-- 3. conventional is too sensitive -->

<!-- 4. make conventional less sensitive (vary alpha) -->

<!-- last week -->

<!-- conclusion: -->

<!-- 1. too sensitive, visual test is needed/preferable -->

<!-- 2. visual test is infeasible in large scale (expensive) -->

<!-- 3. future work (role of computer vision) -->



<!-- Raw residuals and studentized residuals are the two most frequently used residuals in standard residual plots. The debt on which type of residuals should be used is always present. While raw residuals are the most common computer regression software package output, by applying a scaling factor, the ability to reveal nonconstant error variance in standard residual plots will often be enhanced by studentized residuals in small sample size [@gunst1980regression]. As a two-dimensional representation of a model in a $p$-dimensional space, standard residual plots project data points onto the variable of the horizontal axis, which is a vector in $p$-dimensional space. Observations with the same projection will be treated as equivalent as they have the same abscissa. Consequently, standard residual plots are often useful in revealing model inadequacies in the direction of the variable of the horizontal axis but could be inadequate for detecting patterns in other directions, especially in those perpendicular to the variable of the horizontal axis. Hence, in practice, multiple standard residual plots with different horizontal axes will be examined [@cook1982residuals]. Overlapping data points is another general issue in scatter plots not limited to standard residual plots, which often makes plots difficult to interpret because visual patterns are concealed. Thus, for a relatively large sample size, @cleveland1975graphical suggests the use of robust moving statistics as reference lines to give aid to the eye in seeing patterns, which nowadays, are usually replaced with a spline or local polynomial regression line.  -->

<!-- Other types of data plots that are often used in regression diagnostics include partial residual plots and probability plots. Partial residual plots are useful supplements to standard residual plots as they provide additional information on the extent of the non-linearity. Probability plots can be used to compare the sampling distribution of the residuals to the normal distribution for assessing the normality assumptions. -->

<!-- ## Hypothesis testing -->

<!-- Other than checking diagnostic plots, analysts may perform formal hypothesis testing for detecting model defects. Depending on the alternative hypothesis that is focused on, a variety of tests can be applied. For example, the presence of heteroskedasticity can usually be tested by applying the White test [@white_heteroskedasticity-consistent_1980] or the Breusch-Pagan test [@breusch_simple_1979], which are both derived from the Lagrange multiplier test [@silvey1959lagrangian] principle that relies on the asymptotic properties of the null distribution. For testing non-linearity, one may apply the F-test to examine the significance of specific polynomial and non-linear forms of the regressors, or the significance of proxy variables as in the Ramsey Regression Equation Specification Error Test (RESET) [@ramsey_tests_1969].  -->

<!-- As discussed in @cook1982residuals, most residual-based tests for a particular type of departure from model assumptions are sensitive to other types of departures. It is likely the null hypothesis is correctly rejected but for the wrong reason, which is known as the "Type III error". Additionally, outliers will often incorrectly trigger the rejection of the null hypothesis despite the residuals are well-behaved [@cook_applied_1999]. This can be largely avoided in diagnostic plots as experienced analysts can evaluate the acceptability of assumptions flexibly, even in the presence of outliers. @montgomery1982introduction suggested that based on their experience, statistical tests are not widely used in regression diagnostics. The same or even larger amount of information can be provided by diagnostic plots than the corresponding tests in most empirical studies. Not to mention, it is almost impossible to have an exactly correctly specified model in reality. There is a well-known aphorism in statistics stated by George Box - "All models are wrong, but some are useful". This indicates proper hypothesis tests will always reject the null hypothesis as long as the sample size is large enough. The outcome "Not reject" can be interpreted as either "effect size is small" or "sample size is small". and the outcome "reject" doesn't inform us whether and how much the model defects are of actual consequence to the inference and prediction. But still, the effectiveness of statistical tests shall not be disrespected. Statistical tests have a chance to provide analysts with unique information. In situations where no suitable diagnostic plots can be found for a particular violation of the assumptions, or excessive diagnostic plots are needed to be checked, one will have no choice but to fall back on statistical tests if there exists any. A good regression diagnostic practice should be a balanced combination of both methods. -->


<!-- Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualization" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots [@gelman_exploratory_2004]. Later, in the Bayesian context, data plots was systematically considered as model diagnostics by taking advantage of the data simulated from the assumed statistical models [@gelman_bayesian_2003; @gelman_exploratory_2004]. -->

<!-- It was surprising that the essential components of visual inference had actually been established in @buja_inference_1999, but it was not until 10 years later that @buja_statistical_2009 formalized it as an inferential framework to extend confirmatory statistics to visual discoveries. This framework redefines the test statistics, tests, null distribution, significance levels and $p$-value for visual discovery modelled on the confirmatory statistical testing. Figure \ref{fig:parallelism} outlines the parallelism between conventional tests and visual discovery. -->

<!-- In visual inference, a collection of test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})~(i \in I)$ is defined, where $\boldsymbol{\mathrm{y}}$ is the data and $I$ is a set of all possible visual features. @buja_statistical_2009 described each of the test statistics $T^{(i)}(\boldsymbol{\mathrm{y}})$ as a measurement of the degree of presence of a visual feature. Alternatively, @majumder_validation_2013 avoids the use of visual features and defined the visual statistics $T(.)$ as a mapping from a dataset to a data plot. Both definitions of visual test statistics are valid, but in the rest of the paper the first definition will be used as it covers some details needed by the following discussion. A visual discovery is defined as a rejection of a null hypothesis, and the same null hypothesis can be rejected by many different visual discoveries [@buja_statistical_2009]. For regression diagnostics, the null hypothesis would be the assumed model, while the visual discoveries would be any findings that are inconsistent with the null hypothesis. The same regression model can be rejected by many reasons with residual plot, including non-linearity and heteroskedasticity as shown in Figure \ref{fig:residual-plot-cubic-heter}. -->

<!-- ### Sampling from the null distribution {#se:sampling-from-null} -->

<!-- The null distribution of plots refers to the infinite collection of plots of null datasets sampled from $H_0$. It is defined as the analogue of the null distribution of test statistics in conventional test [@buja_statistical_2009]. In practice, a finite number of plots of null datasets could be generated, called null plots. In the context of regression diagnostics, sampling data from $H_0$ is equivalent to sampling data from the assumed model. As @buja_statistical_2009 suggested, $H_0$ is usually composited by a collection of distributions controlled by nuisance parameters. Since regression models can have various forms, there is no general solution to this problem, but it sometimes can be reduced to so called "reference distribution" by applying one of the three methods: (i) sampling from a conditional distribution given a minimal sufficient statistic under $H_0$, (ii) parametric bootstrap sampling with nuisance parameters estimated under $H_0$, and (iii) Bayesian posterior predictive sampling. -->

<!-- The conditional distribution given a minimal sufficient statistic is the best justified reference distribution among the three [@buja_statistical_2009]. Suppose there exists a minimal sufficient statistic $\boldsymbol{S}(\boldsymbol{y})$ under the null hypothesis, any null datasets $\boldsymbol{y^{*}}$ should fulfil the condition $\boldsymbol{S}(\boldsymbol{y}) = \boldsymbol{s}$. Using the classical normal linear regression model as example, the minimal sufficient statistic is $\boldsymbol{S}(\boldsymbol{y}) = (\hat{\boldsymbol{\beta}}, \boldsymbol{e}'\boldsymbol{e})$, where $\hat{\boldsymbol{\beta}}$ are the coefficient estimators and $\boldsymbol{e}'\boldsymbol{e}$ is the residual sum of square. Alternatively, the minimal sufficient statistic can be constructed as $\boldsymbol{S}(\boldsymbol{y}) = (\hat{\boldsymbol{y}}, ||\boldsymbol{e}||)$, where $\hat{\boldsymbol{y}}$ are the fitted values and $||\boldsymbol{e}||$ is the length of residuals, which is more intuitive as suggested by @buja_statistical_2009. Since the fitted values are held fixed, the variation can only occur in the residual space. And because the length of residual is also held fixed, residuals obtained from a null dataset has to be a random rotation of $\boldsymbol{e}$ in the residual space. With this property, null residuals can be simulated by regressing $N$ i.i.d standard normal random draws on the regressors, then rescaling it by the ratio of residual sum of square in two regressions. -->

<!-- ### Lineup protocol {#se:lineup} -->

<!-- With the simulation mechanism of null plots being provided, another aspect of hypothesis testing that needs to be addressed is the control of false positive rate or Type I error. Any visual statistic $T^{(i)}(\boldsymbol{\mathrm{y}})$ needs to pair with a critical value $c^{(i)}$ to form a hypothesis test. When a visual feature $i$ is discovered by the observer from a plot, the corresponding visual statistic $T^{(i)}(\boldsymbol{\mathrm{y}})$ may not be known as there is no general agreement on the measurement of the degree of presence of a visual feature. It is only the event that $T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}$ is confirmed. Similarly, if any visual discovery is found by the observer, we say, there exists $i \in I:~T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}$ [@buja_statistical_2009]. -->

<!-- Using the above definition, the family-wise Type I error can be controlled if one can provide the collection of critical values $c^{(i)}~(i \in I)$ such that $P(\mathrm{there~exists~} i \in I: T^{(i)}(\boldsymbol{\mathrm{y}}) > c^{(i)}|\boldsymbol{\mathrm{y}}) \leq \alpha$, where $\alpha$ is the significance level. However, since the quantity of $T^{(i)}(\boldsymbol{\mathrm{y}})$ may not be known, such collection of critical values can not be provided. -->

<!-- @buja_statistical_2009 proposed the lineup protocol as a visual test to calibrate the Type I error issue without the specification of $c^{(i)}~(i \in I)$. It is inspired by the "police lineup" or "identity parade" which is the act of asking the eyewitness to identify criminal suspect from a group of irrelevant people. The protocol consists of $m$ randomly placed plots, where one plot is the actual data plot, and the remaining $m - 1$ plots have the identical graphical production as the data plot except the data has been replaced with data consistent with the null hypothesis. Then, an observer who have not seen the actual data plot will be asked to point out the most different plot from the lineup. -->

<!-- Under the null hypothesis, it is expected that the actual data plot would have no distinguishable difference with the null plots, and the probability of the observer correctly picks the actual data plot is $1/m$. If we reject the null hypothesis as the observer correctly picks the actual data plot, then the Type I error of this test is $1/m$. -->

<!-- This provides us with an mechanism to control the Type I error, because $m$ - the number of plots in a lineup can be chosen. A larger value of $m$ will result in a smaller Type I error, but the limit to the value of $m$ depends on the number of plots a human is willing to view [@buja_statistical_2009]. Typically, $m$ will be set to $20$ which is equivalent to set $\alpha = 0.05$, a general choice of significance level for conventional testing among statisticians. -->

<!-- Further, a visual test can involve $K$ independent observers. Let $D_i = \{0,1\}$ be a binomial random variable denoting whether subject $i$ correctly detecting the actual data plot, and $X = \sum_{i=1}^{K}X_i$ be the number of observers correctly picking the actual data plot. Then, by imposing a relatively strong assumption on the visual test that all $K$ evaluations are fully independent, under the null hypothesis, $X \sim \mathrm{Binom}_{K,1/m}$. Therefore, the $p$-value of a lineup of size $m$ evaluated by $K$ observer is given as \begin{equation} \label{eq:pvaluesingle} -->
<!-- P(X \geq x) = \sum_{i=x}^{K}{{K}\choose{i}}\left(\frac{1}{m}\right)^i\left(\frac{m-1}{m}\right)^{k-i}, -->
<!-- \end{equation} -->

<!-- where $x$ is the realization of number of observers correctly picking the actual data plot [@majumder_validation_2013]. -->

<!-- The multiple individuals approach avoids the limit of $m$, while provides visual tests with $p$-value much smaller than $0.05$. In fact, the lower bound of $p$-value decreases exponentially as $K$ increases. With just $4$ individuals and $20$ data plots in a lineup, the $p$-value could be as small as $0.0001$. Additionally, by involving multiple observers, variation of individual ability to read plots can be addressed to some degree as different opinions about visual discoveries can be collected. -->

<!-- As pointed out by @vanderplas2021statistical, though Equation (\ref{eq:pvaluesingle}) is trivial, but it doesn't take into account the possible dependencies in the visual test due to repeated evaluations of the same lineup. And it is inapplicable to visual test where subjects are asked to select one or more "most different" plots from the lineup. They summarized three common different scenarios in visual inference: (1) $K$ different lineups are shown to $K$ subjects, (2) $K$ lineups with different null plots but the same actual data plot are shown to $K$ subjects, and (3) the same lineup is shown to $K$ subjects. Out of these three scenarios, Scenario 3 is the most common in previous studies as it puts the least constraints on the experimental design. For Scenario 3, @vanderplas2021statistical modelled the probability of a plot $i$ being selected from a lineup as $\theta_i$, where $\theta_i \sim Dirichlet(\alpha)$ for $i=1,...,m$ and $\alpha > 0$. And defined $c_i$ to be the number of times plot $i$ being selected in $K$ evaluations. In case subject $j$ makes multiple selections, they decided to add $1/s_j$ to $c_i$ instead of one, where $s_j$ is the number of plots subject $j$ selected for $j=1,...K$. This ensured $\sum_{i}c_i=K$. -->

<!-- The full model was a Dirichlet-multinomial mixture distribution -->

<!-- ```{=tex} -->
<!-- \begin{align} \label{eq:dirichlet-multinomial}\begin{split} -->
<!-- \boldsymbol{\theta}&|\alpha \sim Dirichlet(\alpha)\\ -->
<!-- (c_i,...,c_m)&|\boldsymbol{\theta} \sim Multinomial(K, \boldsymbol{\theta}). -->
<!-- \end{split}\end{align} -->
<!-- ``` -->
<!-- Since the p-value calculation only needs to concern the number of times the actual data plot being selected denoted by $C_i$, they showed the model can be simplified to a beta-binomial mixture distribution -->

<!-- ```{=tex} -->
<!-- \begin{align} \label{eq:beta-binomial}\begin{split} -->
<!-- \theta_i&|\alpha \sim Beta(\alpha, (m-1)\alpha)\\ -->
<!-- C_i&|\theta_i \sim Binomial(K, \theta_i). -->
<!-- \end{split}\end{align} -->
<!-- ``` -->
<!-- Thus, the visual p-value followed by the beta-binomial model is given as -->

<!-- ```{=tex} -->
<!-- \begin{equation} \label{eq:pvalue-beta-binomial} -->
<!-- P(C \geq c_i) = \sum_{x=c_i}^{K}{{K}\choose{x}}\frac{1}{B(\alpha, (m-1)\alpha)}B(x + \alpha, K - x + (m - 1)\alpha), -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- where $B(.)$ is the beta function defined as -->

<!-- ```{=tex} -->
<!-- \begin{equation} \label{eq:betafunction} -->
<!-- B(a, b) = \int_{0}^{1}t^{\alpha - 1}(1-t)^{b-1}dt,\quad \text{where}\quad a,b>0.  -->
<!-- \end{equation} -->
<!-- ``` -->

<!-- Note that the use of Equation (\ref{eq:pvalue-beta-binomial}) requires the estimation of $\hat{\alpha}$, which largely depends on the null model, the type of the plot and other aesthetic features. They suggested to estimate $\hat{\alpha}$ visually based on the selections of null plots of the experimental data, or to estimate $\hat{\alpha}$ numerically based on several additional Rorschach lineups, which is a type of lineup containing only null plots. However, when the number of null models are large, it could be expensive to manually estimate each $\alpha$ or include additional Rorschach lineups in the experiment. -->

<!-- Instead, in the experiments that will be described in section \ref{experimental-design}, we adopt a simpler model implicitly used by the `pmulti()` function of the `vinference` `R` package. We assume the attractiveness of the plot $i$ modelled as $w_i \sim Uniform(0,1)$ for $i=1,..,m$. Let $\theta_i = w_i/\sum_{i=1}^{m}w_i$ be the probability of plot $i$ being selected by a subject. Then, given the number of selections $s_j$, for $j=1,...,K$, the distribution of $C_i$ can be approximated by simulating the random selection process with computer. The simulated visual test p-value is formulated as -->

<!-- ```{=tex} -->
<!-- \begin{equation} \label{eq:p-value-multi} -->
<!-- \text{p-value} = \frac{\#draws~that~the~actual~data~plota~being~selected~more~than~c_i~times}{\#simulation}. -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- ### Effectiveness of visual inference in regression diagnostics -->

<!-- Compared to the conventional test, whose power only depends on the parameter of interest $\theta$, several studies [see @hofmann_graphical_2012; @majumder_validation_2013; @majumder2014human; @roy_chowdhury_using_2015; @loy2016variations] have shown the power of the visual test is subject-specific. Thus, to be able to account for individual's ability, an individual is required to evaluate multiple lineups [@majumder_validation_2013].   -->

<!-- Suppose individuals have the same ability and a lineup has been evaluated by multiple individuals, under the alternative hypothesis, the estimated power for a lineup can be expressed as $\hat{p} = x/K$, the estimated probability of identifying the actual data plot from the lineup. If the individual skill needs to be taken into account, and $L$ lineups have been evaluated by $K$ individuals, @majumder_validation_2013 suggests that mixed effects logistic regression model can be fit as: -->

<!-- $$g(p_{li}) = W_{li}\delta + Z_{li}\tau_{li},$$ -->

<!-- where $g(.)$ is the logit link function $g(p) = log(p)  - log(1-p)$; $0 \leq p \leq 1$. $W_{li}$, $1 \leq i \leq K$, $1 \leq l \leq L$, is the covariate matrix including lineup-specific elements and demographic information of individuals, and $\delta$ is a vector of parameters. $Z$ is the random effects matrix, and $\tau$ is a vector of variables follow $N(\boldsymbol{0},\sigma_{\tau}\boldsymbol{I}_{KL\times KL})$.  -->

<!-- Then, the estimated power for lineup $l$ and individual $i$ can be calculated as $\hat{p}_{li} = g^{-1}(W_{li}\hat{\delta} + Z_{li}\hat{\tau}_{li})$ [@majumder_validation_2013]. -->

<!-- ## Effectiveness of visual test in regression diagnostics -->

<!-- The effectiveness of visual inference has already been validated by @majumder_validation_2013 under relatively simple classical normal linear regression model settings with only one or two regressors. Their results suggest visual test is capable of testing the significance of a single regressor with a similar power as a t-test, though they expressed that in general it is unnecessary to use visual inference if there exists a conventional test and they didn't expect the visual test to perform equally well as the conventional test. In their third experiment, where there does not exist a proper conventional test, visual test outperforms the conventional test for a large margin. This is encouraging as it promotes the use of visual inference in border field of data science where there are no existing statistical testing procedures. In fact, lineup protocol has been integrated into some model diagnostic tools such as @loy2013diagnostic. -->

<!-- <!-- refactoring from here --> -->

<!-- With our knowledge, what haven't been examined so far is the effectiveness of visual test relative to the equivalent conventional test in regression diagnostics. Particularly, its ability to detect non-linearity and heteroskedasticity compared to F-test and BP-test. -->
