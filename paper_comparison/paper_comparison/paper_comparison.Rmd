---
title: |
  Why shouldn't you use numerical tests to diagnose the linear regression models?
type: ARTICLE TEMPLATE
author:
  - name: Weihao Li
    affil: a
    email: weihao.li@monash.edu
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a
    email: emi.tanaka@monash.edu
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
bibliography: paper.bib
abstract: |
  Abstract to fill.
keywords: |
  visual inference; model diagnostics;
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
output: rticles::tf_article
---

problem: residual plot diagnostics
conventional test: too sensitive

background: 

<!-- week 1 -->
1. residual plot for model diagnostics

a. residual is widely used
b. what are the types of residual plots
c. comparison

<!-- week 2 -->
2. conventional test: F, BP
3. visual test: lineup, theory

<!-- week 4 -->
desc of experiment:
1. simulation setup
2. experimental design
3. result

<!-- week 3 -->
comparison of conventional tests:
1. power (visual test vs. conventional test)
(visual test most different one (everything test, any departure))
plot figure in a paper, desc, exp
2. investigate the difference (gap), give examples
3. conventional is too sensitive
4. make conventional less sensitive (vary alpha)


<!-- last week -->
conclusion:
1. too sensitive, visual test is needed/preferable
2. visual test is infeasible in large scale (expensive)
3. future work (role of computer vision)



# Introduction

Regression diagnostics conventionally involve evaluating the fitness of the proposed model, detecting the presence of influential observations and outliers, checking the validity of model assumptions and many more. Common diagnostic techniques including summary statistics, hypothesis testing, and data plots are essential tools for a systematic and detailed examination of the regression model [@mansfield1987diagnostic].

## Diagnostic plots

Regression analysis is a field of study with at least a hundred years of history. Many of those regression diagnostic methods and procedures are mature and well-established in books first published in the twentieth century, such as @draper_applied_2014, @montgomery_introduction_2012, @belsley_regression_1980, @cook_applied_1999 and @cook1982residuals. Regardless of the level of difficulty of the book, one will find the importance and usefulness of diagnostic plots being emphasized again and again. Checking diagnostic plots is also the recommended starting point for validating model assumptions like normality, homoscedasticity and linearity [@anscombe_examination_1963]. 

Graphical summaries in which residuals are plotted against fitted values or other functions of the predictor variables that are approximately orthogonal to residuals are refereed to as standard residual plots. They are commonly used to identify patterns which are indicative of nonconstant error variance or non-linearity [@cook1982residuals]. Raw residuals and studentized residuals are the two most frequently used residuals in standard residual plots. The debt on which type of residuals should be used always present. While raw residuals are the most common output of computer regression software package, by applying a scaling factor, the ability of revealing nonconstant error variance in standard residual plots will often be enhanced by studentized residuals in small sample size [@gunst2018regression]. 

As a two-dimensional representation of a model in a $p$-dimensional space, standard residual plots project  data points onto the variable of the horizontal axis, which is a vector in $p$-dimensional space. Observations with the same projection will be treated as equivalent as they have the same position of the abscissa. Therefore, standard residual plots are often useful in revealing model inadequacies in the direction of the variable of the horizontal axis, but could be inadequate for detecting patterns in other directions, especially in those perpendicular to the variable of the horizontal axis. Hence, in practice, multiple standard residual plots with different horizontal axes will be examined.


<!-- large sample size-->
Overlapping data points is a general issue in scatter plots not limited to standard residual plots, which often makes plots difficult to interpret because visual patterns are concealed. Thus, for relatively large sample size, @cleveland1975graphical suggests the use of robust moving statistics as reference lines to give aids to eye in seeing patterns, which nowadays, are usually replaced with a spline or local polynomial regression line.

Other types of data plots that are often used in regression diagnostics include partial residual plots and probability plots. Partial residual plots are useful supplements to standard residual plots as they provide additional information on the extent of the non-linearity. Probability plots can be used to compare the sampling distribution of the residuals to the normal distribution for assessing the normality assumptions.

## Hypothesis testing

In addition to diagnostic plots, researcher may also perform formal tests for detecting model defects. Depends on the alternative, variety of tests can be applied. For example, for testing heteroskedasticity, 
one may use the White test [@white_heteroskedasticity-consistent_1980] or the Breusch-Pagan test [@breusch_simple_1979]. And for testing non-linearity, there are RESET test [@ramsey_tests_1969] and F-test.  

As discussed in @cook1982residuals, most residual based tests for a particular type of departures from model assumptions are sensitive to other types of departures. Especially, outliers will often  incorrectly trigger the rejection of null hypothesis despite the residuals are well-behaved [@cook_applied_1999]. This can be largely avoided in diagnostic plots as experienced analysts can evaluate the acceptability of assumptions flexibly, even in the presence of outliers. Furthermore, @montgomery_introduction_2012 stated that based on their experience, statistical tests are not widely used in regression diagnostics. Most importantly, the same or even larger amount of information can be provided by diagnostic plots than the corresponding tests in most empirical studies. But still, the effectiveness of statistical tests shall not be disrespected. Statistical tests have chance to provide analysts with unique information. There are also situations where no suitable diagnostic plots can be found for a particular violation of the assumptions, or excessive diagnostic plots need to be checked. One will have no choice but to rely on statistical tests if there is any. A good regression diagnostic practice should be a combination of both methods.

## Visual inference

Diagnostic plots are 

<!-- However, unlike confirmatory data analysis built upon rigorous statistical procedures, e.g., hypothesis testing, visual diagnostics relies on graphical perception - human’s ability to interpret and decode the information embedded in the graph [@cleveland_graphical_1984], which is to some extent subjective. Further, visual discovery suffers from its unsecured and unconfirmed nature where the degree of the presence of the visual features typically can not be measured quantitatively and objectively, which may lead to over or under-interpretations of the data. One such example is finding an over-interpretation of the separation between gene groups in a two-dimensional projection from a linear discriminant analysis when in fact there are no differences in the expression levels between the gene groups and separation is not an uncommon occurrence [@roy_chowdhury_using_2015]. -->

<!-- Recently, a new branch of statical inference  -->

<!-- Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualization" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots [@gelman_exploratory_2004]. Later, in the Bayesian context, data plots was systematically considered as model diagnostics by taking advantage of the data simulated from the assumed statistical models [@gelman_bayesian_2003; @gelman_exploratory_2004].  -->


# Experimental design

# Data processing


# Results

## Overview of the Data

We collected 400 lineup evaluations made by 20 participants in experiment I and 880 lineup evaluations made by 44 participants in experiment II. In total, 442 unique lineups were evaluated by 64 subjects. In experiment I, one of the participants skipped all 20 lineups. Hence, the submission was rejected and removed from the dataset. In experiment II, there was a participant failed one of the two attention checks, but there was no further evidence of low-effort throughout the experiment. Therefore, the submission was kept.



## Power comparision

1. power (visual test vs. conventional test)
(visual test most different one (everything test, any departure))
plot figure in a paper, desc, exp
2. investigate the difference (gap), give examples
3. conventional is too sensitive
4. make conventional less sensitive (vary alpha)


To model the power of visual test, 10 logistic regression were fit for different number of evaluations ranged from one to five and two different types of simulation setting. All 10 models used natural logarithm of the effect size as the only fixed effect, and whether the test successfully rejects the null hypothesis as the response variable. Given the way we define the effect size, it was expected that with larger effect size, both conventional test and visual test will have higher probability in rejecting the null hypothesis when it is not true. The modelling result summarized in \ref{tab:powerglmcubic} and \ref{tab:powerglmheter} aligned with the expectation as the coefficients of natural logarithm of the effect size are positive and significant across all 10 models.


Figure \ref{fig:power-com} illustrates the fitted models, while providing the local constant estimate of the power of F-test and Breusch–Pagan test for comparison. Data for the conventional test is simulated under the model setting described in section ... and 5000000 samples are drawn for both cubic and heteroskedasticity model. From Figure \ref{fig:power-com}, it can be observed that the fitted power of visual test increased as the number of evaluations increased for both cubic and heteroskedasticity model.

For heteroskedasticity model, this phenomenon was more obvious as the power of visual tests with evaluations greater than two were always greater than those with evaluations smaller than two. 

For cubic model, the separation between curves was small. The estimated power of visual tests with three to five evaluations were almost identical to each other in regards of effect size. In addition, all five curves peaked at one as effect size increased, suggesting that identification of non-linearity as a visual task can be completed reliably by human as long as the departure from null hypothesis is large enough.

As shown in Figure \ref{fig:power-com}, both F-test and Breusch–Pagan test generally possessed greater power than visual test. A visual tests is a collection of test against any alternatives that would create visual discoverable features, while a conventional test is usually targeting at a pre-specified alternative. Considering the data generating process of the model defect was known and controlled in this research, where all other alternatives have been eliminated except the one we concerned, the result was suggested that conventional tests were more sensitive to violations of linearity and homoscedasticity assumption than visual tests. 

It was also found that there was a noticeable gap between curves of the conventional test and the visual test at around $log(\text{effect size}) = 0$ for the cubic model and $log(\text{effect size}) = 2.5$ for the heteroskedasticity model, where the differences in power were greater than 0.6. We further analysed the lineups with correspoding effect sizes. Figure \ref{fig:cubic-hard} and \ref{fig:heter-hard} showed that human was indeed hard to identify the patterns at this level of difficulty. The visual difference between the true data plot and null plots were almost unnoticeable. 

<!-- sample 1, change values of a and b, plot the data  -->

## Effect of parameters on power of the visual test

The previous section focuses on the change of effect size relative to the power of the visual test. However, effect size is only a one dimensional summarisation of parameters used in data simulation. Individual factor embedded in the simulation process should also be analysed. 

In cubic model, two major factors that influencing the strength of the signal are $a$ and $b$. Figure \ref{fig:power-com-cubic-a} and \ref{fig:power-com-cubic-a} illustrates 30 different logistic regressions fit for different number of evaluations and different number of observations $n$. The regressor used in these models was $|a|/\sigma$ since the noise level $\sigma$ needed to be taken into account. From the figures, we can observe ...



```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r}
library(tidyverse)
library(visage)

set.seed(10086)
```

```{r get-conv-result}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

if (!file.exists(here::here("data/third_study/conv_result.rds"))) {
  conv_result <- map(1:100000,
                   function(x) {
                     shape <- sample(1:4, 1)
                     e_sigma <- sample(c(0.5, 1, 2, 4), 1)
                     x_dist <- sample(c("uniform", "normal", "lognormal", "even_discrete"), 1)
                     x <- switch(x_dist,
                                 uniform = rand_uniform(-1, 1),
                                 normal = {raw_x <- rand_normal(sigma = 0.3); closed_form(~stand_dist(raw_x))},
                                 lognormal = {raw_x <- rand_lognormal(sigma = 0.6); closed_form(~stand_dist(raw_x/3 - 1))},
                                 even_discrete = rand_uniform_d(k = 5, even = TRUE))
                     mod <- poly_model(shape, x = x, sigma = e_sigma)
                     n <- sample(c(50, 100, 300), 1)
                     tmp_dat <- mod$gen(n)

                     tibble(shape = shape,
                            e_sigma = e_sigma,
                            x_dist = x_dist,
                            n = n,
                            effect_size = mod$effect_size(tmp_dat),
                            reject = mod$test(tmp_dat)$p_value < 0.05)
                   }) %>%
  reduce(bind_rows) %>%
  mutate(reject = as.numeric(reject))
  
  saveRDS(conv_result, here::here("data/third_study/conv_result.rds"))
} else {
  conv_result <- readRDS(here::here("data/third_study/conv_result.rds"))
}


```

```{r get-third-result}
if (!file.exists(here::here("data/third_study/polynomials_lineup.rds"))) {
  polynomials_lineup <- get_polynomials_lineup()
  saveRDS(polynomials_lineup, here::here("data/third_study/polynomials_lineup.rds"))
} else {
  polynomials_lineup <- readRDS(here::here("data/third_study/polynomials_lineup.rds"))
}
```


```{r power-vs-log-effect-size}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  xlab("Natural logarithm of effect size") +
  ylab("Power")
```

```{r power-vs-log-effect-size-given-x-dist}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  facet_wrap(~x_dist) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power comparison conditional on the distribution of X")
```

```{r power-of-visual-test-given-x-dist}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = x_dist), method = "glm", method.args = list(family = binomial), se = FALSE) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power of visual test conditional on the distribution of X")
```



```{r power-vs-log-effect-size-given-shape}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  facet_wrap(~shape) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power comparison conditional on the shape of the Hermite polynomials")
```

```{r power-of-visual-test-given-shape}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01)), shape = factor(shape)) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = shape), method = "glm", method.args = list(family = binomial), se = FALSE) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power of visual test conditional on the shape of the Hermite polynomials")
```


```{r power-vs-log-effect-size-given-number-of-observations}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  facet_wrap(~n) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power comparison conditional on the number of observations")
```


```{r power-of-visual-test-given-number-of-observations}
polynomials %>%
  group_by(lineup_id) %>%
  summarise(across(c(type:p_value), first)) %>%
  filter(lineup_id < 577) %>%
  mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01)), n = factor(n)) %>%
  ggplot() +
  geom_smooth(aes(log(effect_size), reject, col = n), method = "glm", method.args = list(family = binomial), se = FALSE) +
  xlab("Natural logarithm of effect size") +
  ylab("Power") +
  ggtitle("Power of visual test conditional on the number of observations")
```
