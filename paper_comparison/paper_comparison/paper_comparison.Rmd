---
title: |
  Why aren't significance tests commonly used for linear regression diagnostics?
type: ARTICLE TEMPLATE
author:
  - name: Weihao Li
    affil: a
    email: weihao.li@monash.edu
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a
    email: emi.tanaka@monash.edu
  - name: Susan VanderPlas
    affil: b
    email: susan.vanderplas@unl.edu
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
  - num: b
    address: |
      Department of Statistics, University of Nebraska, Lincoln, Nebraska, USA
bibliography: paper.bib
abstract: |
  Abstract to fill.
keywords: |
  data visualization; visual inference; hypothesis testing; residual plots;
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \newcommand*{\Perm}[2]{{}^{#1}\!P_{#2}}%
output: rticles::tf_article
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%")
```

```{r}
library(tidyverse)
# remotes::install_github("TengMCing/visage")
library(visage)

set.seed(10086)
```

```{r get-lineup-data}
if (!file.exists(here::here("data/vi_lineup.rds"))) {
  vi_lineup <- get_vi_lineup()
  saveRDS(vi_lineup, here::here("data/vi_lineup.rds"))
} else {
  vi_lineup <- readRDS(here::here("data/vi_lineup.rds"))
}
```

```{r poly-conventional-simulation}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

if (!file.exists(here::here("data/poly_conventional_simulation.rds"))) {
  poly_conv_sim <- list()
  for (i in 1:100)
  {
    poly_conv_sim[[i]] <- map(1:2000,
                            function(x) {
                     shape <- sample(1:4, 1)
                     e_sigma <- sample(c(0.5, 1, 2, 4), 1)
                     x_dist <- sample(c("uniform", "normal", "lognormal", "even_discrete"), 1)
                     x <- switch(x_dist,
                                 uniform = rand_uniform(-1, 1),
                                 normal = {raw_x <- rand_normal(sigma = 0.3); closed_form(~stand_dist(raw_x))},
                                 lognormal = {raw_x <- rand_lognormal(sigma = 0.6); closed_form(~stand_dist(raw_x/3 - 1))},
                                 even_discrete = rand_uniform_d(k = 5, even = TRUE))
                     mod <- poly_model(shape, x = x, sigma = e_sigma)
                     n <- sample(c(50, 100, 300), 1)
                     tmp_dat <- mod$gen(n)

                     tibble(shape = shape,
                            e_sigma = e_sigma,
                            x_dist = x_dist,
                            n = n,
                            effect_size = mod$effect_size(tmp_dat),
                            F_p_value = mod$test(tmp_dat)$p_value,
                            BP_p_value = HETER_MODEL$test(tmp_dat)$p_value,
                            SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
                            boot_id = i)
                   }) %>%
  reduce(bind_rows)
  }
  
  poly_conv_sim <- poly_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(poly_conv_sim, here::here("data/poly_conventional_simulation.rds"))
} else {
  poly_conv_sim <- readRDS(here::here("data/poly_conventional_simulation.rds"))
}
```



```{r heter-conventional-simulation}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

if (!file.exists(here::here("data/heter_conventional_simulation.rds"))) {
  heter_conv_sim <- list()
  for (i in 1:100)
  {
    heter_conv_sim[[i]] <- map(1:2000,
                            function(x) {
                     a <- sample(c(-1, 0, 1), 1)
                     b <- sample(c(0.25, 1, 4, 16, 64), 1)
                     x_dist <- sample(c("uniform", "normal", "lognormal", "even_discrete"), 1)
                     x <- switch(x_dist,
                                 uniform = rand_uniform(-1, 1),
                                 normal = {raw_x <- rand_normal(sigma = 0.3); closed_form(~stand_dist(raw_x))},
                                 lognormal = {raw_x <- rand_lognormal(sigma = 0.6); closed_form(~stand_dist(raw_x/3 - 1))},
                                 even_discrete = rand_uniform_d(-1, 1, k = 5, even = TRUE))
                     mod <- heter_model(a = a, b = b, x = x)
                     n <- sample(c(50, 100, 300), 1)
                     tmp_dat <- mod$gen(n)

                     tibble(a = a,
                            b = b,
                            x_dist = x_dist,
                            n = n,
                            effect_size = mod$effect_size(tmp_dat, type = "kl"),
                            F_p_value = POLY_MODEL$test(mutate(tmp_dat, z = poly_model()$gen(n, computed = select(tmp_dat, x))$z))$p_value,
                            BP_p_value = mod$test(tmp_dat)$p_value,
                            SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
                            boot_id = i)
                   }) %>%
  reduce(bind_rows)
  }
  
  heter_conv_sim <- heter_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(heter_conv_sim, here::here("data/heter_conventional_simulation.rds"))
} else {
  heter_conv_sim <- readRDS(here::here("data/heter_conventional_simulation.rds"))
}
```



# Introduction

> *"Since all models are wrong the scientist must be alert to what is importantly wrong."* [@box1976science]

Diagnosing a model is the key to determining whether there is anything importantly wrong. For linear regression analysis, it is typical to interrogate the residuals. Residuals summarise what is not captured by the model, and thus provide the capacity to identify what might be wrong. There are many ways that residuals could be assessed.

Residuals might be plotted, as a histogram or quantile-quantile plot to examine the distribution. Using the classical normal linear regression model as an example, if the distribution is symmetric and unimodal, it is well-behaved. But if the distribution is skewed, bimodal, multimodal, or contains outliers, there is cause for concern. The distribution could also be inspected by conducting a goodness of fit test, such as the Shapiro-Wilk Normality test [@shapiro1965analysis].

Plotting the residuals against predicted values and each of the explanatory variables on a scatter plot is a recommend way to scrutinize their relationships. If there is any visually discoverable patterns, the model is potentially misspecified. However, it is a very difficult task for a human judge, though to make a decision that there's nothing there. It is especially common, particularly among new data analysts to report patterns when an experienced data analyst might quickly conclude that there are none. Generally, one looks for departures from nothingness like non-linear dependency or heteroskedasticity. It is also possible to conduct hypothesis tests for non-linear dependence [@ramsey_tests_1969], and use a Breusch-Pagan test [@breusch_simple_1979] for heteroskedasticity.

There is an abundance of literature describing appropriate diagnostic methods for linear regression: @draper1998applied, @montgomery1982introduction, @belsley_regression_1980, @cook_applied_1999 and @cook1982residuals. The diligent reader of these sage writings will also notice sentences that express sentiments like *based on their experience, statistical tests are not widely used in regression diagnostics. The same or even larger amount of information can be provided by diagnostic plots than the corresponding tests in most empirical studies.* There is a common guidance by experts that plots are the best for diagnosing model fits.

This is curious, and investigating why this might be common advice is the subject of this paper. The paper is structured as follows. The next background section describes the the types of departures that one expects to detect, and describes a formal process for reading residual plots, called visual inference, that can avoid the concerns about subjectiveness of human readers. Section \ref{experimental-design} describes the experimental setup to enable a comparison between decision made by formal hypothesis testing, and how humans would read diagnostic plots. The results are reported in Section \ref{results}. We finish with a discussion on future work, in particular how the responsibility for residual plot reading might be passed on to computer vision.

```{=html}
<!--Regression diagnostics is an essential step in regression analysis which is a field of study with at least a hundred years of history. The diagnostic procedure conventionally involves evaluating the fitness of the proposed model, detecting the presence of influential observations and outliers, checking the validity of model assumptions and many more. In terms of diagnostic techniques, data plots, hypothesis testing, and summary statistics are vital tools for a systematic and detailed examination of the regression model [@mansfield1987diagnostic].

Many of those regression diagnostic methods and procedures are mature and well-established in books first published in the twentieth century, such as @draper1998applied, @montgomery1982introduction, @belsley_regression_1980, @cook_applied_1999 and @cook1982residuals. Regardless of the level of difficulty, one will find the importance and usefulness of diagnostic plots being emphasized in those books repeatedly. Checking diagnostic plots is also the recommended starting point for validating model assumptions such as normality, homoscedasticity and linearity [@anscombe_examination_1963]. 
-->
```
# Background

## Departures from good residual plots

(This section discusses the visual patterns data analysts expect to see and their implications.)

```{r residual-plot-common-departures, fig.cap = "Example fitted vs residual plots: (A) classically good looking residuals, (B) non-linear pattern indicates that the model has not captured a non-linear association, (C) heteroskedasticity indicating that variance around the fitted model is not uniform, and (D) non-normality where the residual distribution is not symmetric around 0. The latter pattern might best be assessed using a univariate plot of the residuals, but patterns B and C need to be assessed using a residual vs fitted plot."}

set.seed(10086)

mod <- poly_model(include_z = FALSE, sigma = 0.25)
dat <- mod$gen(300)
patchwork::wrap_plots(
  mod$plot(dat, theme = theme_light()) +
    ggtitle("A. Good residuals") + 
    xlab("fitted values") + ylab("residuals"),
  mod$plot(mod$set_prm("include_z", TRUE)$gen(300, 
    computed = select(dat, x, e)), 
    theme = theme_light()) + 
    ggtitle("B. Non-linearity") +
    xlab("fitted values") + ylab("residuals"),
  mod$plot(heter_model(b = 64)$gen(300, 
    computed = select(dat, x, e)), 
    theme = theme_light()) + 
    ggtitle("C. Heteroskedasticity") +
    xlab("fitted values") + ylab("residuals"),
  mod$plot(heter_model(b = 0, 
    e = rand_lognormal(sigma = 0.5))$gen(300, 
    computed = select(dat, x)), 
    theme = theme_light()) + 
    ggtitle("D. Non-normality") +
    xlab("fitted values") + ylab("residuals")
)
```

Graphical summaries in which residuals are plotted against fitted values or other functions of the predictor variables that are approximately orthogonal to residuals are referred to as standard residual plots in @cook1982residuals. As shown in Figure \ref{fig:residual-plot-common-departures}, the top-left panel is a good residual plot with residuals evenly distributed at both sides of the horizontal zero line showing no noticeable patterns. There are various types of departures from a good residual plot. 

Non-linearity, heterskedasticity and non-normality are perhaps the three mostly commonly checked departures. Non-linearity is a type of model misspecification caused by failing to include higher order terms of the regressors in the regression equation. Any non-linear functional form of residuals on fitted values presented in the residual plot could be considered as an indicative of non-linearity. An example residual plot containing visual pattern of non-linearity is given at the top-right of Figure [ref here]. One can clearly observe the "S-shape" from the residual plot as the cubic term is not captured by the misspecified model. Heterskedasticity refers to the presence of nonconst error variance in a regression model. It is mostly due to the strict but false assumptions on the variance-covariance matrix of the error term. The usual pattern of heterskedasticity on a residual plot is the inconsistent spread of the residuals at different x values. Visually, it sometimes results in the so-called "butterfly" shape as shown in the bottom-left panel of Figure \ref{fig:residual-plot-common-departures}, or the "left-triangle" and "right-triganle" shape where the smallest variance occurs at the edges of the x-axis. Compared to non-linearity and heterskedasticity, non-normality is usually harder to be detected from a residual plot since scatter plot is not excel in revealing marginal distribution. A favourable graphical summarise for this task is the quantile-quantile plot. However, for a consistent comparison in the later part of this paper, residual plot will be the focus. Besides, it is important to note that not all regression models assume normality for the error term, but a certain amount do including the classical normal linear regression model. In the case that the normality assumption is violated, it is expected to observe data points that do not centralize around the horizontal zero line and unevenly distribute at both sides of the zero line. For example, given a skewed error distribution, fewer data points and more outliers are on one side of the zero line as shown at the bottom-right of Figure \ref{fig:residual-plot-common-departures}.

## Conventionally testing for departures

(This section discusses the tests that will be used in the analysis and shows the results for the residual plots displayed in the previous section.)

Other than checking diagnostic plots, analysts may perform formal hypothesis testing for detecting model defects. Depending on the alternative hypothesis that is focused on, a variety of tests can be applied. For example, the presence of heteroskedasticity can usually be tested by applying the White test [@white_heteroskedasticity-consistent_1980] or the Breusch-Pagan test [@breusch_simple_1979], which are both derived from the Lagrange multiplier test [@silvey1959lagrangian] principle that relies on the asymptotic properties of the null distribution. For testing non-linearity, one may apply the F-test as a model structural test to examine the significance of specific polynomial and non-linear forms of the regressors, or the significance of proxy variables as in the Ramsey Regression Equation Specification Error Test (RESET) [@ramsey_tests_1969]. And for testing normality, the Shapiro--Wilk test [ref here] is perhaps the most widely used test included by many of the statistical softwares. Another choice will be the Jarque--Bera test [ref here] which directly checks if the sample skewness and kurtosis match a normal distribution.

Example residual plots given in Figure \ref{fig:residual-plot-common-departures} are examined by the corresponding model structural test, Breusch-Pagan test and Shapiro--Wilk test as shown in Table \ref{tab:example-residual-plot-table}. In the example, both the Breusch-Pagan test and the Shapiro--Wilk test rejects the null hypothesis for departures that they do not intend to examine. As discussed in @cook1982residuals, most residual-based tests for a particular type of departure from model assumptions are sensitive to other types of departures. It is likely the null hypothesis is correctly rejected but for the wrong reason, which is known as the "Type III error". Additionally, outliers will often incorrectly trigger the rejection of the null hypothesis despite the residuals are well-behaved [@cook_applied_1999]. This can be largely avoided in diagnostic plots as experienced analysts can evaluate the acceptability of assumptions flexibly, even in the presence of outliers.

<!-- test formula -> appendix -->

<!-- residual plot -> plot -->

<!-- plot a included -->


```{r}

set.seed(10086)

mod <- poly_model(include_z = FALSE, sigma = 0.25)
dat <- mod$gen(300)

dat1 <- mod$set_prm("include_z", TRUE)$gen(300, computed = select(dat, x, e))
dat2 <- heter_model(b = 64)$gen(300, computed = select(dat, x, e))
dat3 <- heter_model(b = 0, e = rand_lognormal(sigma = 0.5))$
               gen(300, computed = select(dat, x))



data.frame(plot = c("A", "B", "C", "D"), 
           model = c("None", "Non-linearity", "Heteroskedasticity", "Non-normality"),
           f = c(POLY_MODEL$test(dat)$p_value, 
                 POLY_MODEL$test(dat1)$p_value,
                 POLY_MODEL$test(mutate(dat2, z = poly_model()$gen(300, computed = select(dat2, x))$z))$p_value,
                 POLY_MODEL$test(mutate(dat3, z = poly_model()$gen(300, computed = select(dat3, x))$z))$p_value),
           b = c(HETER_MODEL$test(dat)$p_value, 
                 HETER_MODEL$test(dat1)$p_value,
                 HETER_MODEL$test(dat2)$p_value,
                 HETER_MODEL$test(dat3)$p_value),
           s = c(shapiro.test(dat$.resid)$p.value, 
                 shapiro.test(dat1$.resid)$p.value,
                 shapiro.test(dat2$.resid)$p.value,
                 shapiro.test(dat3$.resid)$p.value)) %>%
  mutate(f = format(round(f, digits = 3), nsmall = 3), b = format(round(b, digits = 3), nsmall = 3), s = format(round(s, digits = 3), nsmall = 3)) -> table_dat


  table_dat$f <- kableExtra::cell_spec(table_dat$f, italic = as.numeric(table_dat$f) < 0.05)
  table_dat$b <- kableExtra::cell_spec(table_dat$b, italic = as.numeric(table_dat$b) < 0.05)
  table_dat$s <- kableExtra::cell_spec(table_dat$s, italic = as.numeric(table_dat$s) < 0.05)

  table_dat %>%
  knitr::kable(col.names = c("Plot", "Departures", "Model structural", "Breusch-Pagan", "Shapiro–Wilk"),
               align = "llrrr",
               escape = FALSE,
               format = "latex",
               booktabs = TRUE, 
               caption  = 'Statistical significance testing for departures from good residuals for plots in Figure \\ref{fig:residual-plot-common-departures}. Shown are the $p$-values calculated for the conventional Model structural, the Breusch-Pagan and the Shapiro–Wilk tests. The good residual plot (A) is judged a good residual plot, as expected, by all tests. The non-linearity (B) is detected by all tests, as might be expected given the extreme structure.', 
               label = "example-residual-plot-table" ) 

```

## Visual testing for departures

(This section introduces the lineup protocol, and briefly discusses the method for sampling null data, calculating the p-value and estimating power.)

### Lineup protocol

<!-- improve the first sentence -->

Unlike hypothesis testing built upon rigorous statistical procedures, reading diagnostic plots relies on graphical perception - human's ability to interpret and decode the information embedded in the graph [@cleveland_graphical_1984], which is to some extent subjective and indecisive. Further, visual discovery suffers from its unsecured and unconfirmed nature where the degree of the presence of the visual features typically can not be measured quantitatively and objectively, which may lead to over or under-interpretations of the data. One such example is finding an over-interpretation of the separation between gene groups in a two-dimensional projection from a linear discriminant analysis when in fact there are no differences in the expression levels between the gene groups and separation is not an uncommon occurrence [@roy_chowdhury_using_2015].

Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualization" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots [@gelman_exploratory_2004]. Later, @buja_statistical_2009 proposed the lineup protocol as a visual test inspired by the "police lineup" or "identity parade" which is the act of asking the eyewitness to identify criminal suspect from a group of irrelevant people. The protocol consists of $m$ randomly placed plots, where one plot is the actual data plot, and the remaining $m - 1$ plots have the identical graphical production as the data plot except the data has been replaced with data consistent with the null hypothesis. Then, an observer who have not seen the actual data plot will be asked to point out the most different plot from the lineup. Under the null hypothesis, it is expected that the actual data plot would have no distinguishable difference with the null plots, and the probability of the observer correctly picks the actual data plot is $1/m$. If we reject the null hypothesis as the observer correctly picks the actual data plot, then the Type I error of this test is $1/m$.

<!-- use true exp lineup as example lineup -->

```{r first-example-lineup, fig.cap="Visual testing is conducted using a lineup, as in the example here. The residual plot computed from the observed data (plot 11, exhibiting non-linearity) is embedded among 19 null plots, where the residuals were simulated from a standard error model. Computing the $p$-value requires that the lineup be examined by a number of human judges, each asked to select the most different plot. A small $p$-value would result from a substantial number selecting plot 11."}
mod <- poly_model(include_z = TRUE, sigma = 1)
POLY_MODEL$plot_lineup(mod$gen_lineup(300, pos = 11), theme = theme_light(), remove_axis = TRUE, remove_grid_line = TRUE)
```

Figure \ref{fig:first-example-lineup} is an example of a lineup protocol. If the actual data plot at position 11 is identifiable, then it is evidence for the rejection of the null hypothesis that the regression model is correctly specified. In fact, the actual residual plot is obtained from a misspecified regression model with non-linearity issue. 

The effectiveness of lineup protocol has already been validated by @majumder_validation_2013 under relatively simple classical normal linear regression model settings with only one or two regressors. Their results suggest visual test is capable of testing the significance of a single regressor with a similar power as a t-test, though they expressed that in general it is unnecessary to use visual inference if there exists a conventional test and they didn't expect the visual test to perform equally well as the conventional test. In their third experiment, where there does not exist a proper conventional test, visual test outperforms the conventional test for a large margin. This is encouraging as it promotes the use of visual inference in border field of data science where there are no existing statistical testing procedures. In fact, lineup protocol has been integrated successfully into diagnostic tools of hierarchical linear models [@loy2013diagnostic].

<!-- adam work -->

### Sampling from the null distribution

Data used in the $m - 1$ null plots need to be simulated. In the context of regression diagnostics, sampling data from $H_0$ is equivalent to sampling data from the assumed model. As @buja_statistical_2009 suggested, $H_0$ is usually composited by a collection of distributions controlled by nuisance parameters. Since regression models can have various forms, there is no general solution to this problem, but it sometimes can be reduced to so called "reference distribution" by applying one of the three methods: (i) sampling from a conditional distribution given a minimal sufficient statistic under $H_0$, (ii) parametric bootstrap sampling with nuisance parameters estimated under $H_0$, and (iii) Bayesian posterior predictive sampling. The conditional distribution given a minimal sufficient statistic is the best justified reference distribution among the three [@buja_statistical_2009]. Essentially, null residuals can be simulated by regressing $N$ i.i.d standard normal random draws on the regressors, then rescaling it by the ratio of residual sum of square in two regressions.

### Calculating $p$-values 

Further, a visual test can involve $K$ independent observers. Let $D_i = \{0,1\}$ be a binomial random variable denoting whether subject $i$ correctly detecting the actual data plot, and $X = \sum_{i=1}^{K}X_i$ be the number of observers correctly picking the actual data plot. Then, by imposing a relatively strong assumption on the visual test that all $K$ evaluations are fully independent, under the null hypothesis, $X \sim \mathrm{Binom}_{K,1/m}$. Therefore, the $p$-value of a lineup of size $m$ evaluated by $K$ observer is given as $P(X \geq x) = 1 - F(x) + f(x)$, where $F(.)$ is the cumulative distribution function, $f(.)$ is the probability mass function and $x$ is the realization of number of observers correctly picking the actual data plot [@majumder_validation_2013].

As pointed out by @vanderplas2021statistical, the binomial model doesn't take into account the possible dependencies in the visual test due to repeated evaluations of the same lineup. And it is inapplicable to visual test where subjects are asked to select one or more "most different" plots from the lineup. They summarized three common scenarios in visual inference: (1) $K$ different lineups are shown to $K$ subjects, (2) $K$ lineups with different null plots but the same actual data plot are shown to $K$ subjects, and (3) the same lineup is shown to $K$ subjects. Out of these three scenarios, Scenario 3 is the most common in previous studies as it puts the least constraints on the experimental design. For Scenario 3, @vanderplas2021statistical modelled the probability of a plot $i$ being selected from a lineup as $\theta_i$, where $\theta_i \sim Dirichlet(\alpha)$ for $i=1,...,m$ and $\alpha > 0$. And defined $c_i$ to be the number of times plot $i$ being selected in $K$ evaluations. In case subject $j$ makes multiple selections, $1/s_j$ will be added to $c_i$ instead of one, where $s_j$ is the number of plots subject $j$ selected for $j=1,...K$. This ensured $\sum_{i}c_i=K$. Since we are only interested in the selections of the actual data plot $i$, the marginal model can be simplified to a beta-binomial model and thus the visual p-value is given as

<!-- TODO:Clarify this is only for integer c_i -->

```{=tex}
\begin{equation} \label{eq:pvalue-beta-binomial}
P(C \geq c_i) = \sum_{x=c_i}^{K}\frac{\Gamma(K + 1)}{\Gamma(x + 1)\Gamma(K - x + 1)}\frac{B(x + \alpha, K - x + (m - 1)\alpha)}{B(\alpha, (m-1)\alpha)},
\end{equation}
```

where $B(.)$ is the beta function defined as

```{=tex}
\begin{equation} \label{eq:betafunction}
B(a, b) = \int_{0}^{1}t^{\alpha - 1}(1-t)^{b-1}dt,\quad \text{where}\quad a,b>0. 
\end{equation}
```

and $\Gamma(.)$ is the gamma function defined as

```{=tex}
\begin{equation} \label{eq:gammafunction}
\Gamma(z) = \int_{0}^{\infty}t^{z - 1}e^{-t}dt,\quad \text{where}\quad z>0. 
\end{equation}
```

<!-- TODO:Introduce the concept of c interesting plot -->

The parameter $\alpha$ used in Equation \ref{eq:pvalue-beta-binomial} is usually unknown and hence needs to be estimated from the survey data. For low values of $\alpha$, only a few plots are attractive to the observers and tend to be selected. For higher values of $\alpha$, the distribution of the probability of each plot being selected is more evenly. @vanderplas2021statistical suggested that $\alpha$ can be estimated using maximum likelihood estimation or visual estimation. But if $\alpha$ is small and only a few null plots in a lineup are attractive, MLE could fail to provide accurate estimates.


### Power calculation

As discussed in @majumder_validation_2013, individual's skill will affect the number of observers who identify the actual data plot from the lineup. Thus, the power of a visual test depends on the subject-specific abilities. Previously, it was addressed by modelling the probability of a subject $i$ correctly picking the actual data plot from a lineup $l$ using a mixed-effect logistic regression with the subject being treated as a random effect [@majumder_validation_2013]. However, having this probability is insufficient to determine the power of a visual test allowed for multiple selections as it doesn't provide information about the number of selections made by the subject for p-value calculation. Instead, we directly estimated the probability of a lineup being rejected using a logistic regression with the natural logarithm of the effect size as the only regressor. The same model was applied to different subsets of the survey data for making comparison of the power.


```{r residual-plot-cubic-heter, fig.cap='Residuals vs. fitted values plot for a classical normal linear regression model. The residuals are produced by fitting a two-predictor multiple linear regression model with data generated from a cubic linear model. From the residual plot, "butterfly shape" can be observed which generally would be interpretd as evidence of heteroskedasticity. Further, from the outline of the shape, nonlinear patterns exist. Both visual discoveries are evidence against the null hypothesis, though heteroskedasticity actually does not exist in the data generating process. \\label{fig:residual-plot-cubic-heter}', eval = FALSE}

cm <- cubic_model(a = 0, b = 100, c = 1)

cm$gen(1000, fit_model = TRUE) %>%
cm$plot() +
  theme_light()
```


# Experimental design

(This section discusses the experimental design including the motivation of the experiment, an overview of the experiment, the simulation setting of the depatures from good residual plots, parameter choices, allocation of the lineups and other technical details.)

## Experiment overview

<!-- With our knowledge, the effectiveness of visual test relative to the equivalent conventional test in regression diagnostics has not been validated. Particularly, its ability to detect non-linearity and heteroskedasticity compared to model structural test and Breusch–Pagan test. Meanwhile, as mentioned in Section \ref{introduction} section, we keen to understand the differences between conventional hypothesis testing and graphical diagnostics in the application of linear regression diagnostics and why conventional tests are not preferred by data analysts in linear regression diagnostics. Attempting to find the answer to these questions, three experiments were conducted. -->

Three experiments were conducted to investigate the difference between conventional hypothesis testing and visual inference in the application of linear regression diagnostics. The experiment I has ideal scenario for conventional testing, where the visual test is not expected to outperform the conventional test. Meanwhile, the experiment II is a scenario where the conventional test is an approximate test, in which the visual test may have a chance to match the performance of the conventional test. The experiment III is designed for collecting human responses to lineup with only good residual plots such that the parameter $\alpha$ in Equation \ref{eq:pvalue-beta-binomial} can be estimated. Overall, we planned to collect 7974 evaluations on 1152 uniqued lineups performed by 443 subjects throughout three experiment. 

## Simulating departures

Two types of departures, namely non-linearity and heteroskedasticity, were considered with the corresponding data generating process being designed for experiment I and II. 

### Non-linearity

Experiment I is designed to study the ability of human subjects to detect the effect of a non-linear term $\boldsymbol{z}$ which is a probabilist's Hermite polynomial [Herimite ref here] of another random vector $\boldsymbol{x}$ in a two variable statistical model formulated as:

\begin{align} \label{eq:nonlinearity-model}
\boldsymbol{y} = 1 + \boldsymbol{x} + \boldsymbol{z} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} = g(\boldsymbol{x}_{raw}, 1), \\
\boldsymbol{z} = g(\boldsymbol{z}_{raw}, 1), \\
\boldsymbol{z}_{raw} = He_j(g(\boldsymbol{z}, 2)),
\end{align}

where $\boldsymbol{y}$, $\boldsymbol{x}$, $\boldsymbol{\varepsilon}$, $\boldsymbol{x}_{raw}$, $\boldsymbol{z}_{raw}$ are vectors of size $n$, $He_{j}(.)$ is the $j$th-order probabilist's Hermite polynomials, $\varepsilon \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)$, and $g(\boldsymbol{x}, k)$ is a scaling function to enforce the support of the random vector to be $\{-k, k\}$ defined as

```{=tex}
\begin{equation} \label{eq:scaling-function}
g(\boldsymbol{x}, k) = (\boldsymbol{x} - min(\boldsymbol{x}))/max(\boldsymbol{x} - min(\boldsymbol{x}))2k - k, \quad \text{for} \quad k > 0. 
\end{equation}
```

The null regression model used to fit the realizations generated by the above model is formulated as:

```{=tex}
\begin{equation} \label{eq:null-model}
\boldsymbol{y} = \beta_0 + \beta_1 \boldsymbol{x} + \boldsymbol{u},
\end{equation}
```
where $\boldsymbol{u} \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)$.

Since $z = O(x^j)$, for $j > 1$, $z$ is a higher order term leaves out by the null regression, which will result in model misspecification. Visual patterns of non-linearity were simulated using four different order of probabilist's Hermite polynomials ($j = 2, 3, 6, 18$) and four different distribution of $X_{raw}$: (1) $U(-1, 1)$, (2) $N(0, 0.3^2)$, (3) $lognormal(0, 0.6^2)/3$ and (4) $u\{1, 5\}$.

<!-- A summary of the parameters used in this experiment is given in Table \ref{tab:parameter-table}. -->

```{r results='asis'}
data.frame(j = c(2, 3, 6, 18),
           x_dist = c("$U(-1, 1)$", "$N(0, 0.3^2)$", "$lognormal(0, 0.6^2)/3$", "$U\\{1, 5\\}$")) %>%
  knitr::kable(col.names = NULL,
               align = "rl",
               escape = FALSE,
               format = "latex",
               booktabs = TRUE, 
               caption  = 'Parameter values for $j$ and $X_{raw}$', 
               label = "model-parameter-table" ) %>%
  kableExtra::add_header_above(c("Order of Hermite polynomial\n(sym1)", "Distribution of sym4\n")) %>%
  as.character() %>%
  gsub("sym1", "$j$", .) %>%
  gsub("sym4", "$X_{raw}$", .) %>%
  cat()
  # gsub("\\\\\\$", "$", .) %>%
  # cat()
```

The values of $j$ was chosen so that distinct shapes of non-linearity were included in the residual plot. A greater value of $j$ will result in a curve with more turning points. As shown in Figure \ref{fig:different-shape-of-herimite}, it includes "U" shape, "S" shape, "M" shape and "Triple-U" shape. It is expected that the "U" shape will be the easiest one to detect because complex shape tends to be concealed by cluster of data points.

```{r different-shape-of-herimite, fig.cap="Example residual plots of the linear regression model used in experiment I. Four different values of $j$ are included in the experiment results in four different shapes of residuals."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300)

VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(He[2]:"U shape")) + 
  theme_light() -> p1

VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[3]:"S shape")) + 
  theme_light() -> p2

VI_MODEL$plot(poly_model(shape = 3, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[6]:"M shape")) + 
  theme_light() -> p3

VI_MODEL$plot(poly_model(shape = 4, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[18]:"Triple-U shape")) + 
  theme_light() -> p4

patchwork::wrap_plots(p1, p2, p3, p4, ncol = 2)
```

Four different distribution were used to generate $X_{raw}$ as shown in Figure \ref{fig:different-dist}. The uniform and the normal distribution are symmetric and commonly assumed in statistical models. The adjusted log-normal distribution provides skewed density, while the discrete uniform distribution provides discreteness in residual plot, which could enrich the pool of visual patterns.

```{r different-dist, fig.cap="Example residual plots of the linear regression model used in experiment I. Four different distribution of $x_{raw}$ are used in the experiment to provide various visual patterns."}
ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300)

VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(U(-1,1))) + 
  theme_light() -> p1

VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_normal(sigma = 0.3); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle(quote(N(0,0.3^2))) + 
  theme_light() -> p2

VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_lognormal(sigma = 0.6); closed_form(~stand_dist(raw_x/3 - 1))}, sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle(quote(lognormal(0,0.6^2)/3)) + 
  theme_light() -> p3

VI_MODEL$plot(poly_model(shape = 1, x = rand_uniform_d(k = 5, even = TRUE), sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle(quote(U~"{1,5}")) + 
  theme_light() -> p4

patchwork::wrap_plots(p1, p2, p3, p4, ncol = 2)
```

Figure \ref{fig:example-poly-lineup} shows one of the lineups used in experiment I. This lineup was produced under $j = 6$ and $X_{raw} \sim N(0.0.3^2)$. The actual data plot location was four. All five subjects correctly identified the actual data plot for this lineup.

```{r example-poly-lineup, fig.cap="Lineup poly-24 in experiment I. Can you spot the most different plot? \\label{fig:example-poly-lineup}"}
VI_MODEL$plot_lineup(vi_lineup[["poly_24"]]$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```



<!-- ```{r results='asis'} -->
<!-- data.frame(n = c(50, 100, 300, ""), -->
<!--            j = c(2, 3, 6, 18), -->
<!--            sigma = c(0.5, 1, 2, 4), -->
<!--            x_dist = c("$U(-1, 1)$", "$N(0, 0.3^2)$", "$lognormal(0, 0.6^2)/3$", "$U\\{1, 5\\}$")) %>% -->
<!--   knitr::kable(col.names = NULL, -->
<!--                escape = FALSE, -->
<!--                format = "latex", -->
<!--                booktabs = TRUE,  -->
<!--                caption  = 'Parameter values for $n$, $j$ $\\sigma$, $X_{raw}$',  -->
<!--                label = "parameter-table" ) %>% -->
<!--   kableExtra::add_header_above(c("Sample size\n(sym3)", "Order of Hermite polynomial\n(sym1)", "Error SD\n(sym2)", "Distribution of sym4\n")) %>% -->
<!--   as.character() %>% -->
<!--   gsub("sym1", "$j$", .) %>% -->
<!--   gsub("sym2", "$\\\\sigma$", .) %>% -->
<!--   gsub("sym3", "$n$", .) %>% -->
<!--   gsub("sym4", "$X_{raw}$", .) %>% -->
<!--   cat() -->
<!--   # gsub("\\\\\\$", "$", .) %>% -->
<!--   # cat() -->
<!-- ``` -->

### Heteroskedasticity

Experiment II is designed to study the ability of human subjects to detect the appearance of a heteroskedasticity pattern under a simple linear regression model setting:

```{=tex}
\begin{align} \label{eq:heter-model}
\boldsymbol{y} = 1 + \boldsymbol{x} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} = g(\boldsymbol{x}_{raw}, 1)\\
\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}, 1 + 2 - |a| b (\boldsymbol{x} - a)^2 \boldsymbol{I}), \\
\end{align}
```

where $\boldsymbol{y}$, $\boldsymbol{x}$, $\boldsymbol{\varepsilon}$ are vectors of size $n$ and $g(.)$ is the scaling function defined in \ref{eq:scaling-function}.

The null regression model used to fit the realizations generated by the above model is formulated exactly the same as Equation \ref{eq:null-model}.

For $b \neq 0$, the variance-covariance matrix of the error term $\boldsymbol{\varepsilon}$ is correlated with the regressor $x$, which will lead to the presence of heteroskedasticity. Visual patterns were simulated using three different shapes ($a$ = -1, 0, 1) and the same four different distribution of $X_{raw}$ used in experiment I.

The values of $a$ was chosen so that different shapes of heteroskedasticity were included in the residual plot. These include left-triangle shape, butterfly shape and right-triangle shape as displayed in Figure \ref{fig:different-shape-of-heter}.

```{r different-shape-of-heter, fig.cap="Example residual plots of the linear regression model used in experiment II. Three different shapes (a = -1, 0, 1) are used in the experiment to create left-triangle shape, butterfly shape and right-triangle shape."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- heter_model(a = -1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300)

VI_MODEL$plot(ori_dat) +
  ggtitle(quote("a = -1: Left-triangle shape")) +
  theme_light() -> p1

VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("a = 0: Butterfly shape")) +
  theme_light() -> p2

VI_MODEL$plot(heter_model(a = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("a = 1: Right-triangle shape")) +
  theme_light() -> p3

patchwork::wrap_plots(p1, p2, p3, ncol = 2)
```

```{r example-heter-lineup, fig.cap="Lineup heter-169 in experiment II. Can you spot the most different plot? \\label{fig:example-heter-lineup}"}
VI_MODEL$plot_lineup(vi_lineup[["heter_169"]]$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```


An example lineup of this model is shown in Figure \ref{fig:example-heter-lineup} with $a = -1$ and $X_{raw} \sim U(-1, 1)$. The actual data plot location was 15. 8 out of 11 subjects correctly identified the actual data plot for this lineup. 

## Experimental setup

### Controlling the strength of the signal

As summarised by Table \ref{tab:model-parameter-table}, three additional parameters $n$, $\sigma$ and $b$ were used to control the strength of the signal so that different difficulty levels of lineups were generated, and therefore, the estimated power curve would be smooth and continuous. Parameter $\sigma \in \{0.5, 1, 2, 4\}$ and $b \in \{0.25, 1, 4, 16, 64\}$ were used in experiment I and II respectively. Figure \ref{fig:different-sigma} and \ref{fig:different-b} demonstrate the impact of these two parameters.

Three different sample sizes were used (n = 50, 100, 300) in all three experiments. It can be observed from Figure \ref{fig:different-n} that with fewer data points drawn in a residual plot, the visual pattern is more difficult to be detected.

```{r different-n, fig.cap="Three different values of $n$ are used in experiment I, II and III to control the strength of the signal."}
ori_dat <- poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(300)

VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(n~"="~300)) + 
  theme_light() -> p1

VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(50, computed = select(ori_dat[1:50,], x, e))) +
  ggtitle(quote(n~"="~50)) + 
  theme_light() -> p2

VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(50, computed = select(ori_dat[1:100,], x, e))) +
  ggtitle(quote(n~"="~100)) + 
  theme_light() -> p3

patchwork::wrap_plots(p2, p3, p1, ncol = 2)
```


```{r different-sigma, fig.cap="Example residual plots of the linear regression model used in experiment I. Four different values of $\\sigma$ are included in the experiment to control the strength of the signal."}
ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300)

VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(sigma~"="~0.5)) + 
  theme_light() -> p1

VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~1)) + 
  theme_light() -> p2

VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 2)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~2)) + 
  theme_light() -> p3

VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 4)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~4)) + 
  theme_light() -> p4

patchwork::wrap_plots(p1, p2, p3, p4, ncol = 2)
```

```{r different-b, fig.cap="Example residual plots of the linear regression model used in experiment II. Five different values of $b$ are included in the experiment to control the strength of the signal."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 0.25)$gen(300)

VI_MODEL$plot(ori_dat) +
  ggtitle(quote("b = 0.25")) +
  theme_light() -> p1

VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 1)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 1")) +
  theme_light() -> p2

VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 4)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 4")) +
  theme_light() -> p3

VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 16)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 16")) +
  theme_light() -> p4

VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 64)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 64")) +
  theme_light() -> p5

patchwork::wrap_plots(p1, p2, p3, p4, p5, ncol = 3)
```


### Subject allocation

Three replications are made for each of the parameter values shown in Table \ref{tab:parameter-table} resulting in 1152 different lineups. In addition, each lineup is designed to be evaluated by five different subjects and lineups with uniform distribution is designed to be evaluated by 11 subjects to provide reasonable estimates of the visual p-value.

Thus, $443$ subjects were recruited to satisfy the design of the experiment. 
<!-- TODO:subject allocation algorithm -->

### Collecting results

Subjects for all three experiments were recruited from an crowdsourcing platform called Prolific [ref here]. Prescreening procedure was applied during the recruitment, subjects were required to be fluent in English, with $98\%$ minimum approval rate in other studies and 10 minimum submissions. During the experiment, every subject was presented with a block of 20 lineups. For each lineup, the actual data plot was drawn as a standard residual plot of the null model with raw residuals on the y-axis and fitted values on the x-axis. An additional horizontal red line was added at $y = 0$ as a helping line. The 19 null datasets were generated by the residual rotation technique, and plotted in the same way. The lineup consisted of 20 residual plots with one randomly placed actual data plot. And for every lineup, the subject was asked to select one or more plots that are most different from others, provide a reason for their selections, and evaluate how different they think the selected plots were from others. If there was no noticeable difference between plots in a lineup, subjects were permitted to select zero plots without providing the reason. No subject was shown the same lineup twice. Information about preferred pronoun, age group, education, and previous experience in visual experiment were also collected. In every block of 20 lineups that presented to a subject, two lineups with obvious visual patterns were included as attention checks. A subject's submission was only accepted if the actual data plot was identified for at least one attention check. Data of rejected submissions were discarded automatically to maintain the overall data quality.

<!-- TODO:pipeline description -->





## Results 

### Power comparison

```{r}
vi_survey_processed <- readRDS(here::here("data/vi_survey_processed.rds")) 
```



```{r}
boot_y <- function(dat, y, times = 100) {
  map_df(1:times, function(i) slice_sample(dat, n = nrow(dat), replace = TRUE) %>% mutate(boot_id = i))
}
```


```{r}
vi_survey_processed %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "polynomial") %>%
  filter(e_sigma != 0.125) %>%
  mutate(reject_dirichlet = as.numeric(p_value_select_all_plots <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, reject_dirichlet, col = "VI_p_value"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  stat_smooth(geom = "line", 
              data = ~boot_y(.x, reject_dirichlet, times = 500), 
              aes(log_effect_size, reject_dirichlet, group = boot_id, col = "VI_p_value"), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, 
              alpha = 0.03) +
  geom_smooth(data = mutate(poly_conv_sim, log_effect_size = log(effect_size)) %>%
                pivot_longer(F_p_value:SW_p_value), 
              aes(log_effect_size, as.numeric(value < 0.05), col = name), 
              method = "glm", 
              method.args = list(family = binomial),
              se = FALSE) +
  facet_wrap(~x_dist) +
  ggtitle("poly")
```


```{r}
vi_survey_processed %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "heteroskedasticity") %>%
  filter(b != 0) %>%
  filter(b < 128) %>%
  mutate(reject_dirichlet = as.numeric(p_value_select_all_plots <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, reject_dirichlet, col = "VI_p_value"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  stat_smooth(geom = "line", 
              data = ~boot_y(.x, reject_dirichlet, times = 500), 
              aes(log_effect_size, reject_dirichlet, group = boot_id, col = "VI_p_value"), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, 
              alpha = 0.03) +
  geom_smooth(data = mutate(heter_conv_sim, log_effect_size = log(effect_size)) %>%
                pivot_longer(F_p_value:SW_p_value), 
              aes(log_effect_size, as.numeric(value < 0.05), col = name), 
              method = "glm", 
              method.args = list(family = binomial),
              se = FALSE) +
  facet_wrap(~x_dist) +
  ggtitle("heter")
```

### Data overview

<!-- How many people? How many lineups? -->

Subjects recruited from Prolific received a fixed payment for participating in the experiment. However, some subjects will try to maximize their earnings for minimum effort. During the review of submissions, if we found a subject objectively demonstrated clear low-effort throughout the experiment, i.e., failed all attention checks, we rejected the submission. The rejected submissions will be removed immediately, and Prolific will automatically recruit another subject as substitution. Therefore, we only paid for approved submissions and no further data screening procedure needed to be applied on the collected data.

In overall, there were a total of 3200 lineup evaluations made by 160 subjects in both experiment I and experiment II respectively, where 320 lineup evaluations were attention checks and were not used in the following analysis.

The collated dataset is provided in `vi_survey` of the `visage` `R` package.

<!-- ## Nonlinearity -->

<!-- -   Power curve overall -->
<!-- -   Four lineups also shown, selected from ones close to the visual power curve from the uniform treatment -->
<!-- -   Compare visual power against different factors -->

<!-- ## Heteroskedasticity -->

<!-- -   Power curve overall -->
<!-- -   Four lineups also shown, selected from ones close to the visual power curve from the uniform treatment -->
<!-- -   Compare visual power against different factors -->

<!-- ## Demographic summary -->

<!-- Table \ref{tab:demographic-table} tabulates the number of subjects, preferred pronouns, education backgrounds, age groups, and previous experience in visual experiment. Figure \ref{fig:demographic-summary-plot} visualizes the marginal distribution of each of the category. The collated data was a balanced sample among male and female. Most of the ages of subject were between 18 to 39, while many of them were between 18 to 24. Very few subjects were awarded degree higher than Bachelor Degree. Around 40% of subjects had previous experience in visual experiment. -->

<!-- ```{r results="asis"} -->

<!-- polynomials %>% -->
<!--   mutate(exp = 1) %>% -->
<!--   bind_rows(mutate(heter, exp = 2)) %>% -->
<!--   group_by(exp, set) %>% -->
<!--   slice_sample(n = 1) %>% -->
<!--   ungroup() %>% -->
<!--   count(pronoun, education, age_group, previous_experience) %>% -->
<!--   arrange(desc(n)) %>% -->
<!--   knitr::kable(col.names = c("Preferred pronoun", "Education", "Age group", "Previous experience", "Subject"), -->
<!--                escape = FALSE, -->
<!--                format = "latex", -->
<!--                linesep = "", -->
<!--                booktabs = TRUE,  -->
<!--                caption  = 'Summary of demographic information',  -->
<!--                label = "demographic-table" ) %>% -->
<!--   cat() -->
<!-- ``` -->

<!-- ```{r demographic-summary-plot, fig.cap="Summary of demographic information. \\label{fig:demographic-summary-plot}"} -->
<!-- polynomials %>% -->
<!--   mutate(exp = 1) %>% -->
<!--   bind_rows(mutate(heter, exp = 2)) %>% -->
<!--   group_by(exp, set) %>% -->
<!--   summarise(across(age_group:previous_experience, first)) %>% -->
<!--   mutate(education = fct_relevel(education, "High School or below")) %>% -->
<!--   ggplot() + -->
<!--   geom_bar(aes(y = education), orientation = "y") -> education_plot -->

<!-- polynomials %>% -->
<!--   mutate(exp = 1) %>% -->
<!--   bind_rows(mutate(heter, exp = 2)) %>% -->
<!--   group_by(exp, set) %>% -->
<!--   summarise(across(age_group:previous_experience, first)) %>% -->
<!--   ggplot() + -->
<!--   geom_bar(aes(age_group)) -> age_plot -->

<!-- polynomials %>% -->
<!--   mutate(exp = 1) %>% -->
<!--   bind_rows(mutate(heter, exp = 2)) %>% -->
<!--   group_by(exp, set) %>% -->
<!--   summarise(across(age_group:previous_experience, first)) %>% -->
<!--   mutate(pronoun = fct_relevel(pronoun, "He", "She", "They")) %>% -->
<!--   ggplot() + -->
<!--   geom_bar(aes(pronoun)) -> gender_plot -->

<!-- polynomials %>% -->
<!--   mutate(exp = 1) %>% -->
<!--   bind_rows(mutate(heter, exp = 2)) %>% -->
<!--   group_by(exp, set) %>% -->
<!--   summarise(across(age_group:previous_experience, first)) %>% -->
<!--   ggplot() + -->
<!--   geom_bar(aes(previous_experience)) -> experience_plot -->

<!-- patchwork::wrap_plots(education_plot, age_plot, gender_plot, experience_plot) -->
<!-- ``` -->

<!-- ## Model fitting -->

<!-- For each parameter combination, effect $E$ is derived from the Kullback-Leibler divergence (see [appendix ref here]) formulated as: -->

<!-- ```{=tex} -->
<!-- \begin{equation} \label{eq:effect-size-ex1} -->
<!-- E = \frac{1}{2\sigma^2}\boldsymbol{X}_b'\boldsymbol{R}_a'(diag(\boldsymbol{R}_a))^{-1}\boldsymbol{R}_a\boldsymbol{X}_b, -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- where $diag(.)$ is the diagonal matrix constructed from the diagonal elements of $\boldsymbol{R}_a$. -->

<!-- The logistic regression is fit using $log_e(E)$ as the only fixed effect covariate for the power of visual test formulated as: -->

<!-- ```{=tex} -->
<!-- \begin{equation} \label{eq:logistic-regression-1-1} -->
<!-- Pr(\text{reject}~H_0|H_1,E) = \Lambda(\beta_0 + \beta_1 log_e(\boldsymbol{E})), -->
<!-- \end{equation} -->
<!-- ``` -->
<!-- where $\Lambda(.)$ is the standard logistic function given as $\Lambda(z) = exp(z)/(1+exp(z))$. -->

<!-- To study various factors contributing to the power of the visual test, the same logistic regression model is fit on different subsets of the collated data grouped by levels of factors. This includes [expansion]. -->

<!-- Table [table ref here] shows the parameter estimates of the logistic regressions. [Discussion about the numeric estimates here] -->

<!-- ## Power comparison -->

<!-- ### Experiment I -->

<!-- Figure \ref{fig:power-overview} shows an overview of estimated power of visual test against natural logarithm of the effect with comparison to the power of an exact test - F-test, and the power of two other residual-based conventional tests commonly used in regression diagnostics but for testing other departures from the model assumptions. In overall, the power of all four tests increases as the effect becomes larger. The power curve of F-test climbs aggressively from 25% to around 90% as $log_e(E)$ increases from 0 to 2, while others respond inactively to the change of effect and remain lower than 25% throughout the period, showing that as an exact test, the F-test is relatively more sensitive to the type of model defects that being considered. The power of visual test arises steadily and nearly linearly to around 90% as $log_e(E)$ increases from 2 to 5, suggesting that the effect starts to make noticeable impact on the degree of the presence of the designed visual features. Other two inappropriate conventional tests shows improvement at the same time but at a lower rate. This coincides the point made by @cook1982residuals mentioned in \ref{hypothesis-testing} that residual-based tests for a specific type of model defect are sensitive to other types of model defects. At $log_e(E) = 6$, the power curve of F-test reaches almost 100% followed by the visual test by a small margin. The power of Breusch--Pagan test and Shapiro--Wilk test reach around 75% and 63% respectively. -->

<!-- What truly impress us is the huge difference between the estimated power of visual test and the estimated power of F-test. The margin is largest at around $log_e(E) = 2$. An example lineup is included in Figure \ref{fig:power-overview} where none of subjects detect the actual data plot positioned at panel 14. It demonstrates that at this level of difficulty, the designed visual feature is rarely visible, making the actual data plot indistinguishable from residual plots simulated from the assumed model. From a communication perspective, given the fact that the visual difference is unperceivable, the argument that non-linearity present in the fitted model is less convincing to the public even though it is true. At around $log_e(E) = 3$, the margin gets smaller as the chance of identifying the actual data plot becomes larger. At this level of difficulty, the designed visual features are usually detectable but it may not stand out from the lineup as other null plots may happen to include outliers or visual patterns that are are considered to be more attractive by human, and thus recognized as the most different plot. Without knowing the designed visual features beforehand, it is actually hard to identify the actual data plot by pure image comparison. The corresponding example lineup for $log_e(E) = 3$ shown in Figure \ref{fig:power-overview} has the actual data plot positioned at panel 20, where two out of five subjects detect it. It can be observed that a M-shape is presented in plot 20, but the signal is not strong enough to attract all five subjects, resulting in a visual p-value sightly above the desired significance level $\alpha = 0.05$. At $log_e(E) = 4$ and $log_e(E) = 6$, the designed visual features become much clear and attractive, leading to a high percentage of rejection of the null hypothesis. Figure \ref{fig:power-overview} gives example lineups of such cases. -->

<!-- It is also interesting to observe that the power curve of visual test behaves similarly to those inappropriate conventional tests until the designed visual features really becomes visible   -->

<!-- ```{r} -->
<!-- conv_test_plot <- function() { -->
<!--   conv_result %>% -->
<!--   ggplot() + -->
<!--   stat_smooth(geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "F-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 0.05) + -->
<!--   stat_smooth(geom = "line",  -->
<!--               aes(log(effect_size), reject, col = "F-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 1) + -->
<!--   labs(col = "Test") + -->
<!--   theme_light(base_size = 5) + -->
<!--   xlab(quote(log[e](E))) + -->
<!--   ylab("Power") -->
<!-- } -->

<!-- add_wrong_BP_conv_test_plot_layers <- function(p) { -->
<!--   p + -->
<!--   stat_smooth(data = wrong_BP_conv_result, geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "BP-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 0.05) + -->
<!--   stat_smooth(data = wrong_BP_conv_result, geom = "line",  -->
<!--               aes(log(effect_size), reject, col = "BP-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 1) -->
<!-- } -->

<!-- add_wrong_SW_conv_test_plot_layers <- function(p) { -->
<!--   p + -->
<!--   stat_smooth(data = wrong_SW_conv_result, geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "SW-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 0.05) + -->
<!--   stat_smooth(data = wrong_SW_conv_result, geom = "line",  -->
<!--               aes(log(effect_size), reject, col = "SW-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 1) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- boot_dat <- function(dat, times = 100) { -->
<!--   result <- mutate(dat, boot_id = 1) -->
<!--   for (i in 2:times) { -->
<!--     result <- bind_rows(result, mutate(dat[sample(1:nrow(dat), replace = TRUE), ], boot_id = i))  -->
<!--   } -->
<!--   result -->
<!-- } -->

<!-- add_visual_test_plot_layers <- function(p, points = c()) { -->
<!--   visual_dat <- polynomials %>% -->
<!--   group_by(lineup_id) %>% -->
<!--   summarise(across(c(type:p_value, prop_detect), first)) %>% -->
<!--   filter(lineup_id < 577) %>% -->
<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->
<!--   boot_dat(times = 100) -->

<!--   p + -->
<!--   stat_smooth(data = filter(visual_dat, boot_id == 1), geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "Visual"),  -->
<!--               method = "glm", method.args = list(family = binomial), se = FALSE, alpha = 1) + -->
<!--   stat_smooth(data = visual_dat, geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "Visual"),  -->
<!--               method = "glm", method.args = list(family = binomial), se = FALSE, alpha = 0.05) + -->
<!--   geom_point(data = filter(visual_dat, boot_id == 1), aes(log(effect_size), prop_detect), alpha = 0.1) +  -->
<!--   geom_point(data = filter(visual_dat, boot_id == 1, lineup_id %in% points), aes(log(effect_size), prop_detect), alpha = 1, col = "red", size = 2) -->
<!-- } -->

<!-- control_col <- function(p, ...) { -->
<!--   p + scale_color_manual(values = unlist(list(...))) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r eval = FALSE} -->
<!-- filter(polynomials, lineup_id == 131) -->

<!-- polynomials %>% -->
<!--   group_by(lineup_id) %>% -->
<!--   summarise(dd = first(abs(log(effect_size) - 5)), p_value = first(p_value), prop_detect = first(prop_detect)) %>% -->
<!--   arrange(dd) %>% -->
<!--   head(30) %>% -->
<!--   summarise(tt = median(prop_detect)) -->

<!-- polynomials %>% -->
<!--   ggplot() + -->
<!--   geom_point(aes(prop_detect, p_value < 0.06)) -->

<!-- bottom_plot_2 -->

<!-- polynomials %>% -->
<!--   filter(lineup_id == 364) -->

<!-- polynomials %>% -->
<!--   filter(e_sigma == 4) %>% -->
<!--   group_by(lineup_id) %>% -->
<!--   summarise(p_value = first(p_value)) %>% -->
<!--   mutate(reject = p_value < 0.05) %>% -->
<!--   {mean(.$reject)} -->

<!-- 4 * 4 * 3 * 5 -->
<!-- ``` -->

<!-- ```{r power-overview, fig.height=7.5, fig.cap="Power overview. \\label{fig:power-overview}"} -->
<!-- conv_test_plot() %>% -->
<!--   add_wrong_BP_conv_test_plot_layers() %>% -->
<!--   add_wrong_SW_conv_test_plot_layers() %>% -->
<!--   add_visual_test_plot_layers(points = c(415, 131, 322, 378)) + -->
<!--   geom_vline(xintercept = 2, linetype = 2) +  -->
<!--   geom_vline(xintercept = 3, linetype = 2) + -->
<!--   geom_vline(xintercept = 4, linetype = 2) + -->
<!--   geom_vline(xintercept = 5, linetype = 2) -> base_plot -->



<!-- # VI_MODEL$plot_lineup(polynomials_lineup$lineup_314$data, -->
<!-- #                      size = 0.1, -->
<!-- #                      stroke = 0.2, -->
<!-- #                      remove_grid_line = TRUE, -->
<!-- #                      theme = theme_light(base_size = 5), -->
<!-- #                      remove_axis = TRUE) + -->
<!-- #   ggtitle(bquote(log[e](E) == 0~", F-test p-value" == .(format(filter(polynomials, lineup_id == 314)$conventional_p_value[1], digits = 3))~", Visual p-value " == .(format(filter(polynomials, lineup_id == 314)$p_value[1], digits = 3)))) -> bottom_plot_1 -->

<!-- VI_MODEL$plot_lineup(polynomials_lineup$lineup_415$data, -->
<!--                      size = 0.1, -->
<!--                      stroke = 0.2, -->
<!--                      remove_grid_line = TRUE, -->
<!--                      theme = theme_light(base_size = 5), -->
<!--                      remove_axis = TRUE) + -->
<!--   ggtitle(bquote(log[e](E) %~~% 2~","~"F-test p-value" == .(format(filter(polynomials, lineup_id == 415)$conventional_p_value[1], digits = 3))~", Visual p-value " == .(format(filter(polynomials, lineup_id == 415)$p_value[1], digits = 3)))) -> bottom_plot_1 -->

<!-- VI_MODEL$plot_lineup(polynomials_lineup$lineup_131$data, -->
<!--                      size = 0.1, -->
<!--                      stroke = 0.2, -->
<!--                      remove_grid_line = TRUE, -->
<!--                      theme = theme_light(base_size = 5), -->
<!--                      remove_axis = TRUE) + -->
<!--   ggtitle(bquote(log[e](E) %~~% 3~", F-test p-value" == .(format(filter(polynomials, lineup_id == 131)$conventional_p_value[1], digits = 3))~", Visual p-value " == .(format(filter(polynomials, lineup_id == 131)$p_value[1], digits = 3)))) -> bottom_plot_2 -->

<!-- VI_MODEL$plot_lineup(polynomials_lineup$lineup_322$data, -->
<!--                      size = 0.1, -->
<!--                      stroke = 0.2, -->
<!--                      remove_grid_line = TRUE, -->
<!--                      theme = theme_light(base_size = 5), -->
<!--                      remove_axis = TRUE) + -->
<!--   ggtitle(bquote(log[e](E) %~~% 4~","~"F-test p-value" == .(format(filter(polynomials, lineup_id == 322)$conventional_p_value[1], digits = 3))~", Visual p-value " == .(format(filter(polynomials, lineup_id == 322)$p_value[1], digits = 3)))) -> bottom_plot_3 -->

<!-- VI_MODEL$plot_lineup(polynomials_lineup$lineup_378$data, -->
<!--                      size = 0.1, -->
<!--                      stroke = 0.2, -->
<!--                      remove_grid_line = TRUE, -->
<!--                      theme = theme_light(base_size = 5), -->
<!--                      remove_axis = TRUE) + -->
<!--   ggtitle(bquote(log[e](E) %~~% 5~","~"F-test p-value" == .(format(filter(polynomials, lineup_id == 378)$conventional_p_value[1], digits = 3))~", Visual p-value " == .(format(filter(polynomials, lineup_id == 378)$p_value[1], digits = 3)))) -> bottom_plot_4 -->


<!-- design <- c(patchwork::area(1, 1, 1, 8),  -->
<!--             patchwork::area(2, 1, 2, 4),  -->
<!--             patchwork::area(2, 5, 2, 8),  -->
<!--             patchwork::area(3, 1, 3, 4),  -->
<!--             patchwork::area(3, 5, 3, 8)) -->

<!-- patchwork::wrap_plots(base_plot, B = bottom_plot_1, C = bottom_plot_2, D = bottom_plot_3, E = bottom_plot_4, design = design) -->
<!-- ``` -->

<!-- ### Distributions of regressor -->

<!-- The impact of the distribution of $X_raw$ on the power is shown in Figure \ref{fig:dist-power}. The power curve of F-test is stable across different distributions, while the visual test has a steeper power curve for normal and uniform distribution. BP-test performs worse for discrete uniform distribution and uniform distribution but has relatively high power for normal distribution. SW-test outperforms BP-test for discrete uniform distribution but remains as the worst test for other distributions. The results indicate those inappropriate residual-based tests are sensitive to the distribution of the regressor. -->

<!-- ```{r dist-power, fig.cap="\\label{dist-power}"} -->
<!-- conv_test_plot() %>% -->
<!--   add_wrong_BP_conv_test_plot_layers() %>% -->
<!--   add_wrong_SW_conv_test_plot_layers() %>% -->
<!--   add_visual_test_plot_layers() + -->
<!--   facet_wrap(~x_dist) -->
<!-- ``` -->

<!-- #### Shpaes of non-linearity -->

<!-- Figure \ref{fig:shape-power} illustrates the change of power under different shape of non-linearity. Similar to the power curves shown in \ref{fig:dist-power}, F-test is stable under different shapes. The power curve of visual test also behaves similarly across different shapes. What vary are the power curve of BP-test and SW-test. For Triple-U shape, both BP-test and SW-test are insensitive to the change of the effect. And for W shape, both tests have almost identical power curves. It can be observed that both tests performs the best for U-shape. -->

<!-- ```{r shape-power, fig.cap="\\label{shape-power}"} -->
<!-- conv_test_plot() %>% -->
<!--   add_wrong_BP_conv_test_plot_layers() %>% -->
<!--   add_wrong_SW_conv_test_plot_layers() %>% -->
<!--   add_visual_test_plot_layers() + -->
<!--   facet_wrap(~shape) -->
<!-- ``` -->

<!-- ```{r eval = FALSE} -->
<!-- conv_test_plot() %>% -->
<!--   add_wrong_BP_conv_test_plot_layers() %>% -->
<!--   add_wrong_SW_conv_test_plot_layers() %>% -->
<!--   add_visual_test_plot_layers() + -->
<!--   facet_wrap(~e_sigma) -->
<!-- ``` -->

<!-- #### Reasons for making selections -->

<!-- ```{r} -->
<!-- polynomials %>% -->
<!--   mutate(reason_tmp = ifelse(reason %in% c("Cluster(s)", "Shape", "Outlier(s)"), reason, "Other")) %>% -->
<!--   ggplot() + -->
<!--   geom_bar(aes(reason_tmp)) + -->
<!--   facet_grid(x_dist~detect)  -->
<!-- ``` -->

<!-- ### Experiment II -->

<!-- <!-- todo: check the two way interaction for x_dist and a --> -->

<!-- ```{r eval = FALSE} -->
<!-- heter %>% -->
<!--   filter(b < 3) %>% -->
<!--   group_by(lineup_id) %>% -->
<!--   summarise(p_value = first(p_value), n_eval = n()) %>% -->
<!--   filter(n_eval > 3) %>% -->
<!--   mutate(reject = p_value < 0.05) %>% -->
<!--   {mean(.$reject)} -->


<!-- ``` -->

<!-- ```{r} -->
<!-- map_to_group <- function(b) { -->
<!--   round(b / 4) + 1 -->
<!-- } -->


<!-- heter_conv_test_plot <- function() { -->
<!--   conv_result_heter %>% -->
<!--   mutate(cat_b = map_to_group(b)) %>% -->
<!--   ggplot() + -->
<!--   stat_smooth(geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "BP-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 0.05) + -->
<!--   stat_smooth(geom = "line",  -->
<!--               aes(log(effect_size), reject, col = "BP-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 1) + -->
<!--   labs(col = "Test") + -->
<!--   theme_light(base_size = 5) + -->
<!--   xlab(quote(log[e](E))) + -->
<!--   ylab("Power") -->
<!-- } -->

<!-- add_heter_SW_plot_layers <- function(p) { -->
<!--   p + -->
<!--   stat_smooth(data = mutate(conv_result_heter_SW, cat_b = map_to_group(b)), geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "SW-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 0.05) + -->
<!--   stat_smooth(data = mutate(conv_result_heter_SW, cat_b = map_to_group(b)), geom = "line",  -->
<!--               aes(log(effect_size), reject, col = "SW-test"),  -->
<!--               se = FALSE, method = "glm", method.args = list(family = binomial()), alpha = 1) -->
<!-- } -->

<!-- add_heter_visual_test_plot_layers <- function(p, points = c()) { -->

<!--   visual_dat <- heter %>% -->
<!--   mutate(x_dist = ifelse(x_dist == "discrete_uniform", "even_discrete", x_dist)) %>% -->
<!--   mutate(x_dist = ifelse(x_dist == "neglognormal", "lognormal", x_dist)) %>% -->
<!--   group_by(lineup_id) %>% -->
<!--   summarise(across(c(type:p_value, prop_detect), first)) %>% -->
<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->
<!--   boot_dat(times = 100) %>% -->
<!--   mutate(cat_b = map_to_group(b)) -->

<!--   p + -->
<!--   stat_smooth(data = filter(visual_dat, boot_id == 1), geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "Visual"),  -->
<!--               method = "glm", method.args = list(family = binomial), se = FALSE, alpha = 1) + -->
<!--   stat_smooth(data = visual_dat, geom = "line",  -->
<!--               aes(log(effect_size), reject, group = boot_id, col = "Visual"),  -->
<!--               method = "glm", method.args = list(family = binomial), se = FALSE, alpha = 0.05) + -->
<!--   geom_point(data = filter(visual_dat, boot_id == 1), aes(log(effect_size), prop_detect), alpha = 0.1) +  -->
<!--   geom_point(data = filter(visual_dat, boot_id == 1, lineup_id %in% points), aes(log(effect_size), prop_detect), alpha = 1, col = "red", size = 2) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- heter_conv_test_plot() %>% -->
<!--   add_heter_SW_plot_layers() %>% -->
<!--   add_heter_visual_test_plot_layers() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- heter_conv_test_plot() %>% -->
<!--   add_heter_SW_plot_layers() %>% -->
<!--   add_heter_visual_test_plot_layers() + -->
<!--   facet_wrap(~a) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- heter_conv_test_plot() %>% -->
<!--   add_heter_SW_plot_layers() %>% -->
<!--   add_heter_visual_test_plot_layers() + -->
<!--   facet_grid(x_dist~a) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- heter_conv_test_plot() %>% -->
<!--   add_heter_SW_plot_layers() %>% -->
<!--   add_heter_visual_test_plot_layers() + -->
<!--   facet_wrap(~x_dist) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- heter_conv_test_plot() %>% -->
<!--   add_heter_SW_plot_layers() %>% -->
<!--   add_heter_visual_test_plot_layers() + -->
<!--   facet_wrap(~cat_b) -->
<!-- ``` -->

<!-- # time taken vs order of the plot -->

<!-- # accuracy vs order of the plot -->

<!-- # num of selection vs order of the plot -->

<!-- # order of the plot? -->

<!-- # summary of the demographic information -->

<!-- # Old -->

<!-- ## Overview of the Data -->

<!-- We collected 400 lineup evaluations made by 20 participants in experiment I and 880 lineup evaluations made by 44 participants in experiment II. In total, 442 unique lineups were evaluated by 64 subjects. In experiment I, one of the participants skipped all 20 lineups. Hence, the submission was rejected and removed from the dataset. In experiment II, there was a participant failed one of the two attention checks, but there was no further evidence of low-effort throughout the experiment. Therefore, the submission was kept. -->

<!-- ## Power comparision -->

<!-- 1. power (visual test vs. conventional test) -->

<!-- (visual test most different one (everything test, any departure)) -->

<!-- plot figure in a paper, desc, exp -->

<!-- 2. investigate the difference (gap), give examples -->

<!-- 3. conventional is too sensitive -->

<!-- 4. make conventional less sensitive (vary alpha) -->

<!-- To model the power of visual test, 10 logistic regression were fit for different number of evaluations ranged from one to five and two different types of simulation setting. All 10 models used natural logarithm of the effect size as the only fixed effect, and whether the test successfully rejects the null hypothesis as the response variable. Given the way we define the effect size, it was expected that with larger effect size, both conventional test and visual test will have higher probability in rejecting the null hypothesis when it is not true. The modelling result summarized in \ref{tab:powerglmcubic} and \ref{tab:powerglmheter} aligned with the expectation as the coefficients of natural logarithm of the effect size are positive and significant across all 10 models. -->

<!-- Figure \ref{fig:power-com} illustrates the fitted models, while providing the local constant estimate of the power of F-test and Breusch–Pagan test for comparison. Data for the conventional test is simulated under the model setting described in section ... and 5000000 samples are drawn for both cubic and heteroskedasticity model. From Figure \ref{fig:power-com}, it can be observed that the fitted power of visual test increased as the number of evaluations increased for both cubic and heteroskedasticity model. -->

<!-- For heteroskedasticity model, this phenomenon was more obvious as the power of visual tests with evaluations greater than two were always greater than those with evaluations smaller than two.  -->

<!-- For cubic model, the separation between curves was small. The estimated power of visual tests with three to five evaluations were almost identical to each other in regards of effect size. In addition, all five curves peaked at one as effect size increased, suggesting that identification of non-linearity as a visual task can be completed reliably by human as long as the departure from null hypothesis is large enough. -->

<!-- As shown in Figure \ref{fig:power-com}, both F-test and Breusch–Pagan test generally possessed greater power than visual test. A visual tests is a collection of test against any alternatives that would create visual discoverable features, while a conventional test is usually targeting at a pre-specified alternative. Considering the data generating process of the model defect was known and controlled in this research, where all other alternatives have been eliminated except the one we concerned, the result was suggested that conventional tests were more sensitive to violations of linearity and homoscedasticity assumption than visual tests.  -->

<!-- It was also found that there was a noticeable gap between curves of the conventional test and the visual test at around $log(\text{effect size}) = 0$ for the cubic model and $log(\text{effect size}) = 2.5$ for the heteroskedasticity model, where the differences in power were greater than 0.6. We further analysed the lineups with correspoding effect sizes. Figure \ref{fig:cubic-hard} and \ref{fig:heter-hard} showed that human was indeed hard to identify the patterns at this level of difficulty. The visual difference between the true data plot and null plots were almost unnoticeable.  -->

<!-- <!-- sample 1, change values of a and b, plot the data  -->

--\>

<!-- ## Effect of parameters on power of the visual test -->

<!-- The previous section focuses on the change of effect size relative to the power of the visual test. However, effect size is only a one dimensional summarisation of parameters used in data simulation. Individual factor embedded in the simulation process should also be analysed.  -->

<!-- In cubic model, two major factors that influencing the strength of the signal are $a$ and $b$. Figure \ref{fig:power-com-cubic-a} and \ref{fig:power-com-cubic-a} illustrates 30 different logistic regressions fit for different number of evaluations and different number of observations $n$. The regressor used in these models was $|a|/\sigma$ since the noise level $\sigma$ needed to be taken into account. From the figures, we can observe ... -->

<!-- ```{r power-vs-log-effect-size} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") -->

<!-- ``` -->

<!-- ```{r power-vs-log-effect-size-given-x-dist} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   facet_wrap(~x_dist) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power comparison conditional on the distribution of X") -->

<!-- ``` -->

<!-- ```{r power-of-visual-test-given-x-dist} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = x_dist), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power of visual test conditional on the distribution of X") -->

<!-- ``` -->

<!-- ```{r power-vs-log-effect-size-given-shape} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   facet_wrap(~shape) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power comparison conditional on the shape of the Hermite polynomials") -->

<!-- ``` -->

<!-- ```{r power-of-visual-test-given-shape} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01)), shape = factor(shape)) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = shape), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power of visual test conditional on the shape of the Hermite polynomials") -->

<!-- ``` -->

<!-- ```{r power-vs-log-effect-size-given-number-of-observations} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = conv_result, aes(log(effect_size), reject, col = "Conventional test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   facet_wrap(~n) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power comparison conditional on the number of observations") -->

<!-- ``` -->

<!-- ```{r power-of-visual-test-given-number-of-observations} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01)), n = factor(n)) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = n), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") + -->

<!--   ggtitle("Power of visual test conditional on the number of observations") -->

<!-- ``` -->

<!-- ```{r} -->

<!-- polynomials %>% -->

<!--   group_by(lineup_id) %>% -->

<!--   summarise(across(c(type:p_value), first)) %>% -->

<!--   filter(lineup_id < 577) %>% -->

<!--   mutate(reject = as.numeric(eval_p_value(p_value, tol = 0.01))) %>% -->

<!--   ggplot() + -->

<!--   geom_smooth(aes(log(effect_size), reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = mutate(conv_result, reject = as.numeric(p_value < 0.05)), aes(log(effect_size), reject, col = "Conventional test:0.05"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = mutate(conv_result, reject = as.numeric(p_value < 0.01)), aes(log(effect_size), reject, col = "Conventional test:0.01"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = mutate(conv_result, reject = as.numeric(p_value < 0.005)), aes(log(effect_size), reject, col = "Conventional test:0.005"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = mutate(conv_result, reject = as.numeric(p_value < 0.0005)), aes(log(effect_size), reject, col = "Conventional test:0.0005"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   geom_smooth(data = mutate(conv_result, reject = as.numeric(p_value < 0.0000005)), aes(log(effect_size), reject, col = "Conventional test:0.0000005"), method = "glm", method.args = list(family = binomial), se = FALSE) + -->

<!--   xlab("Natural logarithm of effect size") + -->

<!--   ylab("Power") -->

<!-- ``` -->
