---
title: |
  Why is it that statistical tests for residuals are not widely used? An explanation using visual inference.
type: ARTICLE TEMPLATE
author:
  - name: Weihao Li
    affil: a
    email: weihao.li@monash.edu
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a
    email: emi.tanaka@monash.edu
  - name: Susan VanderPlas
    affil: b
    email: susan.vanderplas@unl.edu
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
  - num: b
    address: |
      Department of Statistics, University of Nebraska, Lincoln, Nebraska, USA
bibliography: paper.bib
abstract: |
  Abstract to fill.
keywords: |
  data visualization; visual inference; hypothesis testing; residual plots;
header-includes: |
  \usepackage{lscape}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{xcolor}
  \definecolor{orange-red}{rgb}{1.0, 0.27, 0.0}
  \newcommand{\svp}[1]{{\textcolor{orange-red}{#1}}}
  \newcommand*{\Perm}[2]{{}^{#1}\!P_{#2}}%
output: rticles::tf_article
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%",
  fig.align = "center")
```

```{r}
# OOP supports needed by `visage`
# remotes::install_github("TengMCing/bandicoot")
# 
# Visual inference models and p-value calculation
# remotes::install_github("TengMCing/visage")

library(tidyverse)
library(visage)

# To control the simulation in this file
set.seed(10086)
```

```{r get-lineup-data}
# The lineup data used to draw residual plot needed to be downloaded
# from the github repo. Cache it.
if (!file.exists(here::here("data/vi_lineup.rds"))) {
  vi_lineup <- get_vi_lineup()
  if (!dir.exists(here::here("data/"))) dir.create(here::here("data/"))
  saveRDS(vi_lineup, here::here("data/vi_lineup.rds"))
} else {
  vi_lineup <- readRDS(here::here("data/vi_lineup.rds"))
}
```

```{r poly-conventional-simulation}
# Ensure the support of the regressor is [-1, 1]
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

# Run simulations to get the behaviours of conventional tests
if (!file.exists(here::here("data/poly_conventional_simulation.rds"))) {
  poly_conv_sim <- list()
  
  # 100 bootstrap samples
  for (i in 1:100)
  {
    poly_conv_sim[[i]] <- 
      # Every sample contains 2000 lineups
      map(1:2000, function(i) {
        
        # Sample a set of parameters
        shape <- sample(1:4, 1)
        e_sigma <- sample(c(0.5, 1, 2, 4), 1)
        x_dist <- sample(c("uniform", 
                           "normal", 
                           "lognormal", 
                           "even_discrete"), 1)
        x <- switch(x_dist,
                    uniform = rand_uniform(-1, 1),
                    normal = {
                      raw_x <- rand_normal(sigma = 0.3)
                      closed_form(~stand_dist(raw_x))
                      },
                    lognormal = {
                      raw_x <- rand_lognormal(sigma = 0.6)
                      closed_form(~stand_dist(raw_x/3 - 1))
                      },
                    even_discrete = rand_uniform_d(k = 5, even = TRUE))
        
        # Build the model
        mod <- poly_model(shape, x = x, sigma = e_sigma)
        
        # Sample the number of observations
        n <- sample(c(50, 100, 300), 1)
        
        # Generate data from the model 
        tmp_dat <- mod$gen(n)
        
        # Get the effect size from the pre-calculated table 
        es <- filter(vi_survey, 
                     shape == {{shape}},
                     e_sigma == {{e_sigma}},
                     x_dist == {{x_dist}},
                     n == {{n}}) %>%
          head(1) %>%
          pull(effect_size)
        
        # Return a data frame containing p-values of
        # F, RESET, BP and SW tests
        tibble(shape = shape,
               e_sigma = e_sigma,
               x_dist = x_dist,
               n = n,
               effect_size = es,
               F_p_value = mod$test(tmp_dat)$p_value,
               RESET3_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:3, 
                                         power_type = "fitted")$p_value,
               RESET4_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:4, 
                                         power_type = "fitted")$p_value,
               RESET5_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:5, 
                                         power_type = "fitted")$p_value,
               RESET6_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:6, 
                                         power_type = "fitted")$p_value,
               RESET7_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:7, 
                                         power_type = "fitted")$p_value,
               RESET8_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:8, 
                                         power_type = "fitted")$p_value,
               RESET9_p_value = mod$test(tmp_dat, 
                                         test = "RESET", 
                                         power = 2:9, 
                                         power_type = "fitted")$p_value,
               RESET10_p_value = mod$test(tmp_dat, 
                                          test = "RESET", 
                                          power = 2:10, 
                                          power_type = "fitted")$p_value,
               BP_p_value = HETER_MODEL$test(tmp_dat)$p_value,
               SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
               boot_id = i)
        }) %>%
  reduce(bind_rows)
  }
  
  poly_conv_sim <- poly_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(poly_conv_sim, here::here("data/poly_conventional_simulation.rds"))
} else {
  poly_conv_sim <- readRDS(here::here("data/poly_conventional_simulation.rds"))
}
```



```{r heter-conventional-simulation}
# Run simulations to get the behaviours of conventional tests
if (!file.exists(here::here("data/heter_conventional_simulation.rds"))) {
  heter_conv_sim <- list()
  
  # 100 bootstrap samples
  for (i in 1:100)
  {
    heter_conv_sim[[i]] <-
      # Every sample contains 2000 lineups
      map(1:2000, function(x) {
        
        # Sample a set of parameters
        a <- sample(c(-1, 0, 1), 1)
        b <- sample(c(0.25, 1, 4, 16, 64), 1)
        x_dist <- sample(c("uniform", 
                           "normal", 
                           "lognormal", 
                           "even_discrete"), 1)
        x <- switch(x_dist,
                    uniform = rand_uniform(-1, 1),
                    normal = {
                      raw_x <- rand_normal(sigma = 0.3)
                      closed_form(~stand_dist(raw_x))
                      },
                    lognormal = {
                      raw_x <- rand_lognormal(sigma = 0.6)
                      closed_form(~stand_dist(raw_x/3 - 1))
                      },
                    even_discrete = rand_uniform_d(-1, 1, k = 5, even = TRUE))
        
        # Build the model
        mod <- heter_model(a = a, b = b, x = x)
        
        # Sample the number of observations
        n <- sample(c(50, 100, 300), 1)
        
        # Generate data from the model 
        tmp_dat <- mod$gen(n)
        
        # Get the effect size from the pre-calculated table 
        es <- filter(vi_survey, 
                     a == {{a}}, 
                     b == {{b}},
                     x_dist == {{x_dist}},
                     n == {{n}}) %>%
          head(1) %>%
          pull(effect_size)
        
        # Return a data frame containing p-values of
        # F, RESET, BP and SW tests
        tibble(a = a,
               b = b,
               x_dist = x_dist,
               n = n,
               effect_size = es,
               F_p_value = POLY_MODEL$test(
                 
                 # Create a pseudo z to be able to use F-test
                 tmp_dat %>%
                   mutate(z = poly_model()$
                            gen(n, computed = select(tmp_dat, x)) %>%
                            pull(z))
                 )$p_value,
               RESET3_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:3, 
                                                power_type = "fitted")$p_value,
               RESET4_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:4, 
                                                power_type = "fitted")$p_value,
               RESET5_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:5, 
                                                power_type = "fitted")$p_value,
               RESET6_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:6, 
                                                power_type = "fitted")$p_value,
               RESET7_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:7, 
                                                power_type = "fitted")$p_value,
               RESET8_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:8, 
                                                power_type = "fitted")$p_value,
               RESET9_p_value = POLY_MODEL$test(tmp_dat, 
                                                test = "RESET", 
                                                power = 2:9, 
                                                power_type = "fitted")$p_value,
               RESET10_p_value = POLY_MODEL$test(tmp_dat, 
                                                 test = "RESET", 
                                                 power = 2:10, 
                                                 power_type = "fitted")$p_value,
               BP_p_value = mod$test(tmp_dat)$p_value,
               SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
               boot_id = i)
        }) %>%
  reduce(bind_rows)
  }
  
  heter_conv_sim <- heter_conv_sim %>%
    reduce(bind_rows)
  
  saveRDS(heter_conv_sim, here::here("data/heter_conventional_simulation.rds"))
} else {
  heter_conv_sim <- readRDS(here::here("data/heter_conventional_simulation.rds"))
}
```

Title 
(jokes: finding the pattern in residual tests) 
(why using visulization is important in regression diagnostics? an explanation using visual inference)
(a picture is worth a thousand word)
(a plot is worth a thousand test: )

# Introduction

> *"Since all models are wrong the scientist must be alert to what is importantly wrong."* [@box1976science]

Diagnosing a model is the key to determining whether there is anything importantly wrong. In linear regression analysis, residuals are typically examined for model diagnostics. Residuals summarise what is not captured by the model, and thus provide the capacity to identify what might be wrong. 

We can assess residuals in multiple ways. Residuals might be plotted, as a histogram or quantile-quantile plot to examine the distribution. Using the classical normal linear regression model as an example, if the distribution is symmetric and unimodal, we consider it well-behaved. But if the distribution is skewed, bimodal, multimodal, or contains outliers, there is cause for concern. One could also inspect the distribution by conducting a goodness of fit test, such as the Shapiro-Wilk Normality test [@shapiro1965analysis].

More typically, residuals will be plotted, as a scatter plot against the predicted values and each of the explanatory variables to scrutinize their relationships. If there are any visually discoverable patterns, the model is potentially misspecified. In general, one looks for noticeable departures from the model like non-linear dependency or heteroskedasticity. However, correctly judging a residual plot where no pattern exists is a painstakingly difficult task for humans (?citation). It is especially common, particularly among new data analysts, to report patterns when an experienced one might quickly conclude that there are none. It is also possible to conduct hypothesis tests for non-linear dependence [@ramsey_tests_1969], and use a Breusch-Pagan test [@breusch_simple_1979] for heteroskedasticity.

Abundance of literature describe appropriate diagnostic methods for linear regression: @draper1998applied, @montgomery1982introduction, @belsley_regression_1980, @cook_applied_1999 and @cook1982residuals. The diligent reader of these sage writings will also notice sentences that express sentiments like *based on their experience, statistical tests are not widely used in regression diagnostics since the same or even larger amount of information can be provided by diagnostic plots than the corresponding tests in most empirical studies.* A common guidance by experts is that optimal method for diagnosing model fits is by plotting the data.

The persistence of this advice to check the plots is curious, and investigating why this might be common advice is the subject of this paper. The paper is structured as follows. The next background section describes the types of departures that one expects to detect, and outlines a formal statistical process for reading residual plots, called visual inference. Section \ref{experimental-design} details the experimental design to compare the decision made by formal hypothesis testing, and how humans would read diagnostic plots. The results are reported in Section \ref{results}. We finish with a discussion on future work, in particular how the responsibility for residual plot reading might be relieved.

# Background

## Departures from good residual plots

```{r residual-plot-common-departures, fig.width = 10, fig.height = 2.5, fig.cap = "Example residual vs fitted plots: (A) classically good looking residuals, (B) non-linear pattern indicates that the model has not captured a non-linear association, (C) heteroskedasticity indicating that variance around the fitted model is not uniform, and (D) non-normality where the residual distribution is not symmetric around 0. The latter pattern might best be assessed using a univariate plot of the residuals, but patterns B and C need to be assessed using a residual vs fitted plot."}

set.seed(10086)

# Base model
mod <- poly_model(include_z = FALSE, sigma = 0.25)

# Base data
dat <- mod$gen(300)

patchwork::wrap_plots(
  
  # Plot base data
  mod$plot(dat, theme = theme_light()) +
    ggtitle("A. Good residuals") + 
    xlab("Fitted values") + 
    ylab("Residuals"),
  
  # Include the z term
  # Reuse the x and e terms
  mod$plot(mod$
             set_prm("include_z", TRUE)$
             gen(300, computed = select(dat, x, e)), 
    theme = theme_light()) + 
    ggtitle("B. Non-linearity") +
    xlab("Fitted values") + 
    ylab("Residuals"),
  
  # Build a heter model
  # Reuse the x and e terms
  mod$plot(heter_model(b = 64)$
             gen(300, computed = select(dat, x, e)), 
    theme = theme_light()) + 
    ggtitle("C. Heteroskedasticity") +
    xlab("Fitted values") + 
    ylab("Residuals"),
  
  # Build a heter model with non-normal error
  # Reuse the x term
  mod$plot(heter_model(b = 0, e = rand_lognormal(sigma = 0.5))$
             gen(300, computed = select(dat, x)), 
    theme = theme_light()) + 
    ggtitle("D. Non-normality") +
    xlab("Fitted values") + 
    ylab("Residuals"),
  ncol = 4
)
```

Graphical summaries in which residuals are plotted against fitted values or other functions of the regressors that are approximately orthogonal to residuals are referred to as standard residual plots in @cook1982residuals. Figure \ref{fig:residual-plot-common-departures}A shows an ideal residual plot with residuals evenly distributed at both sides of the horizontal zero line, with no noticeable patterns. 

There are various types of departures from an ideal residual plot. Non-linearity, heterskedasticity and non-normality are perhaps the three mostly checked departures. 

Non-linearity is a type of model misspecification caused by failing to include higher order terms of the regressors in the regression equation. Any non-linear functional form of residuals on fitted values in the residual plot could be indicative of non-linearity. An example residual plot containing visual pattern of non-linearity is given at Figure \ref{fig:residual-plot-common-departures}B. One can clearly observe the "S-shape" from the residual plot as the cubic term is not captured by the misspecified model.

Heteroskedasticity refers to the presence of nonconstant error variance in a regression model. It is mostly due to the strict but false assumptions on the variance-covariance matrix of the error term. The usual pattern of heteroskedasticity on a residual plot is the inconsistent spread of the residuals across the horizontal axis. Visually, it sometimes results in the so-called "butterfly" shape as shown in the Figure \ref{fig:residual-plot-common-departures}C, or the "left-triangle" and "right-triangle" shape where the smallest variance occurs at one side of the horizontal axis.

Compared to non-linearity and heteroskedasticity, non-normality is usually harder to detect from a residual plot since a scatter plot do not readily reveal the marginal distribution. A favourable graphical summary for this task is the quantile-quantile plot. As we mainly discuss residual plots, non-normality will not be the focus of this paper. For a consistent comparison, the residual plot of this departure is still presented in Figure \ref{fig:residual-plot-common-departures}D. When the number residuals below and above the horizontal axis are uneven across the local regions along the $x$-axis, we expect that the normality assumption is violated. For example, given a skewed error distribution, there will be fewer data points and more outliers on one side of the horizontal axis as shown in Figure \ref{fig:residual-plot-common-departures}D.

## Conventionally testing for departures

Other than checking diagnostic plots, analysts may perform formal hypothesis testing for detecting model defects. Depending on the alternative hypothesis that is focused on, a variety of tests can be applied. For example, the presence of heteroskedasticity can usually be tested by applying the White test [@white_heteroskedasticity-consistent_1980] or the Breusch-Pagan test [@breusch_simple_1979], which are both derived from the Lagrange multiplier test [@silvey1959lagrangian] principle that relies on the asymptotic properties of the null distribution. For testing non-linearity, one may apply the F-test as a model structural test to examine the significance of specific polynomial and non-linear forms of the regressors, or the significance of proxy variables as in the Ramsey Regression Equation Specification Error Test (RESET) [@ramsey_tests_1969]. The Shapiro-Wilk test [@shapiro1965analysis] is the most widely used test of non-normality included by many of the statistical softwares. The Jarque--Bera test [@jarque1980efficient] is also used to directly checks if the sample skewness and kurtosis match a normal distribution.

Example residual plots given in Figure \ref{fig:residual-plot-common-departures} are examined by the corresponding RESET test, Breusch-Pagan test and Shapiro-Wilk test as shown in Table \ref{tab:example-residual-plot-table}. In the example, the Breusch-Pagan test and the Shapiro--Wilk test both reject the null hypothesis $H_0$ for departures that they do not intend to examine. As discussed in @cook1982residuals, most residual-based tests for a particular type of departure from model assumptions are also sensitive to other types of departures. It is likely $H_0$ is correctly rejected but for the wrong reason, a phenomenon known as the "Type III error". Additionally, outliers will often incorrectly trigger the rejection of $H_0$ despite when majority of the residuals are well-behaved [@cook_applied_1999]. Furthermore, with a sufficiently large sample size, residual-based tests may reject $H_0$ due to a slight departure that is of little practical significance. These can be largely avoided in diagnostic plots as experienced analysts can evaluate the acceptability of assumptions flexibly, even in the presence of outliers and slight departures. 


```{r}
set.seed(10086)

# Base model
mod <- poly_model(include_z = FALSE, sigma = 0.25)

# Base data
dat0 <- mod$gen(300)

# Replicate data in Figure 1
dat1 <- mod$set_prm("include_z", TRUE)$gen(300, computed = select(dat0, x, e))
dat2 <- heter_model(b = 64)$gen(300, computed = select(dat0, x, e))
dat3 <- heter_model(b = 0, e = rand_lognormal(sigma = 0.5))$
               gen(300, computed = select(dat0, x))

dat_list <- list(dat0, dat1, dat2, dat3)

table_dat <- data.frame(plot = c("A", "B", "C", "D"), 
                        model = c("None", 
                                  "Non-linearity", 
                                  "Heteroskedasticity", 
                                  "Non-normality"),
                        r = map_dbl(dat_list, 
                                    ~POLY_MODEL$test(.x, 
                                                     test = "RESET", 
                                                     power = 2:4)$p_value),
                        b = map_dbl(dat_list, 
                                    ~HETER_MODEL$test(.x)$p_value),
                        
                        # SW test has not been included by visage
                        s = map_dbl(dat_list, 
                                    ~shapiro.test(.x$.resid)$p.value)) %>%
  
  # 3 decimal points
  mutate(across(r:s, ~ format(round(.x, digits = 3), nsmall = 3))) %>%
  
  # Italic if reject
  mutate(across(r:s, ~ kableExtra::cell_spec(.x, italic = as.numeric(.x) < 0.05)))

  table_dat %>%
  knitr::kable(col.names = c("Plot", 
                             "Departures", 
                             "RESET", 
                             "Breusch-Pagan", 
                             "Shapiro–Wilk"),
               align = "llrrr",
               escape = FALSE,
               format = "latex",
               booktabs = TRUE, 
               caption  = 'Statistical significance testing for departures from good residuals for plots in Figure \\ref{fig:residual-plot-common-departures}. Shown are the $p$-values calculated for the RESET, the Breusch-Pagan and the Shapiro–Wilk tests. The good residual plot (A) is judged a good residual plot, as expected, by all tests. The non-linearity (B) is detected by all tests, as might be expected given the extreme structure.', 
               label = "example-residual-plot-table" ) 
```


## Visual test procedure based on lineups

### Lineup protocol

One may argue that reading diagnostic plots is to some extent subjective and indecisive compared to those rigorous statistical procedures as it relies on graphical perception - human ability to interpret and decode the information embedded in graph [@cleveland_graphical_1984]. Further, the degree of the presence of the visual features typically can not be measured quantitatively and objectively, which may lead to over or under-interpretations of the data. For instance, people 
over-interpret the separation between gene groups in a two-dimensional projection from a linear discriminant analysis when in fact there are no differences in the expression levels between the gene groups and separation is not an uncommon occurrence [@roy_chowdhury_using_2015].

Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualization" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots. Later, @buja_statistical_2009 proposed the lineup protocol as a visual test inspired by the "police lineup" or "identity parade" which is the act of asking the eyewitness to identify criminal suspect from a group of irrelevant people. The protocol consists of $m$ randomly placed plots, where one plot is the data plot, and the remaining $m - 1$ null plots have the identical graphical procedure except the data has been replaced with data consistent with $H_0$. Then, an observer who have not seen the data plot will be asked to point out the most different plot from the lineup. Under $H_0$, it is expected that the data plot would have no distinguishable difference from the null plots, and the probability that the observer correctly picks the data plot is $1/m$. If one rejects $H_0$ as the observer correctly picks the data plot, then the Type I error of this test is $1/m$.

```{r first-example-lineup, fig.height = 7, fig.cap="Visual testing is conducted using a lineup, as in the example here. The residual plot computed from the observed data (plot $2^2 + 2$, exhibiting non-linearity) is embedded among 19 null plots, where the residuals are simulated from a standard error model. Computing the $p$-value requires that the lineup be examined by a number of human judges, each asked to select the most different plot. A small $p$-value would result from a substantial number selecting plot $2^2 + 2$."}

# a bit stronger
# plot 7 -> plot 2^3 - 1


# vi_survey %>%
#   filter(!attention_check) %>%
#   filter(!null_lineup) %>%
#   filter(type == "polynomial") %>%
#   filter(x_dist == "uniform") %>%
#   filter(n == 300) %>%
#   filter(shape == 1) %>%
#   group_by(unique_lineup_id) %>%
#   summarise(effect_size = first(effect_size)) %>%
#   arrange(effect_size)

vi_lineup$poly_300$data %>%
  VI_MODEL$plot_lineup(theme = theme_light(), remove_axis = TRUE, remove_grid_line = TRUE)
```

Figure \ref{fig:first-example-lineup} is an example of a lineup protocol. If the data plot at position $2^2 + 2$ is identifiable, then it is evidence for the rejection of $H_0$ that the regression model is correctly specified. In fact, the actual residual plot is obtained from a misspecified regression model with non-linearity defect. 

The effectiveness of lineup protocol for regression analysis is validated by @majumder_validation_2013 under relatively simple settings with up to two regressors. Their results suggest that visual tests are capable of testing the significance of a single regressor with a similar power as a t-test, though they express that in general it is unnecessary to use visual inference if there exists a conventional test, and they do not expect the visual test to perform equally well as the conventional test. (contradict to our claim? expand?) In their third experiment, where there is not a conventional test, visual test outperforms the conventional test for a large margin. This is encouraging, as it promotes the use of visual inference in situations where there are no existing statistical testing procedures. Visual inference have also been integrated into diagnostic of hierarchical linear models by @loy2013diagnostic, @loy2014hlmdiag and @loy2015you. They use lineup protocols to judge the assumption of linearity, normality and constant error variance for both the level-1 and level-2 residuals. (expand?)

### Sampling from the null distribution

Data used in the $m - 1$ null plots needs to be simulated. In regression diagnostics, sampling data consistent with $H_0$ is equivalent to sampling data from the assumed model. As @buja_statistical_2009 suggested, $H_0$ is usually a composite hypothesis controlled by nuisance parameters. Since regression models can have various forms, there is no general solution to this problem, but it sometimes can be reduced to a so called "reference distribution" by applying one of the three methods: (i) sampling from a conditional distribution given a minimal sufficient statistic under $H_0$, (ii) parametric bootstrap sampling with nuisance parameters estimated under $H_0$, and (iii) Bayesian posterior predictive sampling. The conditional distribution given a minimal sufficient statistic is the best justified reference distribution among the three [@buja_statistical_2009]. Essentially, null residuals can be simulated by regressing $N$ i.i.d standard normal random draws on the regressors, then rescaling it by the ratio of residual sum of square in two regressions.

### Calculating $p$-values for the visual test

In hypothesis testing, a $p$-value is defined as the probability of observing test results as least as extreme as the observed result given $H_0$ is true. Within the context of visual inference, by involving $k$ independent observers, the $p$-value can be interpreted as the probability of having as many or more subjects detect the data plot than the observed result.

Let $X_j = \{0,1\}$ be a Bernoulli random variable denoting whether subject $j$ correctly detecting the data plot, and $X = \sum_{j=1}^{K}X_j$ be the number of observers correctly picking the data plot. Then, by imposing a relatively strong assumption on the visual test that all $K$ evaluations are fully independent, under $H_0$, $X \sim \mathrm{Binom}_{K,1/m}$. Therefore, the $p$-value of a lineup of size $m$ evaluated by $K$ observer is given as $P(X \geq x) = 1 - F(x) + f(x)$, where $F(.)$ is the cumulative distribution function, $f(.)$ is the probability mass function and $x$ is the realization of number of observers correctly picking the data plot [@majumder_validation_2013].

As pointed out by @vanderplas2021statistical, this basic binomial model doesn't take into account the possible dependencies in the visual test due to repeated evaluations of the same lineup. And it is inapplicable to visual test where subjects are asked to select one or more "most different" plots from the lineup. @vanderplas2021statistical summarises three common scenarios in visual inference: (1) $K$ different lineups are shown to $K$ subjects, (2) $K$ lineups with different null plots but the same data plot are shown to $K$ subjects, and (3) the same lineup is shown to $K$ subjects. Out of these three scenarios, Scenario 3 is the most common in previous studies as it puts the least constraints on the experiment design. For Scenario 3, @vanderplas2021statistical models the probability of a plot $i$ being selected from a lineup as $\theta_i$, where $\theta_i \sim Dirichlet(\alpha)$ for $i=1,...,m$ and $\alpha > 0$. The number of times plot $i$ being selected in $K$ evaluations is denoted as $c_i$. In case subject $j$ makes multiple selections, $1/s_j$ will be added to $c_i$ instead of one, where $s_j$ is the number of plots subject $j$ selected for $j=1,...K$. This ensures $\sum_{i}c_i=K$. Since we are only interested in the selections of the data plot $i$, the marginal model can be simplified to a beta-binomial model and thus the visual p-value is given as

<!-- TODO:Clarify this is only for integer c_i -->

```{=tex}
\begin{equation} \label{eq:pvalue-beta-binomial}
P(C \geq c_i) = \sum_{x=c_i}^{K}{K \choose x}\frac{B(x + \alpha, K - x + (m - 1)\alpha)}{B(\alpha, (m-1)\alpha)},\quad \text{for} \quad c_i \in \mathbb{Z}_0^+
\end{equation}
```

\noindent where $B(.)$ is the beta function defined as

```{=tex}
\begin{equation} \label{eq:betafunction}
B(a, b) = \int_{0}^{1}t^{\alpha - 1}(1-t)^{b-1}dt,\quad \text{where}\quad a,b>0. 
\end{equation}
```

Note that Equation \ref{eq:pvalue-beta-binomial} given in @vanderplas2021statistical only works with non-negative integer $c_i$. We extend the equation to non-negative real number $c_i$ by applying a linear approximation

```{=tex}
\begin{equation} \label{eq:pvalue-beta-binomial-approx}
P(C \geq c_i) = P(C \geq \lceil c_i \rceil) + (\lceil c_i \rceil - c_i) P(C = \lfloor c_i \rfloor), \quad \text{for}\quad c_i \in \mathbb{R}_0^+,
\end{equation}
```

where $P(C \geq \lceil c_i \rceil)$ is calculated using Equation \ref{eq:pvalue-beta-binomial} and $P(C = \lfloor c_i \rfloor)$ is calculated by

\begin{equation} \label{eq:pmf-beta-binomial}
P(C = c_i) = {K \choose c_i}\frac{B(c_i + \alpha, K - c_i + (m - 1)\alpha)}{B(\alpha, (m-1)\alpha)},\quad \text{for} \quad c_i \in \mathbb{Z}_0^+.
\end{equation}


Besides, the parameter $\alpha$ used in Equation \ref{eq:pvalue-beta-binomial} and \ref{eq:pmf-beta-binomial} is usually unknown and hence needs to be estimated from the survey data. For low values of $\alpha$, only a few plots are attractive to the observers and tend to be selected. For higher values of $\alpha$, the distribution of the probability of each plot being selected is more even. @vanderplas2021statistical defines that a plot is $c$-interesting if $c$ or more participants select the plot as the most different. Given the definition, The expected number of plots selected at least $c$ times, $E[Z_c]$, is calculated as

```{=tex}
\begin{equation} \label{eq:c-interesting-expectation}
E[Z_c(\alpha)] = \frac{m}{B(\alpha, (m-1)\alpha)}\sum_{\lceil c \rceil}^{K}{K \choose x} B(x + \alpha, K - x + (m-1)\alpha).\end{equation}
```

With Equation \ref{eq:c-interesting-expectation}, $\alpha$ can be estimated using maximum likelihood estimation. But for precise estimate of $\alpha$, additional responses to Rorschach lineups, which is a type of lineup that consists of plots constructed from the same null data generating mechanism, are required.

### Power of a visual test

The power of a model misspecification test is the probability that the null hypothesis is rejected given the regression model is misspecified. It is an important indicator when one is concerned about whether model assumptions have been violated. Although in practice, one might be more interested in knowing how much the residuals deviate from the model assumptions, and whether this deviation is of practical significance.

As discussed in @majumder_validation_2013, the power of a visual test may depend on the ability of the particular subject, as the skill of the individual may affect the number of observers who identify the data plot from the lineup. Previously, it is addressed by modelling the probability of a subject $j$ correctly picking the data plot from a lineup $l$ using a mixed-effect logistic regression, with subjects treated as random effects [@majumder_validation_2013]. However, in the multiple selections scenario, having this probability is not sufficient to determine the power of a visual test because it does not provide information about the number of selections made by the subject for the calculation of the p-value. 

Instead, we directly estimate the probability of a lineup being rejected by assuming that individual skill has negligible effect on the variation of the power. The assumption is to simplify the model structure, thereby obviate costly large-scale experiments to estimate complex covariance matrices. The model is a logistic regression with the natural logarithm of the effect as the only regressor formulated as

```{=tex}
\begin{equation} \label{eq:logistic-regression-1-1}
Pr(\text{reject}~H_0|H_1,E) = \Lambda(\beta_0 + \beta_1 log(E)),
\end{equation}
```
\noindent where $\Lambda(.)$ is the standard logistic function given as $\Lambda(z) = exp(z)/(1+exp(z))$.

```{r eval = FALSE}
mix_effect_dat <- vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  filter(type == "polynomial") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(subject = factor(as.integer(factor(paste(exp, set))))) %>%
  mutate(reject = p_value < 0.05) %>%
  select(log_effect_size, reject, subject)

# Singularity issue (random effect too small? not worth to fit a mix-effect model?)
mix_mod <- lme4::glmer(reject ~ scale(log_effect_size) + (1 | subject), data = mix_effect_dat, family = binomial()) 

pred_dat <- map_df(seq(-1, 6, 0.1),
    function(x) {
      result <- data.frame(log_effect_size = rep(x, length(unique(mix_effect_dat$subject))))
      pred <- predict(mix_mod, 
                      type = "response", 
                      newdata = data.frame(log_effect_size = x, 
                                           subject = unique(mix_effect_dat$subject)))
      result$subject <- names(pred)
      result$pred <- pred
      result
    })

pred_dat %>%
  ggplot() +
  geom_line(aes(log_effect_size, pred, group = subject))
```


```{r eval = FALSE}
MASS::glmmPQL(weighted_detect ~ scale(log_effect_size), random = ~ scale(log_effect_size) | subject,  family = quasibinomial(), data = vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  filter(type == "polynomial") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(subject = factor(as.integer(factor(paste(exp, set))))) %>%
  select(log_effect_size, weighted_detect, subject)) -> mod

mod_dat <- vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  filter(type == "polynomial") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  # mutate(log_effect_size = scale(log_effect_size)) %>%
  mutate(subject = factor(as.integer(factor(paste(exp, set))))) %>%
  select(log_effect_size, weighted_detect, subject)

mod <- lme4::glmer(weighted_detect ~ scale(log_effect_size)  + (scale(log_effect_size) | subject), 
                   family = quasibinomial(),
                   data = mod_dat)

mod_dat$log_effect_size

map(seq(-1, 6, 0.1), ~predict(mod, newdata = data.frame(subject = 1:283, log_effect_size = .x), type = "response")) %>%
  `names<-`()


vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform") %>%
  filter(type == "polynomial") %>%
  mutate(log_effect_size = log10(effect_size)) %>%
  mutate(subject = factor(as.integer(factor(paste(exp, set))))) %>%
  mutate(pred_wd = predict(mod, type = "response")) %>%
  group_by(unique_lineup_id) %>%
  mutate(pred_ci = pred_wd * n()) %>%
  mutate(n_eval = n()) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(pred_pval = (function(c_i, n_eval, alpha) {
  target_dist <- exact_dist(n_eval, 20, dist = "dirichlet", alpha = alpha)
  floor_c_i <- floor(c_i)
  ceil_c_i <- ceiling(c_i)
  return(unname(sum(target_dist[(ceil_c_i:n_eval) + 1]) + (ceil_c_i - c_i) * target_dist[floor_c_i + 1]))
})(pred_ci, n_eval, alpha)) %>%
  ungroup() %>%
  mutate(pred_reject = pred_pval < 0.05) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, as.numeric(pred_reject)), method = "glm", method.args = list(family = binomial()), se = FALSE)


 
```


Effect $E$ is derived from the Kullback-Leibler divergence (see [appendix ref here]) formulated as

\small

\begin{gather}  \label{eq:effect-size}
E = \frac{1}{2}\left(log\frac{|diag(\boldsymbol{R}_a\boldsymbol{V})|}{|diag(\boldsymbol{R}_a\sigma^2)|} - n + tr(diag(\boldsymbol{R}_a\boldsymbol{V})^{-1}diag(\boldsymbol{R}_a\sigma^2)) + \boldsymbol{\mu}_z^{T}(\boldsymbol{R}_a\boldsymbol{V})^{-1}\boldsymbol{\mu}_z\right),\\
\boldsymbol{R}_a = \boldsymbol{I}_n - \boldsymbol{X}_a(\boldsymbol{X}_a^T\boldsymbol{X}_a)^{-1}\boldsymbol{X}_a^T,
\end{gather}

\normalsize

\noindent where $diag(.)$ is the diagonal matrix constructed from the diagonal elements of a matrix,  $\boldsymbol{X}_a = (\boldsymbol{1},  \boldsymbol{X})$ is the matrix of regressors including the intercept used in the regression equation, $\sigma^2$ is the assumed variance of the error term when $H_0$ is true, $\boldsymbol{V}$ is the actual variance of the error term, and $\boldsymbol{\mu}_z = \boldsymbol{R}_a\boldsymbol{Z}\boldsymbol{\beta}_z$ is the expected values of the residuals with $\boldsymbol{Z}$ be any variables leave out by the model and $\boldsymbol{\beta}_z$ be the corresponding coefficients.

To study various factors contributing to the power of the visual test, the same logistic regression model is fit on different subsets of the collated data grouped by levels of factors. These include the distribution of the regressor and the type of the simulated model.


```{r residual-plot-cubic-heter, fig.cap='Residuals vs. fitted values plot for a classical normal linear regression model. The residuals are produced by fitting a two-regressor multiple linear regression model with data generated from a cubic linear model. From the residual plot, "butterfly shape" can be observed which generally would be interpretd as evidence of heteroskedasticity. Further, from the outline of the shape, nonlinear patterns exist. Both visual discoveries are evidence against $H_0$, though heteroskedasticity actually does not exist in the data generating process. \\label{fig:residual-plot-cubic-heter}', eval = FALSE}

cm <- cubic_model(a = 0, b = 100, c = 1)

cm$gen(1000, fit_model = TRUE) %>%
cm$plot() +
  theme_light()
```


# Experimental design

Three experiments are conducted to investigate the difference between conventional hypothesis testing and visual inference in the application of linear regression diagnostics. The experiment I has ideal scenario for conventional testing, where the visual test is not expected to outperform the conventional test. Meanwhile, the experiment II is a scenario where the conventional test is an approximate test, in which the visual test may have a chance to match the performance of the conventional test. The experiment III is designed for collecting human responses to null lineups such that the parameter $\alpha$ in Equation \ref{eq:pvalue-beta-binomial} can be estimated. Overall, we plan to collect 7974 evaluations on 1152 uniqued lineups performed by 443 subjects throughout three experiments. 

## Simulating departures from good residuals

Two types of departures, namely non-linearity and heteroskedasticity, are considered with the corresponding data generating process being designed for experiment I and II. 

### Non-linearity


<!-- (handbook of mathathetical function, https://aapt.scitation.org/doi/10.1119/1.15378, cite the package) -->

<!-- (hermite poly is created by ... in 18..,) -->

Experiment I is designed to study the ability of human subjects to detect the effect of a non-linear term $\boldsymbol{z}$ constructed using Hermite polynomials on random vector $\boldsymbol{x}$ formulated as

\begin{align} \label{eq:nonlinearity-model}
\boldsymbol{y} &= 1 + \boldsymbol{x} + \boldsymbol{z} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} &= g(\boldsymbol{x}_{raw}, 1), \\
\boldsymbol{z} &= g(\boldsymbol{z}_{raw}, 1), \\
\boldsymbol{z}_{raw} &= He_j(g(\boldsymbol{x}, 2)),
\end{align}

\noindent where $\boldsymbol{y}$, $\boldsymbol{x}$, $\boldsymbol{\varepsilon}$, $\boldsymbol{x}_{raw}$, $\boldsymbol{z}_{raw}$ are vectors of size $n$, $He_{j}(.)$ is the $j$th-order probabilist's Hermite polynomials, $\varepsilon \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)$, and $g(\boldsymbol{x}, k)$ is a scaling function to enforce the support of the random vector to be $[-k, k]^n$ defined as

```{=tex}
\begin{equation} \label{eq:scaling-function}
g(\boldsymbol{x}, k) = (\boldsymbol{x} - min(\boldsymbol{x}))/max(\boldsymbol{x} - min(\boldsymbol{x}))2k - k, \quad \text{for} \quad k > 0. 
\end{equation}
```

According to @abramowitz1964handbook, Hermite polynomials were initially defined by @de1820theorie, but named after Hermite [@hermite1864nouveau] because of the unrecognisable form of Laplace's work. When simulating $\boldsymbol{z}_{raw}$, function `hermite` from the R package `mpoly` [@mpoly] is used to generate Hermite polynomials. 

The null regression model used to fit the realizations generated by the above model is formulated as

```{=tex}
\begin{equation} \label{eq:null-model}
\boldsymbol{y} = \beta_0 + \beta_1 \boldsymbol{x} + \boldsymbol{u},
\end{equation}
```
\noindent where $\boldsymbol{u} \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I}_n)$.

Since $z = O(x^j)$, for $j > 1$, $z$ is a higher order term leaves out by the null regression, which will lead to model misspecification. 

Visual patterns of non-linearity are simulated using four different orders of probabilist's Hermite polynomials ($j = 2, 3, 6, 18$) and four different distributions of $X_{raw}$: (1) $U(-1, 1)$, (2) $N(0, 0.3^2)$, (3) $lognormal(0, 0.6^2)/3$ and (4) $u\{1, 5\}$. A summary of the factors is given in Table \ref{tab:model-factor-table}.


```{r results='asis'}
data.frame(j = c("2", "3", "6", "18", ""),
           x_dist = c("$U(-1, 1)$", "$N(0, 0.3^2)$", "$lognormal(0, 0.6^2)/3$", "$U\\{1, 5\\}$", ""),
           sigma = c("0.25", "1.00", "2.00", "4.00", ""),
           a = c("-1", "0", "1", "", ""),
           b = c("0.25", "1.00", "4.00", "16.00", "64.00"),
           n = c("50", "100", "300", "", "")) %>%
  knitr::kable(col.names = NULL,
               align = "rcrrrr",
               escape = FALSE,
               format = "latex",
               booktabs = TRUE, 
               caption  = 'Description of all factors involved in the non-linear and heteroskedasticity studies.', 
               label = "model-factor-table" ) %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::add_header_above(c("Poly Order (sym1)", #"Order of Hermite polynomial\n(sym1)", 
                                 "Distribution of sym2\n", 
                                 "SD (sym3)", #"Standard deviation of polynomial model\n(sym3)",
                                 "Heteroskedasticity Shape (sym4)", # "Shape of heteroskedasticity\n(sym4)", 
                                 "Heteroskedasticity (sym5)", # Variance factor of heteroskedasticity model\n(sym5)",
                                 "Size (sym6)")) %>% #"Sample size\n(sym6)")) %>%
  as.character() %>%
  gsub("sym1", "$j$", .) %>%
  gsub("sym2", "$X_{raw}$", .) %>%
  gsub("sym3", "$\\\\sigma$", .) %>%
  gsub("sym4", "$a$", .) %>%
  gsub("sym5", "$b$", .) %>%
  gsub("sym6", "$n$", .) %>%
  cat()
  # gsub("\\\\\\$", "$", .) %>%
  # cat()
```

The values of $j$ is chosen so that distinct shapes of non-linearity are included in the residual plot. These include "U", "S", "M" and "Triple-U" shape as shown in Figure \ref{fig:different-shape-of-herimite}. A greater value of $j$ will result in a curve with more turning points. It is expected that the "U" shape will be the easiest one to detect because complex shape tends to be concealed by cluster of data points.

Different distributions of $X_{raw}$ help enriching the pool of visual patterns as illustrated in Figure \ref{fig:different-dist}. The uniform and the normal distribution are symmetric and commonly assumed in statistical models. The adjusted log-normal distribution provides skewed density, while the discrete uniform distribution provides discreteness in residual plot.

```{r different-shape-of-herimite, fig.height = 2, fig.cap="Polynomial forms generated for the residual plots used in experiment I. The four shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300)

pp1 <- VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(He[2]:"U")) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

pp2 <- VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[3]:"S")) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

pp3 <- VI_MODEL$plot(poly_model(shape = 3, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[6]:"M")) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

pp4 <- VI_MODEL$plot(poly_model(shape = 4, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.05)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote(He[18]:"Triple-U")) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(pp1, pp2, pp3, pp4, ncol = 4)
```

```{r different-dist, fig.height = 2, fig.cap="Variations in fitted values ($X_{raw}$), that might affect perception of residual plots. Four different distribution are used."}
ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300)

dp1 <- VI_MODEL$plot(ori_dat) + 
  ggtitle("A. Uniform") + #quote(U(-1,1))) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

dp2 <- VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_normal(sigma = 0.3); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle("B. Normal") + #quote(N(0,0.3^2))) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

dp3 <- VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_lognormal(sigma = 0.6); closed_form(~stand_dist(raw_x/3 - 1))}, sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle("C. Skewed") + #quote(lognormal(0,0.6^2)/3)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

dp4 <- VI_MODEL$plot(poly_model(shape = 1, x = rand_uniform_d(k = 5, even = TRUE), sigma = 0.5)$gen(300, computed = select(ori_dat, e))) +
  ggtitle("D. Discrete") + #quote(U~"{1,5}")) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(dp1, dp2, dp3, dp4, ncol = 4)
```

Figure \ref{fig:example-poly-lineup} demonstrates one of the lineups used in experiment I. This lineup is produced by the non-linearity model under $j = 6$ and $X_{raw} \sim N(0.0.3^2)$. The data plot location is $2^3 - 4$. All five subjects correctly identify the data plot from this lineup.

```{r different-n, fig.height = 2.67, fig.cap="Three different values of $n$ are used in experiment I, II and III to control the strength of the signal."}
ori_dat <- poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(300)

np1 <- VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(n~"="~300)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

np2 <- VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(50, computed = select(ori_dat[1:50,], x, e))) +
  ggtitle(quote(n~"="~50)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

np3 <- VI_MODEL$plot(poly_model(shape = 2, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1.5)$gen(50, computed = select(ori_dat[1:100,], x, e))) +
  ggtitle(quote(n~"="~100)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(np2, np3, np1, ncol = 3)
```


```{r different-sigma, fig.height = 2, fig.cap="Four different values of $\\sigma$ are used in the experiment I to control the strength of the signal."}
ori_dat <- poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 0.5)$gen(300)

sp1 <- VI_MODEL$plot(ori_dat) + 
  ggtitle(quote(sigma~"="~0.5)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

sp2 <- VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 1)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~1)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

sp3 <- VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 2)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~2)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

sp4 <- VI_MODEL$plot(poly_model(shape = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, sigma = 4)$gen(300, computed = select(ori_dat, x))) +
  ggtitle(quote(sigma~"="~4)) + 
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(sp1, sp2, sp3, sp4, ncol = 4)
```

```{r example-poly-lineup, fig.height=7, fig.cap="Lineup poly-24 in experiment I. Can you spot the most different plot? \\label{fig:example-poly-lineup}"}
VI_MODEL$plot_lineup(vi_lineup[["poly_24"]]$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```

### Heteroskedasticity

Experiment II is designed to study the ability of human subjects to detect the appearance of a heteroskedasticity pattern under a simple linear regression model setting:

```{=tex}
\begin{align} \label{eq:heter-model}
\boldsymbol{y} &= 1 + \boldsymbol{x} + \boldsymbol{\varepsilon},\\
\boldsymbol{x} &= g(\boldsymbol{x}_{raw}, 1),\\
\boldsymbol{\varepsilon} &\sim N(\boldsymbol{0}, 1 + (2 - |a|)(\boldsymbol{x} - a)^2b \boldsymbol{I}), 
\end{align}
```

\noindent where $\boldsymbol{y}$, $\boldsymbol{x}$, $\boldsymbol{\varepsilon}$ are vectors of size $n$ and $g(.)$ is the scaling function defined in Equation \ref{eq:scaling-function}.

The null regression model used to fit the realizations generated by the above model is formulated exactly the same as Equation \ref{eq:null-model}.

For $b \neq 0$, the variance-covariance matrix of the error term $\boldsymbol{\varepsilon}$ is correlated with the regressor $\boldsymbol{x}$, which will lead to the presence of heteroskedasticity. Visual patterns of heteroskedasticity are simulated using three different shapes ($a$ = -1, 0, 1) and the same four different distribution of $X_{raw}$ used in experiment I. A summary of the factors can also be found in Table \ref{tab:model-factor-table}.

Since $supp(X) = [-1, 1]$, choosing $a$ to be $-1$, $0$ and $1$ can generate "left-triangle", "butterfly" and "right-triangle" shape as displayed in Figure \ref{fig:different-shape-of-heter}. The term $(2 - |a|)$ maintains the magnitude of residuals across different values of $a$.

```{r different-shape-of-heter, fig.height=2.67, fig.cap="Heteroskedasticity forms used in experiment II. Three different shapes (a = -1, 0, 1) are used in the experiment to create left-triangle, butterfly and right-triangle shapes."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- heter_model(a = -1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300)

hp1 <- VI_MODEL$plot(ori_dat) +
  ggtitle(quote("A. a = -1: Left-triangle")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

hp2 <- VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("B. a = 0: Butterfly")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

hp3 <- VI_MODEL$plot(heter_model(a = 1, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 128)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("C. a = 1: Right-triangle")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(hp1, hp2, hp3, ncol = 3)
```


```{r different-b, fig.height = 2, fig.cap="Five different values of $b$ are used in experiment II to control the strength of the signal."}
stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

ori_dat <- heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 0.25)$gen(300)

bp1 <- VI_MODEL$plot(ori_dat) +
  ggtitle(quote("b = 0.25")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

bp2 <- VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 1)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 1")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

bp3 <- VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 4)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 4")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

bp4 <- VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 16)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 16")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

bp5 <- VI_MODEL$plot(heter_model(a = 0, x = {raw_x <- rand_uniform(-1, 1); closed_form(~stand_dist(raw_x))}, b = 64)$gen(300, computed = select(ori_dat, x, e))) +
  ggtitle(quote("b = 64")) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme(axis.text = element_blank())

patchwork::wrap_plots(bp1, bp2, bp3, bp4, bp5, ncol = 5)
```


```{r example-heter-lineup, fig.height = 7, fig.cap="Lineup heter-169 in experiment II. Can you spot the most different plot? \\label{fig:example-heter-lineup}"}

# vi_survey %>%
#   filter(!attention_check) %>%
#   filter(!null_lineup) %>%
#   filter(type == "heteroskedasticity") %>%
#   filter(x_dist == "uniform") %>%
#   filter(a == -1) %>%
#   filter(n == 300) %>%
#   group_by(unique_lineup_id) %>%
#   summarise(effect_size = first(effect_size)) %>%
#   arrange(effect_size)

# vi_survey %>%
#   filter(unique_lineup_id == "heter_471") %>%
#   View()

VI_MODEL$plot_lineup(vi_lineup$heter_471$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```


An example lineup of this model used in Experiment II is shown in Figure \ref{fig:example-heter-lineup} with $a = -1$ and $X_{raw} \sim U(-1, 1)$. The data plot location is $2^4 + 2$. Nine out of 11 subjects correctly identify the data plot from this lineup. 

## Experimental setup

### Controlling the strength of the signal

As summarised in Table \ref{tab:model-factor-table}, three additional parameters $n$, $\sigma$ and $b$ are used to control the strength of the signal so that different difficulty levels of lineups are generated, and therefore, the estimated power curve will be smooth and continuous. Parameter $\sigma \in \{0.5, 1, 2, 4\}$ and $b \in \{0.25, 1, 4, 16, 64\}$ are used in experiment I and II respectively. Figure \ref{fig:different-sigma} and \ref{fig:different-b} demonstrate the impact of these two parameters. A large value of $\sigma$ will increase the variation of the error of the non-linearity model and decrease the visibility of the visual pattern. The parameter $b$ controls the standard deviation of the error across the support of the regressor. Given $x \neq a$, a larger value of $b$ will lead to a larger ratio of the variance at $x$ to the variance at $x - a = 0$, making the visual pattern more obvious.

Three different sample sizes are used (n = 50, 100, 300) in all three experiments. It can be observed from Figure \ref{fig:different-n} that with fewer data points drawn in a residual plot, the visual pattern is more difficult to be detected.



### Subject allocation

Three replications are made for each of the parameter values shown in Table \ref{tab:model-factor-table} resulting in $(4 \times 4 \times 4 \times 3 + 4 \times 3 \times 5 \times 3) \times 3 = 1116$ different lineups. In addition, each lineup is designed to be evaluated by five different subjects. After attempting some pilot studies internally in our research group, we decide to present a block of 20 lineups to every subject. And to ensure the quality of the survey data, two lineups with obvious visual patterns are included as attention checks. Thus, $576 \times 5 / (20-2) = 160$ and $540 \times 5 / (20-2) = 150$ subjects are recruited to satisfy the design of the experiment I and experiment II respectively.

As mentioned in Section \ref{calculating-p-values-for-the-visual-test}, $\alpha$ used in Equation \ref{eq:pvalue-beta-binomial} needs to be estimated using \svp{null} lineups. Hence, 36 lineups with all combinations of $n$ and $X_{raw}$ and three replications are included in experiment III. In these lineups, the data of the data plot is generated from a model \svp{with zero effect size}, while the data of the 19 null plots are generated using the same simulation method discussed in Section \ref{sampling-from-the-null-distribution}. \svp{This generation procedure differs from the canonical Rorschach lineup procedure, which requires that all 20 plots are generated from the null hypothesis. However, these lineups serve the same fundamental purpose: to assess the number of visually interesting plots generated under the null hypothesis.}
<!-- Technically, this was an alternative to Rorschach lineup but with the null distribution further conditioning on the estimated coefficients and the residual sum of squares.  -->
\svp{To account for the fact that our simulation method for these lineups is not the Rorschach procedure,} we use the method suggested in @vanderplas2021statistical for typical lineups containing a data plot to estimate $\alpha$. 
We have included a sensitivity analysis in the Appendix to examine the impact of the variance of the $\alpha$ estimate on our findings.

All lineups consist of only null plots are planned to be evaluated by 20 subjects. 
However, presenting only these lineups to subjects are considered to be bad practices as subjects will lose interest quickly. 
Therefore, we plan to collect 6 more evaluations on the 279 lineups with $X_{raw} \sim U(-1,1)$, result in $(36 \times 20 + (4 \times 4 \times 3 + 3 \times 5 \times 3) \times 3) / (20-2) = 133$ subjects recruited for experiment III.

### Collecting results

Subjects for all three experiments are recruited from an crowdsourcing platform called Prolific [@palan2018prolific]. Prescreening procedure is applied during the recruitment, subjects are required to be fluent in English, with $98\%$ minimum approval rate and 10 minimum submissions in other studies. 

During the experiment, every subject is presented with a block of 20 lineups. A lineup consists of a randomly placed data plot and 19 null plots, which are all residual plots drawn with raw residuals on the y-axis and fitted values on the x-axis. An additional horizontal red line is added at $y = 0$ as a helping line.

The data of the data plot is simulated from one of two models described in Section \ref{simulating-departures-from-good-residuals}, while the data of the remaining 19 null plots are generated by the residual rotation technique discussed in Section \ref{sampling-from-the-null-distribution}.

In every lineup evaluation, the subject is asked to select one or more plots that are most different from others, provide a reason for their selections, and evaluate how different they think the selected plots are from others. If there is no noticeable difference between plots in a lineup, subjects are permitted to select zero plots without providing the reason. No subject are shown the same lineup twice. Information about preferred pronoun, age group, education, and previous experience in visual experiment are also collected. A subject's submission is only accepted if the data plot is identified for at least one attention check. Data of rejected submissions are discarded automatically to maintain the overall data quality.

# Results 

## Overview

<!-- How many people? How many lineups? -->

There are 2880, 2700 and 2394 lineups evaluation made by 160, 150 and 133 subjects recruited for experiment I, II and III respectively. In the total of 7974 lineup evaluations, 3744 use lineups produced by the non-linearity model, 3510 use lineups produced by the heteroskedasticity model, and 720 use null lineups. Besides, there are 886 attention checks not included in the following analysis. The collated dataset is provided in `vi_survey` of the `visage` `R` package. 

In the following analysis, lineups with $X_{raw} \sim U(-1,1)$ will be the focus of next two sections, because visual patterns are more likely to be revealed with uniform distribution. Besides, six more evaluations are collected for the uniform distribution in experiment III, which will produce more reliable and stable results. Analysis of other distributions can be found in Section \ref{effect-of-the-distribution-of-the-regressor}.

## Power comparison of different tests 

Figure \ref{fig:polypower} shows the estimated power of visual test on lineups produced by the non-linearity model with $X_{raw} \sim U(-1,1)$, against the natural logarithm of the effect $log_e(E)$, with a 5% significance level. At the bottom of the figure \ref{fig:polypower}, there are a sequence of example residual plots with increasing levels of $log_e(E)$. Readers can evaluate them from left to right and determine at which level the departure from a good residual plot becomes detectable.

As discussed in Section \ref{conventionally-testing-for-departures}, many conventionally tests are available for detecting residual departures. Implementation-wise, the built-in R package `stats` provides some commonly used residual-based tests, such as Shapiro-Wilk test. A more comprehensive collection of regression diagnostics tests can be found in the R package `lmtest` [@lmtest]. In terms of heteroskedasticity diagnostics, the R package `skedastic` [@skedastic] collects and implements 25 existing conventional tests published since 1961.

To compare the power of visual test and conventional test, we pick RESET test (`resettest`) and Breusch-Pagan test (`bptest`) from the R package `lmtest`, and Shapiro-Wilk test (`shapiro.test`) from the built-in R package `stats`. Among them, RESET test is the only exact and appropriate test in this scenario. Both the Breusch-Pagan test and the Shapiro-Wilk test are approximate and inappropriate tests. Their estimated power is shown in Figure \ref{fig:polypower}. To set up the RESET test, we include different powers of fitted values as proxies. According to @ramsey_tests_1969, there are no general rules for the power of the fitted values needed by the RESET test, but it finds power up to four is usually sufficient. Thus, we follow this guideline to conduct the RESET test. For the Breusch-Pagan test, the choice of regressors in the auxiliary regression is left to the user [@breusch_simple_1979]. But as @waldman1983note suggested, it is a good choice for the set of auxiliary regressors in the Breusch-Pagan test be the same as the White test. Thus, we include both $\boldsymbol{x}$ and $\boldsymbol{x}^2$ in the auxiliary regression.

Figure \ref{fig:heterpower} is similar to Figure \ref{fig:polypower}, but shows corresponding information on lineups produced by the heteroskedasticity model. In this scenario, the visual test is compared to an approximate test - Breusch-Pagan test, and two other inappropriate tests - RESET test and Shapiro-Wilk test. 

For the non-linearity model, the power curve of RESET test climbs aggressively from 13% to around 70% as $log_e(E)$ increases from 0 to 2, while power of other tests respond inactively to the change of effect, showing that RESET test is way more sensitive to the type of model defects that being considered. Meanwhile, no noticeable visual features can be spotted from the example residual plots.

In terms of the heteroskedasticity model, the power of Breusch-Pagan test is also almost always greater than the power of visual test. For $0 \leq log_e(E) \leq 2$, where the power curve of the visual test remains at a low level, the Breusch-Pagan test still have a decent amount of chance of rejecting $H_0$. Similarly, the visual feature is nearly unobservable from the example residual plots.

The power of visual test arises steadily as $log_e(E)$ increases from 2 to 5 for both non-linearity model and heteroskedasticity model, suggesting that the effect starts to make significant impact on the degree of the presence of the designed visual features. This can also be observed from the example residual plots that when $log_e(E) = 2.5$, a weak "S-shape" and a weak "triangle" shape are presented in Figure \ref{fig:polypower} and Figure \ref{fig:heterpower} respectively. The visual pattern becomes much clearer as $log_e(E)$ increases. At $log_e(E) \approx 6$, the power reaches almost 100%.

The power of all inappropriate tests except for RESET test shows improvement as the effect increases but at a lower rate than the visual test in both scenarios. This coincides the point made by @cook1982residuals that residual-based tests for a specific type of model defect may be sensitive to other types of model defects. The power curve of RESET test remains at around 5% in Figure \ref{fig:heterpower} since there are no non-linear terms leave out in the heteroskedasticity model and $H_0$ of the test is always satisfied.

Overall, the power comparison suggests that conventional tests differs significantly from visual tests in two regression diagnostics scenarios designed by us. Visual test have much higher tolerance of the residual departures than the conventional test. Since fail to reject $H_0$ in a visual test usually means that there are no obvious visual discoveries found in the residual plot, analysts and the general public as the consumers of the output may not be convinced of the existence of significant residual departures in spite of the rejection of $H_0$ given by the conventional test. Even if the rejection is accepted, the model violation may be considered as impactless due to the fact that they are not clearly visible. Besides, the sensitivity of the conventional test could also distract and discourage analysts from finding simple but good linear approximation to the data. The rejection of $H_0$ because of human acceptable and negligible residual departures is not practically meaningful and useful. This may limit the popularity of conventional tests in residual diagnostics among analysts.

```{r}
boot_y <- function(dat, y, times = 100) {
  map_df(1:times, function(i) slice_sample(dat, n = nrow(dat), replace = TRUE) %>% mutate(boot_id = i))
}
```

```{r cache = TRUE}
target_sigma <- c(4.51, 3.52, 2.74, 2.14, 1.66, 1.29, 1.01, 0.79, 0.61, 0.48, 0.37)
es <- map_dbl(target_sigma, ~log(poly_model(shape = 2, x = rand_uniform(-1, 1), sigma = .x)$effect_size(n = 100)))
```


```{r}
p <- vi_survey %>%
  filter(x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "polynomial") %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  ggplot() +
  geom_point(data = ~.x %>% 
             group_by(log_effect_size, reject) %>% 
             summarise(num_response = n()),
           aes(log_effect_size, reject, size = factor(num_response)), alpha = 0.3)  +
  geom_smooth(data = mutate(poly_conv_sim, log_effect_size = log(effect_size)) %>%
                select(-(RESET5_p_value:RESET10_p_value)) %>%
                select(-RESET3_p_value, -F_p_value) %>%
                rename(RESET_p_value = RESET4_p_value) %>%
                filter(x_dist == "uniform") %>%
                pivot_longer(RESET_p_value:SW_p_value) %>%
                mutate(name = gsub("_p_value", " test", name)), 
              aes(log_effect_size, as.numeric(value < 0.05), col = name), 
              method = "glm", 
              method.args = list(family = binomial),
              se = FALSE) +
  stat_smooth(geom = "line", 
              data = ~boot_y(.x, reject, times = 500), 
              aes(log_effect_size, reject, group = boot_id, col = "Visual test"), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, 
              alpha = 0.03) +
  geom_smooth(aes(log_effect_size, reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  theme_light(base_size = 5) +
  theme(panel.grid.major.x =  element_line(colour = "grey70", linewidth = 0.5)) +
  scale_color_manual(values = rev(rcartocolor::carto_pal(4, "Vivid"))) +
  scale_size_manual(values = c(1, 2, 3)) +
  scale_x_continuous(breaks = seq(0, 5, 0.5), minor_breaks = NULL) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```


```{r polypower, fig.height = 3, fig.width = 5, fig.cap="Comparison of power between different tests for non-linear patterns (uniform fitted values only). Main plot shows the power curves estimated using logistic regression, with dots indicating human evaluations of lineups. Surrounding curves of the visual test show 500 samples of bootstrap estimation.  Small row of plots shows typical residual plots corresponding to specific effect sizes, marked by dashed lines in main plot. Where would you draw the line of too much non-linearity in the residuals? For the RESET test this is around log effect size 1.5, but for the visual test it is around 3.5."}
set.seed(106)
ex_dat <- poly_model(shape = 2, x = rand_uniform(), sigma = 1)$gen(n = 100)
plist <- list()
j <- 0

for (i in target_sigma) {
  j <- j + 1
  plist[[j]] <- 
  VI_MODEL$plot(poly_model(shape = 2, x = rand_uniform())$
                  gen(n = 100, computed = select(mutate(ex_dat, e = sqrt(i) * e), x, e)), 
                theme = theme_light(base_size =  5), 
              remove_axis = TRUE, 
              remove_grid_line = TRUE, size = 0.05) +
    ggtitle(round(es[j], 1)) +
    theme(aspect.ratio=0.8)
}

patchwork::wrap_plots(append(plist, list(p), 
                             after = 0), 
                      design = map(1:11, ~patchwork::area(21, .x, 22, .x)) %>%
                        reduce(c) %>%
                        c(patchwork::area(3, 1, 20, 11), .))
```
  

```{r cache = TRUE}
target_b <- c(0.14, 0.19, 0.27, 0.38, 0.57, 0.91, 1.53, 2.9, 6.4, 17.9, 75)
es <- map_dbl(target_b, ~log(heter_model(a = 1, b = .x, x = rand_uniform(-1, 1))$effect_size(n = 100, type = "kl")))
```


```{r}
p <- vi_survey %>%
  filter(x_dist == "uniform") %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "heteroskedasticity") %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  ggplot() +
  geom_point(data = ~.x %>% 
               group_by(log_effect_size, reject) %>% 
               summarise(num_response = n()),
             aes(log_effect_size, reject, size = factor(num_response)), alpha = 0.3) +
  geom_smooth(data = mutate(heter_conv_sim, log_effect_size = log(effect_size)) %>%
                filter(x_dist == "uniform") %>%
                select(-(RESET5_p_value:RESET10_p_value)) %>%
                select(-RESET3_p_value, -F_p_value) %>%
                rename(RESET_p_value = RESET4_p_value) %>%
                pivot_longer(RESET_p_value:SW_p_value) %>%
                mutate(name = gsub("_p_value", " test", name)), 
              aes(log_effect_size, as.numeric(value < 0.05), col = name), 
              method = "glm", 
              method.args = list(family = binomial),
              se = FALSE) +
  stat_smooth(geom = "line", 
              data = ~boot_y(.x, reject, times = 500), 
              aes(log_effect_size, reject, group = boot_id, col = "Visual test"), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, 
              alpha = 0.03) +
  geom_smooth(aes(log_effect_size, reject, col = "Visual test"), method = "glm", method.args = list(family = binomial), se = FALSE) +
  theme_light(base_size = 5) +
  theme(panel.grid.major.x =  element_line(colour = "grey70", linewidth = 0.5)) +
  scale_color_manual(values = rev(rcartocolor::carto_pal(4, "Vivid"))) +
  scale_x_continuous(breaks = seq(0, 5, 0.5), minor_breaks = NULL) +
  scale_size_manual(values = c(1, 2, 3)) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```  
  
  
  
```{r heterpower, fig.height = 3, fig.width = 5, fig.cap="Comparison of power between different tests for heteroskedasticity patterns (uniform fitted values only). Main plot shows the power curves, with dots indicating human evaluations of lineups. Surrounding curves of the visual test show 500 samples of bootstrap estimation. Small row of plots shows typical residual plots corresponding to specific effect sizes, marked by dashed lines in main plot. Where would you draw the line of too much heteroskedasticity in the residuals? For the BP test this is around log effect size 1.5, but for the visual test it is around 3."}
set.seed(10091)
ex_dat <- heter_model(a = 1, x = rand_uniform(-1, 1), b = 1)$gen(n = 100)
plist <- list()
j <- 0

for (i in target_b) {
  j <- j + 1
  plist[[j]] <-
  VI_MODEL$plot(heter_model(a = 1, b = i,x = rand_uniform(-1, 1))$
                  gen(n = 100, computed = select(ex_dat, x, e)), theme = theme_light(base_size =  5),
              remove_axis = TRUE,
              remove_grid_line = TRUE, size = 0.05) +
    ggtitle(round(es[j], 1)) +
    theme(aspect.ratio = 0.8)
}

patchwork::wrap_plots(append(plist, list(p), after = 0), 
                      design = map(1:11, ~patchwork::area(21, .x, 22, .x)) %>%
                        reduce(c) %>%
                        c(patchwork::area(3, 1, 20, 11), .))
```  


## Comparison of test decisions based on $p$-values



```{r p-value-comparison, fig.width=8, fig.height=4, fig.cap="Rejection rate ($p$-value $<0.05$) of visual test conditional on the conventional test decision on non-linearity (left) and heteroskedasticity (right) lineups (uniform fitted values only) displayed using a mosaic plot. The visual test rejects less frequently than the conventional test. We would generally expect that the visual test would only reject when the conventional test does. Surprisingly, this is not what we see: the visual test sometimes rejects when the conventional test doesn't. "}

library(ggmosaic)

p_value_cmp_dat <- vi_survey %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  filter(x_dist == "uniform") %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  left_join(vi_survey %>% 
           count(unique_lineup_id) %>%
           rename(num_subject = n)) %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  mutate(conv_reject = ifelse(conventional_p_value < 0.05, "Reject", "Not"),
         reject = ifelse(p_value < 0.05, "Reject", "Not")) %>%
  mutate(conv_reject = factor(conv_reject, levels = c("Reject", "Not")),
         reject = factor(reject, levels = c("Reject", "Not")))

p_value_cmp_dat %>%
  ggplot() +
  ggmosaic::geom_mosaic(aes(x = ggmosaic::product(reject, conv_reject), fill = reject)) +
  facet_grid(~type) +
  ylab("Visual tests") +
  xlab("Conventional tests") +
  labs(fill = "Conventional tests") +
  scale_fill_brewer("", palette = "Dark2") +
  theme_bw() +
  theme(legend.position = "none") +
  coord_fixed()
```

The power comparison illustrates that appropriate conventional tests reject $H_0$ more aggressively than visual tests. In this section, we explore how often they agree with each other by investigate the rejection rates based on $p$-values. 

Figure \ref{fig:p-value-comparison} provides a mosaic plot showing the rejection rate of visual tests and conventional tests for both the non-linearity model and the heteroskedasticity model.

For lineups produced by the non-linearity model, conventional tests reject `r p_value_cmp_dat %>% filter(type == "non-linearity") %>% count(conv_reject) %>% summarise(pct=round(n[1]/(n[1]+n[2])*100, 0))`% 
and visual tests reject `r p_value_cmp_dat %>% filter(type == "non-linearity") %>% count(reject) %>% summarise(pct=round(n[1]/(n[1]+n[2])*100, 0))`% of the time. Of the tests rejected by the conventional test, `r p_value_cmp_dat %>% filter(type == "non-linearity") %>% filter(conv_reject == "Reject") %>% count(reject) %>% summarise(pct=round(n[1]/(n[1]+n[2])*100, 0))`% are rejected by the visual test, that is, approximately half as many as the conventional test. Surprisingly, the visual test rejects `r p_value_cmp_dat %>% filter(type == "non-linearity") %>% filter(conv_reject == "Not") %>% count(reject) %>% summarise(pct=round(n[1]/(n[1]+n[2])*100, 0))`% (i.e. 10 out of 70) of lineups where the conventional test does not reject. 

Figure \ref{fig:poly-example} shows one of the ten lineups where the visual test rejected the residual plot. The data plot, in position three, is very obvious, and different from the nulls. The visual test was correct to reject. To understand why this is, one needs to return to the way the RESET test is applied. It requires a parameter indicating degree of polynomial to test for, and the recommendation is to generically use 4 (XXX REF). However, the "M" and the "Triple-U" shapes constructed from the Hermite polynomials use power up to 6 and 18 respectively. All 10 lineups rejected by the visual test but not the RESET were of types "M" or "Triple-U". If the RESET test had been applied using the power up to 6, all the decisions for these lineups would have been to reject. The recommendation of the polynomial power for the RESET should be revised, perhaps. This illustrates the sensitivity of conventional testing to the parameters, and it also points to a limitation that one needs to know the data structure in order to set the parameters for the test. 

<!--We also can observe from the example plot displayed in Figure \ref{fig:poly-example} that panel three is distinctly different from others, exhibiting a "M" shape. This clearly suggests users of the RESET test and any other residual-based conventional tests that require the specification of variables of interest to check the residual plot before conducting the test, such that the correct choice of variables can be made.-->

```{r poly-example, fig.height = 7, fig.cap = "Example of a non-linearity that is rejected by the visual test but not by the RESET test. The data plot at panel three exhibits strong non-linear pattern. It has effect size $= 3.89$. Interestingly, all the lineups in this group are higher order polynomials (M, Triple-U), and it is apparent that the default RESET test under-performs. The RESET test using the higher order polynomial parameter could be used, but this would change it's performance for lower order polynomials. The only way for it to be sensitive to these type of departure is to know the order prior to testing!"}
# plot poly-270 
VI_MODEL$plot_lineup(vi_lineup$poly_270$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```

For lineups produced by the heteroskedasticity model, 55% are rejected by both the conventional test and the visual test, and 24% are accepted by both, result in 79% agreement rate. Percentage of lineups only rejected by conventional test are 20%.  Besides, only 1 lineup, which is less than 1% of all the lineups, are rejected by the visual test only. This lineup shows relatively strong heteroskedasticity pattern as displayed in Figure \ref{fig:heter-example}. The visual test rejects it with a $p\text{-value} = 0.026$, but the Breusch-Pagan test produces a $p$-value slightly above 5%. However, as we can see, it is an exceptional case.


```{r heter-example, fig.height = 7, fig.cap = 'The single heteroskedasticity lineup that is rejected by the visual test but not by the BP test. The data plot at panel 17 contains a "Butterfly" shape. It has effect size$ = 4.02$. It is a mystery as to why this is not detected by the BP test.'}
# plot heter-331
VI_MODEL$plot_lineup(vi_lineup$heter_331$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```


## Effect of other factors

### Fitted value distribution 

(calc the diff in power and plot)

(randomly subset uniform and check the change of the power)

Although the distribution of the regressor is classically not restricted by the assumptions of linear regression, it will have an impact on the distribution of the fitted values and the visual effect of the residual plot. In our experiments, four distributions of the regressor are used to accommodate this variation.

Figure \ref{fig:different-x-dist-poly-power} and Figure \ref{fig:different-x-dist-heter-power} illustrate the change of power of visual tests and conventional tests for different distribution of $X_{raw}$ used in the non-linearity model and the heteroskedasticity model. We focus on $log(E) > 2$ as the visual patterns become visible. 

For the non-linearity model, we do not observe significant difference of power of visual test between distributions, except for the discrete uniform distribution. It could be due to the fact that humans have difficulties in recognising visual patterns from discreteness, which makes the shape disconnected and incomplete (ref?). In terms of conventional tests, the discrete uniform  distribution is instead the one with the greatest power, followed by the lognormal distribution, uniform distribution and the normal distribution.

For the heteroskedasticity model, the power of visual test on lineups with uniform distribution has the greatest power. This is as expected since other three distributions could reduce the chance of revealing the underlying visual pattern because of uneven data points. Although the power of visual test under the discrete uniform  distribution is the lowest in the case of non-linearity model, it has a relatively great power this time. Considering for heteroskedasticity, the visual patterns are usually detected by connecting the maximum and minimum residuals separately at different fitted values. It will not be greatly affected by the discrete uniform distribution compared to the uniform distribution. Visual tests have low power on lineups with the normal distribution and the lognormal distribution. This suggests the use of transformation of fitted values in reading residual plots. In the case of conventional tests, the discrete uniform distribution consistently doing great, followed by the uniform distribution, the normal distribution and the lognormal distribution. 

In regards of the type of the simulated model, the power of visual tests under different distributions of regressor are always lower than the corresponding conventional tests, consistent with the result from the previous sections.

```{r eval = FALSE}
vi_survey %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  filter(type == "polynomial") %>%
  filter(x_dist == "even_discrete") %>%
  filter(log(effect_size) > 4) %>%
  filter(p_value > 0.05) %>%
  .$unique_lineup_id %>%
  unique()
```

```{r eval = FALSE}
VI_MODEL$plot_lineup(vi_lineup$poly_64$data, 
                     remove_grid_line = TRUE, 
                     theme = theme_light(),
                     remove_axis = TRUE)
```




```{r different-x-dist-poly-power, fig.height = 3, fig.width = 7, fig.cap = "Power of conventional tests and visual tests on lineups produced by the non-linearity model under different fitted value distributions."}

dat_different_dist <- poly_conv_sim %>% 
  mutate(log_effect_size = log(effect_size)) %>%
  select(x_dist, log_effect_size, RESET4_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value) %>%
  mutate(x_dist = ifelse(x_dist == "even_discrete", 
                                       "Discrete", 
                                       stringr::str_to_title(x_dist))) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value < 0.05) %>%
  mutate(conv_or_not = "Conventional") %>%
  bind_rows(vi_survey %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "polynomial") %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(x_dist = ifelse(x_dist == "even_discrete", 
                         "Discrete", 
                         stringr::str_to_title(x_dist))) %>%
  mutate(conv_or_not = "Visual")) %>%
  mutate(x_dist = factor(x_dist, 
      levels = c("Uniform", "Normal", 
                 "Lognormal", "Discrete")))

ggplot() +
  geom_smooth(data=filter(dat_different_dist, 
                          x_dist == "Uniform"),
              aes(log_effect_size, 
                  reject), 
                  col = "grey", 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, size=3) +
  geom_smooth(data=dat_different_dist,
              aes(log_effect_size, 
                  reject, 
                  col = x_dist), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE, size=1) +
  theme_light(base_size = 5) +
  scale_color_manual(values = rcartocolor::carto_pal(4, "Safe")[c(4,1,3,2)]) +
  scale_size_manual(values = c(1, 2, 3, 4)) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", linetype = "Test") +
  facet_wrap(~conv_or_not) +
  theme(legend.position="bottom")
```


```{r different-x-dist-heter-power, fig.height = 3, fig.width = 5, fig.cap = "Power of conventional tests and visual tests on lineups produced by the heteroskedasticity model under different distributions of regressor."}

dat_different_dist <- heter_conv_sim %>% 
  mutate(log_effect_size = log(effect_size)) %>%
  select(x_dist, log_effect_size, BP_p_value) %>%
  pivot_longer(BP_p_value) %>%
  mutate(x_dist = ifelse(x_dist == "even_discrete", 
                                       "Discrete", 
                                       stringr::str_to_title(x_dist))) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value < 0.05) %>%
  mutate(conv_or_not = "Conventional") %>%
  bind_rows(vi_survey %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  filter(type == "heteroskedasticity") %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  mutate(reject = as.numeric(p_value <= 0.05)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(x_dist = ifelse(x_dist == "even_discrete", 
                         "Discrete", 
                         stringr::str_to_title(x_dist))) %>%
  mutate(conv_or_not = "Visual"))

dat_different_dist %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, 
                  reject, 
                  col = x_dist), 
              method = "glm", 
              method.args = list(family = binomial), 
              se = FALSE) +
  theme_light(base_size = 5) +
  scale_color_manual(values = rcartocolor::carto_pal(4, "Safe")[c(1,3,2,4)]) +
  scale_size_manual(values = c(1, 2, 3)) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "Distribution of regressor", linetype = "Test") +
  facet_wrap(~conv_or_not)
```


### Shape

```{r eval = FALSE}
poly_conv_sim %>%
  filter(x_dist == "uniform", shape %in% c(3, 4)) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  select(shape, log_effect_size, (RESET3_p_value:RESET9_p_value)) %>%
  pivot_longer(RESET3_p_value:RESET9_p_value) %>%
  mutate(name = gsub("_p_value", " ", name)) %>%
  mutate(name = gsub("RESET", "", name)) %>%
  mutate(name = gsub("test", "", name)) %>%
  mutate(reject = as.numeric(value < 0.05)) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, reject, col = name), 
              se = FALSE, 
              method = "glm",
              method.args = list(family = binomial())) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  scale_color_manual(values = rev(rcartocolor::carto_pal(7, "RedOr"))) +
  theme_light() +
  labs(col = "RESET test order") +
  facet_wrap(~shape)
```


```{r poly-power-j, fig.cap = "Power of conventional tests and visual tests on lineups produced by the non-linearity model with four different shapes controlled by the parameter $j$."}

poly_conv_sim %>%
  filter(x_dist == "uniform") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(reject = as.numeric(RESET4_p_value < 0.05)) %>%
  mutate(shape = c("U", "S", "M", "Triple-U")[shape]) %>%
  mutate(conv_type = "Conventional") %>%
  bind_rows(vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform", type == "polynomial") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(reject = as.numeric(p_value < 0.05)) %>%
  mutate(shape = c("U", "S", "M", "Triple-U")[shape]) %>%
  mutate(conv_type = "Visual")) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, reject, col = factor(shape)), 
              se = FALSE, 
              method = "glm",
              method.args = list(family = binomial())) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  scale_color_manual(values = rcartocolor::carto_pal(4, "Vivid")) +
  theme_light() +
  facet_wrap(~conv_type) +
  labs(col = "Non-linearity shape")
```

```{r heter-power-a, fig.cap = "Power of conventional tests and visual tests on lineups produced by the heteroskedasticity model with three different shapes controlled by the parameter $a$."}

heter_conv_sim %>%
  filter(x_dist == "uniform") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(reject = as.numeric(BP_p_value < 0.05)) %>%
  mutate(a = c("left-triangle", "butterfly", "right-triangle")[a + 2]) %>%
  mutate(conv_type = "Conventional") %>%
  bind_rows(vi_survey %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  filter(x_dist == "uniform", type == "heteroskedasticity") %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(reject = as.numeric(p_value < 0.05)) %>%
  mutate(a = c("left-triangle", "butterfly", "right-triangle")[a + 2]) %>%
  mutate(conv_type = "Visual")) %>%
  ggplot() +
  geom_smooth(aes(log_effect_size, reject, col = factor(a)), 
              se = FALSE, 
              method = "glm",
              method.args = list(family = binomial())) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  scale_color_manual(values = rcartocolor::carto_pal(3, "Vivid")) +
  theme_light() +
  facet_wrap(~conv_type) +
  labs(col = "Heteroskedasticity shape")
```


# Appendix

## Effect derivation



## $p$-value scatter plot

```{r p-value-comparison-old, eval = FALSE, fig.height=4, fig.cap="Visual test p-value compared to conventional test p-value for lineups produced by heteroskedasticity and non-linearity model. The scatter plot is drawn on a square root scale. Every dot on the plot corresponds to a lineup. Most of the dots are fallen on the left of the y = x line indicating visual test p-value is almost always higher than conventional test p-value. The blue curve is a smoothing of the dots showing a positive trend. "}
vi_survey %>%
  filter(!null_lineup) %>%
  filter(!attention_check) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  left_join(vi_survey %>% 
           count(unique_lineup_id) %>%
           rename(num_subject = n)) %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  mutate(type = factor(type, levels = c("non-linearity", "heteroskedasticity"))) %>%
  ggplot() +
  geom_abline() +
  geom_point(aes(conventional_p_value, p_value), alpha = 0.4) +
  geom_smooth(aes(conventional_p_value, p_value), se = FALSE, span = 0.6) +
  scale_x_sqrt() +
  scale_y_sqrt() +
  facet_wrap(~type) +
  ylab("Visual test p-value on square root scale") +
  xlab("Conventional test p-value on square root scale") +
  theme_light()
```

## A collection interesting lineups (unusual results)

why unusual? what is the possible explanations?

# targe journal

JRSSB: Journal of the Royal Statistical Society Series B (Statistical Methodology)
Deadline: Jan 1, Apr 1

Reading: style of writing, author guideline (https://rss.onlinelibrary.wiley.com/hub/journal/14679868/author-guidelines)

JCGS
JDSS

\newpage
