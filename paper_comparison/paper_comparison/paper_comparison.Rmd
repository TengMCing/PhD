---
title: |
  Why shouldn't you use numerical tests to diagnose the linear regression models?
type: ARTICLE TEMPLATE
author:
  - name: Weihao Li
    affil: a
    email: weihao.li@monash.edu
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a
    email: emi.tanaka@monash.edu
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
bibliography: paper.bib
abstract: |
  Abstract to fill.
keywords: |
  visual inference; model diagnostics;
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
output: rticles::tf_article
---

problem: residual plot diagnostics
conventional test: too sensitive

background: 

<!-- week 1 -->
1. residual plot for model diagnostics

a. residual is widely used
b. what are the types of residual plots
c. comparison

<!-- week 2 -->
2. conventional test: F, BP
3. visual test: lineup, theory

<!-- week 4 -->
desc of experiment:
1. simulation setup
2. experimental design
3. result

<!-- week 3 -->
comparison of conventional tests:
1. power (visual test vs. conventional test)
(visual test most different one (everything test, any departure))
plot figure in a paper, desc, exp
2. investigate the difference (gap), give examples
3. conventional is too sensitive
4. make conventional less sensitive (vary alpha)


<!-- last week -->
conclusion:
1. too sensitive, visual test is needed/preferable
2. visual test is infeasible in large scale (expensive)
3. future work (role of computer vision)

# Introduction

Regression diagnostics conventionally involve evaluating the fitness of the proposed model, detecting the presence of influential observations and outliers, checking the validity of model assumptions and many more. Common diagnostic techniques including summary statistics, hypothesis testing, and data plots are essential tools for a systematic and detailed examination of the regression model [@mansfield1987diagnostic].

## Diagnostic plots

Regression analysis is a field of study with at least a hundred years of history. Many of those regression diagnostic methods and procedures are mature and well-established in books first published in the twentieth century, such as @draper_applied_2014, @montgomery_introduction_2012, @belsley_regression_1980, @cook_applied_1999 and @cook1982residuals. Regardless of the level of difficulty of the book, one will find the importance and usefulness of diagnostic plots being emphasized again and again. Checking diagnostic plots is also the recommended starting point for validating model assumptions like normality, homoscedasticity and linearity [@anscombe_examination_1963]. 

Graphical summaries in which residuals are plotted against fitted values or other functions of the predictor variables that are approximately orthogonal to residuals are refereed to as standard residual plots. They are commonly used to identify patterns which are indicative of nonconstant error variance or non-linearity [@cook1982residuals]. Raw residuals and studentized residuals are the two most frequently used residuals in standard residual plots. The debt on which type of residuals should be used always present. While raw residuals are the most common output of computer regression software package, by applying a scaling factor, the ability of revealing nonconstant error variance in standard residual plots will often be enhanced by studentized residuals in small sample size [@gunst2018regression]. 

As a two-dimensional representation of a model in a $p$-dimensional space, standard residual plots project  data points onto the variable of the horizontal axis, which is a vector in $p$-dimensional space. Observations with the same projection will be treated as equivalent as they have the same position of the abscissa. Therefore, standard residual plots are often useful in revealing model inadequacies in the direction of the variable of the horizontal axis, but could be inadequate for detecting patterns in other directions, especially in those perpendicular to the variable of the horizontal axis. Hence, in practice, multiple standard residual plots with different horizontal axes will be examined.


<!-- large sample size-->
Overlapping data points is a general issue in scatter plots not limited to standard residual plots, which often makes plots difficult to interpret because visual patterns are concealed. Thus, for relatively large sample size, @cleveland1975graphical suggests the use of robust moving statistics as reference lines to give aids to eye in seeing patterns, which nowadays, are usually replaced with a spline or local polynomial regression line.

Other types of data plots that are often used in regression diagnostics include partial residual plots and probability plots. Partial residual plots are useful supplements to standard residual plots as they provide additional information on the extent of the non-linearity. Probability plots can be used to compare the sampling distribution of the residuals to the normal distribution for assessing the normality assumptions.

## Hypothesis testing

In addition to diagnostic plots, researcher may also perform formal tests for detecting model defects. Depends on the alternative, variety of tests can be applied. For example, for testing heteroskedasticity, 
one may use the White test [@white_heteroskedasticity-consistent_1980] or the Breusch-Pagan test [@breusch_simple_1979]. And for testing non-linearity, there are RESET test [@ramsey_tests_1969] and F-test.  

As discussed in @cook1982residuals, most residual based tests for a particular type of departures from model assumptions are sensitive to other types of departures. Especially, outliers will often  incorrectly trigger the rejection of null hypothesis despite the residuals are well-behaved [@cook_applied_1999]. This can be largely avoided in diagnostic plots as experienced analysts can evaluate the acceptability of assumptions flexibly, even in the presence of outliers. Furthermore, @montgomery_introduction_2012 stated that based on their experience, statistical tests are not widely used in regression diagnostics. Most importantly, the same or even larger amount of information can be provided by diagnostic plots than the corresponding tests in most empirical studies. But still, the effectiveness of statistical tests shall not be disrespected. Statistical tests have chance to provide analysts with unique information. There are also situations where no suitable diagnostic plots can be found for a particular violation of the assumptions, or excessive diagnostic plots need to be checked. One will have no choice but to rely on statistical tests if there is any. A good regression diagnostic practice should be a combination of both methods.

## Visual inference

Diagnostic plots are 

<!-- However, unlike confirmatory data analysis built upon rigorous statistical procedures, e.g., hypothesis testing, visual diagnostics relies on graphical perception - human’s ability to interpret and decode the information embedded in the graph [@cleveland_graphical_1984], which is to some extent subjective. Further, visual discovery suffers from its unsecured and unconfirmed nature where the degree of the presence of the visual features typically can not be measured quantitatively and objectively, which may lead to over or under-interpretations of the data. One such example is finding an over-interpretation of the separation between gene groups in a two-dimensional projection from a linear discriminant analysis when in fact there are no differences in the expression levels between the gene groups and separation is not an uncommon occurrence [@roy_chowdhury_using_2015]. -->

<!-- Recently, a new branch of statical inference  -->

<!-- Visual inference was first introduced in a 1999 Joint Statistical Meetings (JSM) talk with the title "Inference for Data Visualization" by @buja_inference_1999 as an idea to address the issue of valid inference for visual discoveries of data plots [@gelman_exploratory_2004]. Later, in the Bayesian context, data plots was systematically considered as model diagnostics by taking advantage of the data simulated from the assumed statistical models [@gelman_bayesian_2003; @gelman_exploratory_2004].  -->


# Experimental design

# Data processing


# Results

## Overview of the Data

We collected 400 lineup evaluations made by 20 participants in experiment I and 880 lineup evaluations made by 44 participants in experiment II. In total, 442 unique lineups were evaluated by 64 subjects. In experiment I, one of the participants skipped all 20 lineups. Hence, the submission was rejected and removed from the dataset. In experiment II, there was a participant failed one of the two attention checks, but there was no further evidence of low-effort throughout the experiment. Therefore, the submission was kept.



## Power comparision

1. power (visual test vs. conventional test)
(visual test most different one (everything test, any departure))
plot figure in a paper, desc, exp
2. investigate the difference (gap), give examples
3. conventional is too sensitive
4. make conventional less sensitive (vary alpha)

It was expected that with larger effect size, both conventional test and visual test will have higher probability in rejecting the null hypothesis when it is not true. To explore the difference in power between conventional test and visual test, ten logistic regression models are fit to the data, with natural logarithm of the effect size as the only fixed effect, and whether the test successfully rejects the null hypothesis as the response variable. Table \ref{tab:powerglmheter} and \ref{tab:powerglmheter} summarizes the results. The natural logarithm of the effect size is positive and significant across all models. Figure \ref{fig:power-com} illustrates the fitted models, while providing the local constant estimate of the power of F-test and Breusch–Pagan test for comparison. Data for the conventional test is simulated under the same model setting used in two experiments and 500000 samples are drawn for both cubic and heteroskedasticity model.

From Figure \ref{fig:power-com}, it can be observed that the fitted power of visual test increased as the number of evaluations increased for both cubic and heteroskedasticity model. 

For heteroskedasticity model, this phenomenon was more obvious as the curves of visual tests with evaluations greater than two were always above the curves of visual test with evaluations smaller than two. However, this only held for large enough effect size. For small effect size, visual tests with fewer evaluations might have greater power. Note that, the expected power of visual test derived by @majumder_validation_2013 followed by the assumption that human has the ability to select the plot with the highest t-statistic from a lineup under the classical linear model regression setting showed similar properties, where visual tests with fewer evaluations were expected to perform better when the parameter values close to the null hypothesis. 

For cubic model, the separation between curves was small. Fitted power of visual test with three to five evaluations were almost identical to each other in regards of effect size. In addition, all five curves peaked at one as effect size increased, suggesting that identification of non-linearity as a visual task can be completed reliably by human when the effect size is large enough.  

As shown in Figure \@ref(fig:power-com), both F-test and Breusch–Pagan test generally possessed greater power than visual test. It was also found that there was a huge gap between the conventional test curves and the visual test curves at around $log(\text{effect size}) = 0$ for the cubic model and $log(\text{effect size}) = 2.5$ for the heteroskedasticity model. 




We further analysed the lineup with the corresponding effect size.


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```


```{r}
library(tidyverse)
library(visage)
pilot <- read_csv(here::here("data/processed/processed_pilot.csv"))
survey <- read_csv(here::here("data/processed/processed_surveys.csv"))
sim_conv <- read_csv(here::here("data/processed/sim_conv_result.csv"), col_types = "dddlc")
```

```{r}
survey <- survey %>%
  mutate(log_effect_size = log(effect_size))

sim_conv <- sim_conv %>%
  mutate(log_effect_size = log(effect_size))

lineup_info <- select(survey, type:answer, conventional_p_value, ori_difficulty, exp, exp_lineup_id, log_effect_size) %>%
  group_by(exp_lineup_id) %>% 
  filter(row_number() == 1) %>%
  ungroup()

write_csv(lineup_info, here::here("data/processed/processed_lineup_info.csv"))


# Local constant estimator
kernel_regression_cubic <- function(h = 0.3) {
  with(filter(sim_conv, type == "cubic", log_effect_size > -5, log_effect_size < 10),
     KernSmooth::locpoly(x = log_effect_size, y = reject, bandwidth = h, degree = 0, gridsize = 1000)) %>%
    as_tibble() %>%
    mutate(type = "cubic", h = h) %>%
    rename(log_effect_size = x) %>%
    rename(reject = y)
}

# Local constant estimator
kernel_regression_heter <- function(h = 0.4) {
  with(filter(sim_conv, type != "cubic", log_effect_size > -3),
     KernSmooth::locpoly(x = log_effect_size, y = reject, bandwidth = h, degree = 0, gridsize = 1000)) %>%
    as_tibble() %>%
    mutate(type = "heteroskedasticity", h = h) %>%
    rename(log_effect_size = x) %>%
    rename(reject = y)
}
```


```{r}
if (file.exists(here::here("data/processed/lineup_env.rds"))) {
  env <- readRDS(here::here("data/processed/lineup_env.rds"))
} else {
  env <- new.env()
}

save_lineup_env <- function(env) {
  saveRDS(env, here::here("data/processed/lineup_env.rds"))
}

survey_pvalue <- survey %>%
  calc_p_value_multi(lineup_id = "exp_lineup_id", 
                     detected = "detect", 
                     n_sel = "num_selection",
                     comb = TRUE,
                     n_eval = 1:5,
                     n_sim = 100000,
                     cache_env = env) %>%
  rename(exp_lineup_id = lineup_id) %>%
  unnest_longer(col = p_value) %>%
  left_join(lineup_info) %>%
  mutate(reject = eval_p_value(p_value, tol = 0.01))

save_lineup_env(env)
```


```{r results='asis'}
map(1:5, function(x) glm(reject ~ log_effect_size,
                         family = binomial(),
                         data = filter(survey_pvalue, n_eval == {{x}}, type == "cubic"))) %>%
  stargazer::stargazer(header = FALSE,
            label = "tab:powerglmcubic",
            title = "Logistic regressions with rejection as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for cubic model. Five regression were fitted for different number of evaluations.",
            style = "default",
            column.labels = c("num\\_eval: 1", "num\\_eval:2", "num\\_eval: 3", "num\\_eval: 4", "num\\_eval:5"),
            model.numbers = FALSE)
```


```{r results="asis"}
map(1:5, function(x) glm(reject ~ log_effect_size,
                         family = binomial(),
                         data = filter(survey_pvalue, n_eval == {{x}}, type == "heteroskedasticity"))) %>%
  stargazer::stargazer(header = FALSE,
            label = "tab:powerglmheter",
            title = "Logistic regressions with rejection as response variable and natural logarithm of the effect size as regressor fitted on the data collected from experiment I and II for heteroskedasticity model. Five regression were fitted for different number of evaluations.",
            style = "default",
            column.labels = c("num\\_eval: 1", "num\\_eval:2", "num\\_eval: 3", "num\\_eval: 4", "num\\_eval:5"),
            model.numbers = FALSE)
```





```{r power-com, fig.cap="\\label{fig:power-com}power com"}
survey_pvalue %>%
  mutate(reject = as.numeric(reject)) %>%
  mutate(n_eval = factor(n_eval)) %>%
  ggplot() +
  geom_point(aes(log_effect_size, reject), alpha = 0.01) +
  geom_line(data = kernel_regression_cubic(), aes(log_effect_size, reject), linetype = 2) +
  geom_line(data = kernel_regression_heter(), aes(log_effect_size, reject), linetype = 2) +
  geom_smooth(aes(log_effect_size, reject, group = n_eval, col = n_eval),
              method = "glm", method.args = list(family = binomial), size = 0.5, se = FALSE) +
  scale_colour_viridis_d(option = "B", begin = 0.5, end = 0.9) +
  facet_wrap(~type, scales = "free_x") +
  ylab("Estimated power")
```

